{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "72cfee65",
   "metadata": {},
   "source": [
    "# 문제 1) Tokenizer 생성하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0d549254",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tokenizer():\n",
    "     \n",
    "    def __init__(self):\n",
    "        self.word_dict = {'oov': 0}\n",
    "        self.fit_checker = False\n",
    "\n",
    "    def preprocessing(self, sequences):\n",
    "        \n",
    "        import re\n",
    "        \n",
    "        result = []\n",
    "        for i in range(len(sequences)):\n",
    "            sequences[i] = re.sub(r\"[^a-zA-Z0-9ㄱ-ㅎ가-힣]\", ' ', sequences[i])  # 정규표현식으로 한국어/영어를 제외한 문자를 제거\n",
    "            split_str = [ i.lower() for i in sequences[i].split() ]    # 문장을 단어 단위로 나누고 소문자로 변환한 후 list화\n",
    "            result.append(split_str) \n",
    "\n",
    "        return result\n",
    "    \n",
    "    def fit(self, sequences):\n",
    "        self.fit_checker = False\n",
    "\n",
    "        idx = 0    # 딕셔너리에 key값으로 설정하기 위한 변수\n",
    "        splited_word = self.preprocessing(sequences)\n",
    "        for go_list in splited_word:     # nested list를 푼다.\n",
    "            for word in go_list:         # list 안의 단어를 하나씩 꺼낸다.\n",
    "                if word in self.word_dict:   # 똑같은 단어가 list 안에 중복하게 존재할 경우 \n",
    "                    continue                 # 가장 먼저 추가된 단어만을 추가하기 위한 제한\n",
    "                idx += 1                     # 0이 oov이기 때문에 1부터 추가 시작\n",
    "                self.word_dict[word] = idx   # word_dict에 key값에 따른 value값 추가\n",
    "                \n",
    "        self.fit_checker = True\n",
    "\n",
    "    def transform(self, sequences):\n",
    "\n",
    "        tokens = self.preprocessing(sequences)\n",
    "        result = tokens.copy()\n",
    "        \n",
    "        if self.fit_checker:\n",
    "            tmp = 0\n",
    "                    \n",
    "            for list_token in result:\n",
    "                for idx, word in enumerate(list_token):\n",
    "                    if word in self.word_dict.keys():\n",
    "                        result[tmp][idx] = self.word_dict[word]\n",
    "                    else:\n",
    "                        result[tmp][idx] = self.word_dict['oov']\n",
    "                tmp += 1\n",
    "            return result\n",
    "        else:\n",
    "            raise Exception(\"Tokenizer instance is not fitted yet.\")\n",
    "\n",
    "    def fit_transform(self, sequences):\n",
    "        self.fit(sequences)\n",
    "        result = self.transform(sequences)\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5f2aa851",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 2, 3, 4], [1, 5, 6], [7, 8, 2, 9]]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequences = ['I go to school.', 'I LIKE pizza!', 'we can go home!']\n",
    "a = Tokenizer()\n",
    "a.fit_transform(sequences)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdfb797b",
   "metadata": {},
   "source": [
    "# 문제 2) TfidfVectorizer 생성하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7e3af345",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TfidfVectorizer:\n",
    "    def __init__(self, tokenizer):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.fit_checker = False\n",
    "\n",
    "    def fit(self, sequences):        \n",
    "        tokenized = self.tokenizer.fit_transform(sequences)\n",
    "                \n",
    "        from math import log\n",
    "        import numpy as np\n",
    "        \n",
    "        self.idf = []                # idf 값을 추가하기 위한 초기 객체\n",
    "        self.words = np.array([])    # tokenzied되어 있는 단어들을 저장하는 초기 객체\n",
    "        N = len(tokenized)    # 문장 단위로 끊기 위한 객체 생성 \n",
    "        \n",
    "                                     # sent_idx = sentense_index(문장을 나누기 위한 인덱스)\n",
    "        for sent_idx in tokenized:   # tokenized를 array화하여 전체 단어 숫자를 세기 위함 \n",
    "            self.words = np.append(self.words, sent_idx)\n",
    "        self.words = list(set(self.words)) # 중복 제거 \n",
    "\n",
    "                                                    # word_idx = word_index(단어 하나하나를 나누기 위한 인덱스)\n",
    "        for word_idx in range(len(self.words)):     # 단어들의 숫자만큼 for loop\n",
    "            t = self.words[word_idx]     # (중복제거 된) 단어를 하나씩 꺼내기\n",
    "            df= 0\n",
    "            for doc in tokenized:   # 문장을 하니씩 꺼내기\n",
    "                df += t in doc      # 문장안에 꺼낸 단어가 있을 때\n",
    "                idf = log(N/(df+1)) # 공식에 집어넣기\n",
    "\n",
    "            self.idf.append(idf)    # idf 객체에 추가하여 저장\n",
    "            \n",
    "        self.fit_checker = True\n",
    "\n",
    "    def transform(self, sequences):\n",
    "        \n",
    "        import numpy as np\n",
    "        \n",
    "        if self.fit_checker:\n",
    "            tokenized = self.tokenizer.transform(sequences)\n",
    "        \n",
    "        self.tfidf_matrix = []       # matrix값을 추가하기 위한 초기 객체        \n",
    "\n",
    "        N = len(tokenized)\n",
    "\n",
    "                                               # sent_idx = sentense_index(문장을 나누기 위한 인덱스)\n",
    "        for sent_idx in range(N):          # 문장 단위의 for loop\n",
    "            self.tfidf_matrix.append([])   # 문장의 갯수만큼의 nested list 생성\n",
    "            sentens = tokenized[sent_idx]  # 문장 하나씩 slicing\n",
    "\n",
    "            for word_idx in range(len(self.words)):\n",
    "                word = self.words[word_idx]                            # self.word에 저장되어 있던 word를 하나씩 꺼냄\n",
    "                if word in sentens:                                    # 만약 문장에 단어가 존재한다면\n",
    "                    self.tfidf_matrix[-1].append(sentens.count(word))  # 문장안에 몇개의 단어가 있는지를 파악하여 그 수를 추가한다.\n",
    "                else:\n",
    "                    self.tfidf_matrix[-1].append(0)                    # 단어가 존재하지 않으면 0을 추가\n",
    "        result = np.array(self.tfidf_matrix) * np.array(self.idf)      # numpy의 broadcast를 사용하여 곱셈\n",
    "\n",
    "        return result.tolist() # nested list화\n",
    "        \n",
    "    def fit_transform(self, sequences):\n",
    "        self.fit(sequences)\n",
    "        return self.transform(sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c0ab068a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0.0, 0.0, 0.4054651081081644, 0.4054651081081644, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " [0.0, 0.0, 0.0, 0.0, 0.4054651081081644, 0.4054651081081644, 0.0, 0.0, 0.0],\n",
       " [0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.4054651081081644,\n",
       "  0.4054651081081644,\n",
       "  0.4054651081081644]]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequences = ['I go to school.', 'I LIKE pizza!', 'we can go home!']\n",
    "\n",
    "se = TfidfVectorizer(Tokenizer())\n",
    "se.fit(sequences)\n",
    "se.transform(sequences)\n",
    "se.fit_transform(sequences)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
