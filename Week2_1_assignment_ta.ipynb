{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Week2_1_assignment_ta.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMf5NDEO5ONQ3CdEJstmCMZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/unknown-jun/wanted_pre_onboarding/blob/main/Week2_1_assignment_ta.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# [Introduction](https://riverkangg.github.io/nlp/nlp-bertWordEmbedding/)\n",
        "## Why BERT embeddings?\n",
        "이 포스팅에서는 BERT를 사용하여 텍스트 데이터에서 특정, 즉 단어 및 문장 임베딩 벡터를 추출하는 방법을 설명하고자한다. 우선 단어와 문장 임베딩 벡터로 무엇을 할 수 있을까?\n",
        "- 임베딩은 키워드/검색어 확장, 의미 찾기 및 정보 검색에 유용하다. 예를 들어, 고객의 질문(검색)을 이미 답변된 질문이나 잘 문서화된 검색과 비교하려는 경우, 임베딩 벡터를 사용하면 키워드나 구문이 겹치지 않더라도 고객의 의도와 일치하는 결과를 찾을 수 있다.\n",
        "- 임베딩 벡터는 다운 스트림 모델에서 고품질 입력 피처로 사용된다. LSTM 또는 CNN과 같은 NLP 모델에는 숫자 벡터 형식의 입력이 필요하며, 이는 일반적으로 어휘 및 품사와 같은 기능을 숫자로 변환하는 것을 의미한다. 이전에는 단어가 고유한 인덱스값(원-핫 인코딩)으로 표현되거나 Word2Vec 또는 Tasttect와 같은 모델에서 생성된 고정 길이 임베딩과 어휘 단어가 일치하는 neural word embeddings으로 더 유용하게 표현되었다.\n",
        "\n",
        "BERT는 Word2Vec과 같은 모델에 비해 문맥을 고려한 임베딩이 된다는 장점이 있다. Word2Vec은 단어가 나타나는 문맥에 관계없이 각 단어가 고정된 표현을 가지지만, BERT는 주변 단어에 의해 동적으로 변하는 단어표현을 생성한다. 예를 들어 다음 두 문장이 주어진다면:\n",
        "- 배를 타고 여행을 간다.\n",
        "- 추석에 먹은 배가 맛있었다.\n",
        "\n",
        "Word2Vec은 두 문장의 \"배\"라는 단어에 대해 동일한 단어 임베딩을 생성하는 반면 BERT에서는 \"배\"에 대한 단어 임베딩이 문장마다 다르다. 다의어를 포착하는 것 외에도 문맥에 맞는 단어 임베딩은 다른 형태의 정보를 알아낸다. 결과적으로 더 정확한 feature representation이 가능하며 이에 따라 모델 성능이 향상된다."
      ],
      "metadata": {
        "id": "NaXtGeiX7pVa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Loading Pre-Trained BERT\n",
        "Hugging Face(이 라이브러리에는 OpenAI의 GPT 및 GPT-2와 같은 사전 학습된 다른 언어 모델에 대한 인터페이스를 포함)로 BERT용 PyTorch 인터페이스를 설치힌다.\n",
        "\n",
        "이 튜토리얼에서는 PyTorch를 사용한다. high-level API는 사용하기 쉽지만 작동방식에 대한 통찰력을 제공하지 않고, tensorflow는 설정해야할 사항이 많다.(하지만 BERT를 사용하다보면 tensorflow를 사용할 일이 많다.)"
      ],
      "metadata": {
        "id": "61ssf3Iu9iZO"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6DHKCxUvRHif",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f973bc5c-d8a4-48d1-cd54-0c642ae3986f"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.17.0)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.49)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.6.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.11.6)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.5)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.11.3)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.4.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.63.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.7.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "BERT 모델은 Google의 사전 학습된 모델로 다양한 장르의 도서가 10,000개 이상 포함된 데이터 세트인 Wikipedia, Book Corpus에서 긴 시간 동안 학습된 것이다. 이 모델은 NLP의 여러 과제에서 최고점수를 달성했다.\n",
        "\n",
        "`transformers`는 BERT를 다른 작업(토큰 분류, 텍스트 분류 등)에 적용하기 위해 여러 클래스를 제공한다. 이번 포스팅에서는 단어 임베딩이 목적이기 때문에, 출력이 없는 기본`BertModel`을 사용한다."
      ],
      "metadata": {
        "id": "wrQn_fU9-GdW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import BertTokenizer, BertModel\n",
        "\n",
        "# OPTIONAL: if you want to have more information on what's happening, activiate the logger as follows\n",
        "import logging\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load pre-trained model tokenizer(vocabulary)\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')"
      ],
      "metadata": {
        "id": "Hn8scrPYrB7e"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Input Formatting\n",
        "BERT는 특정 형식의 입력 데이터를 필요로 한다.\n",
        "1. **special token**`[sep]`은 문장의 끝을 표시하거나 두 문장을 분리할 때 사용한다.\n",
        "2. **special token**`[CLS]`은 문장을 시작할 때 사용한다. 이 토큰은 분류문제에서 사용되지만, 어떤 문제를 풀더라도 입력해야 한다.\n",
        "3. BERT에서 사용되는 단어사전에 있는 토큰\n",
        "4. BERT 토크나이저의 토큰에 대한 **Token ID**\n",
        "5. 시퀀스에서 어떤 요소가 토큰이고 패딩 요소인지를 나타내는 **Mask ID**\n",
        "6. 다른 문장을 구별하는데 사용되는 **Segment ID**\n",
        "7. 시퀀스 내에서 토큰 위치를 표시하는 데 사용되는 **Positional Embeddings**\n",
        "\n",
        "다행히도 `transformers`인터페이스는 위의 모든 사항을 처리한다.(tokenizer.encode_plus 함수 사용). 하지만 이 포스팅은 BERT 작업을 소개하기 위한 것이므로 (대부분)수동으로 이러한 단계를 진행한다."
      ],
      "metadata": {
        "id": "AT-oWHCY-0WD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.1. Special Tokens\n",
        "-----\n",
        "BERT는 하나 또는 두개의 문장을 입력으로 사용할 수 있고, 특수 토큰 `[SEP]`으로 구분한다. `[CLS]`토큰은 항상 텍스트 시작 부분에 나타나며 분류 문제를 해결할 때만 사용되지만 다른 문제를 풀더라도 입력은 무조건 해야 한다."
      ],
      "metadata": {
        "id": "dgLvxbHd_uVe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 2 Sentence input 예시\n",
        "sent1 = '드디어 내일 주말이다. '\n",
        "sent2 = '날씨가 맑으면 공원에 가야겠다.'\n",
        "cls_token= '[CLS] '\n",
        "sep_token= '[SEP] '\n",
        "\n",
        "cls_token + sent1 + sep_token + sent2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "dsr1RvQMnRgq",
        "outputId": "ad05a4ad-f857-44a2-f239-b3fc84616426"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'[CLS] 드디어 내일 주말이다. [SEP] 날씨가 맑으면 공원에 가야겠다.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1 Sentence input 예시\n",
        "cls_token + sent1 + sep_token"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "jGJF9vCXApsD",
        "outputId": "8b0da53e-781a-41df-c470-6311dc6af0e0"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'[CLS] 드디어 내일 주말이다. [SEP] '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.2. Tokenization\n",
        "-----\n",
        "BERT는 자체 토크나이저를 제공한다.BERT의 토크나이저는 WordPiece 모델을 사용한다. 이 모델은 언어데이터에 가장 적합한 개별 문자, 하위 단어(subwords) 및 단어의 고정 크기 단어사전를 탐욕스럽게(greedily) 만든다. BERT 토크나이저는 30,000개로 단어사전의 크기를 제한하기 때문에, WordPiece 모델은 영어 코처스에서 최대 30,000개의 가장 일반적인 단어 및 하위 단어로 단어사전을 만든다.\n",
        "- 한국어의 경우 영어에 비해 학습된 텍스트의 갯수가 현저히 적다."
      ],
      "metadata": {
        "id": "d1uQSAOSBMJS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text= \"임베딩을 시도할 문장이다.\"\n",
        "marked_text= \"[CLS] \" + text + \" [SEP]\"\n",
        "\n",
        "# Tokenize our sentence with the BERT tokenizer.\n",
        "tokenized_text= tokenizer.tokenize(marked_text)\n",
        "\n",
        "print(tokenized_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BwHp2xYyBv6d",
        "outputId": "8135915e-1a91-4bdb-8c70-d60fb9c45c75"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['[CLS]', '임', '##베', '##딩', '##을', '시', '##도', '##할', '문', '##장이', '##다', '.', '[SEP]']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "단어사전에는 다음 네가지가 포함된다.\n",
        "1. 전체 단어\n",
        "2. 단어의 앞에 또는 분리되어 발생하는 하위단어(\"embeddings\"에서와 같이 \"em\"에는 \"got get em\"에서와 같이 독립형 문자 \"em\"시퀀스와 동일한 벡터가 할당 됨)\n",
        "3. 단어 앞에 있지 않은 하위 단어. 이 경우를 나타내기 위해'##'이 앞에 붙는다.\n",
        "4. 개별 문자\n",
        "\n",
        "이 모델에서 단어를 토큰화하기 위해 토크나이저는 먼저 전체 단어가 어휘에 있는지 확인한다. 그렇지 않은 경우 단어를 어휘에 포함된 가능한 가장 큰 하위단어로 나누고, 하위 단어로도 나뉘어지지 않는다면 개별 문자로 분해한다. 이 때문에 우리는 항상 최소한 개별 문자의 모음으로 단어를 표현할 수 있다.\n",
        "\n",
        "다시 말하자면, 사전에 없는 단어를 'OOV' 또는 'UNK'와 같은 토큰을 주는 대신, 어휘에 포함되지 않은 단어는 사전에 있는 하위 단어 및 문자 토큰으로 분해한다.\n",
        "\n",
        "위의 결과를 보면 단어사전에 없는 \"임베딩\"이라는 단어는 ['임', '##베','##딩']으로 분할된다. 이러한 하위 단어 임베딩 벡터를 평균하여 원래 단어에 대한 근사 벡터를 생성할 수도 있다.\n",
        "\n",
        "텍스트를 토큰으로 분리한 후, 토큰화된 문자 리스트를 숫자리스트로 바꿔야 한다."
      ],
      "metadata": {
        "id": "Qj_DlyhACICR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
        "# Define a new example sentence with multiple meanings of the word \"bank\"\n",
        "text = \"After stealing money from the bank vault, the bank robber was seen \" \\\n",
        "       \"fishing on the Mississippi river bank.\"\n",
        "\n",
        "# Add the special tokens.\n",
        "marked_text= \"[CLS] \" + text + \" [SEP]\"\n",
        "\n",
        "# Split the sentence into tokens.\n",
        "tokenized_text= tokenizer.tokenize(marked_text)\n",
        "\n",
        "# Map the token strings to their vocabulary indeces.\n",
        "indexed_tokens= tokenizer.convert_tokens_to_ids(tokenized_text)\n",
        "\n",
        "# Display the words with their indeces.\n",
        "for tup in zip(tokenized_text, indexed_tokens):\n",
        "    print('{:<12} {:>6,}'.format(tup[0], tup[1]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cNiQGI20D6ul",
        "outputId": "d872afb1-d000-4043-e3c2-ca528c6cd8b7"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CLS]           101\n",
            "After         1,258\n",
            "stealing     11,569\n",
            "money         1,948\n",
            "from          1,121\n",
            "the           1,103\n",
            "bank          3,085\n",
            "vault        13,454\n",
            ",               117\n",
            "the           1,103\n",
            "bank          3,085\n",
            "r               187\n",
            "##ob         12,809\n",
            "##ber         3,169\n",
            "was           1,108\n",
            "seen          1,562\n",
            "fishing       5,339\n",
            "on            1,113\n",
            "the           1,103\n",
            "Mississippi   5,201\n",
            "river         2,186\n",
            "bank          3,085\n",
            ".               119\n",
            "[SEP]           102\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.3. Segment ID\n",
        "-----\n",
        "BERT는 두 문장을 구별하기 위해 1과 0을 사용하여 문장 쌍을 학습하고 예상한다. 즉, 토큰화된 텍스트의 각 토큰에 대해 어떤 문장에 속하는지 지정해야 한다: 문장 0(0 리스트) 또는 문장1(1 리스트). 우리의 목적을 위해 단일 문장 입력에는 1 리스트만 필요하므로 입력 문장의 각 토큰에 대해 1로 구성된 벡터를 생성한다."
      ],
      "metadata": {
        "id": "UGSsaE7GFNZ4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Mark each of the 29 tokens as belonging to sentence \"1\"\n",
        "segments_ids= [1] * len(tokenized_text)\n",
        "\n",
        "print(segments_ids)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mov4Y32GFj7f",
        "outputId": "0214bc15-6c47-42e9-fa17-a628b336c705"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Extracting Embeddings"
      ],
      "metadata": {
        "id": "bvDMJmePK_2C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.1.Running BERT on our text\n",
        "-----\n",
        "데이터를 토치 텐서(torch tensor)로 변환하고 BERT 모델을 호출해야 한다. BERT PyTorch 인터페이스에서는 데이터 형태가 Python list가 아닌 토치 텐서가 필요하므로 이번 장에서 변환한다.- 이 작업은 형태나 데이터를 변경하지 않는다."
      ],
      "metadata": {
        "id": "OrigQbTzGD1G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert inputs to PyTorch tensors\n",
        "tokens_tensor= torch.tensor([indexed_tokens])\n",
        "segments_tensor= torch.tensor([segments_ids])"
      ],
      "metadata": {
        "id": "L17tGLbmGbeH"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokens_tensor, segments_tensor"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EWrY9vLAlX0g",
        "outputId": "5365cfdc-80c8-411c-b1eb-26a44b995557"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[  101,  1258, 11569,  1948,  1121,  1103,  3085, 13454,   117,  1103,\n",
              "           3085,   187, 12809,  3169,  1108,  1562,  5339,  1113,  1103,  5201,\n",
              "           2186,  3085,   119,   102]]),\n",
              " tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]))"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "`from_pretrained`를 호출하면 웹에서 모델을 다운로드한다. `bert-base-multilingual-case`를 로드하면 로깅에 인쇄된 모델의 정의를 볼 수 있다. 이 모델은 12개의 레이러로 구성된 심층 신경망이다.\n",
        "\n",
        "`model.eval()`은 학습 모드가 아닌 평가 모드로 모델을 설정한다. 이 경우 평가 모드는 훈련에 사용되는 드롭아웃 정규화(dropout regularization)를 해제한다.\n",
        "\n",
        "*Side note: torch.no_grad는 PyTorch가 순방향 패스(forward pass)를 하는 동안 컴퓨팅 그래프를 구성하지 않도록한다(여기서 backprop를 실행하지 않기 때문에). 메모리 소비를 줄이고 작업 속도를 약간 높일 수 있다.*"
      ],
      "metadata": {
        "id": "U0_bWy24GqfG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load pre-trained model (weights)\n",
        "model = BertModel.from_pretrained('bert-base-cased',\n",
        "                                  output_hidden_states = True, # Whether the model returns all hidden-states.\n",
        "                                  )\n",
        "\n",
        "# Put the model in \"evaluation\" mode, meaning feed-forward operation.\n",
        "model.eval()\n",
        "\n",
        "# Run the text through BERT, and collect all of the hidden states produced\n",
        "# from all 12 layers\n",
        "with torch.no_grad():\n",
        "\n",
        "    outputs= model(tokens_tensor, segments_tensor)\n",
        "\n",
        "    # Evaluation the model will return a different number of objects based on\n",
        "    # how it's configured in the 'from_pretrained' call earlier. In this case,\n",
        "    # because we set 'output_hidden_states=True', the third item will be the \n",
        "    # hidden states from all layers. \n",
        "    hidden_states= outputs[2]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vgbXkuJqI4nq",
        "outputId": "2c69b7ee-75a9-4017-f234-37573fd3f22a"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.2. Understanding the Output\n",
        "-----\n",
        "`hidden_states`개체에 저장된 이 모델의 전체 은닉층은 약간 복잡하다. 이 개체에는 다음 순서로 4개의 차원이 있다.\n",
        "1. The layer number (13 layers)\n",
        "2. The batch number (1 sentence)\n",
        "3. The word / token number (36 tokens in our sentence)\n",
        "4. The hidden unit / feature number (768 features)\n",
        "\n",
        "BERT는 12개의 레이어만을 가지고 있지만 입력 임베딩을 고려한 구조이다. 즉 첫번째 요소는 입력 임베딩이고 나머지는 BERT의 12개 레이어 각각의 결과이므로 13개이다."
      ],
      "metadata": {
        "id": "t-ULokWYK2C_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print('Number of layers:', len(hidden_states), '  (inital embeddings + 12 BERT layers)')\n",
        "layer_i= 0\n",
        "\n",
        "print('Number of batches:', len(hidden_states[layer_i]))\n",
        "batch_i= 0\n",
        "\n",
        "print('Number of tokens:', len(hidden_states[layer_i][batch_i]))\n",
        "token_i= 0\n",
        "\n",
        "print('Number of hidden units:', len(hidden_states[layer_i][batch_i][token_i]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OiQSQWrXLue6",
        "outputId": "e90bc2c5-6c85-4d0a-aebe-8d43db7c49ce"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of layers: 13   (inital embeddings + 12 BERT layers)\n",
            "Number of batches: 1\n",
            "Number of tokens: 24\n",
            "Number of hidden units: 768\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "주어진 레이어와 토큰에 대한 값의 범위가 모든 레이어와 토큰에 상당히 유사하다는 것을 알 수 있다. 대부분의 값이 [-2.5, 2.5] 사이에 있다."
      ],
      "metadata": {
        "id": "CfIvSNrEORem"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# For the 5th token in our sentence, select its feature values from layer 5.\n",
        "token_i = 5\n",
        "layer_i = 5\n",
        "vec = hidden_states[layer_i][batch_i][token_i]\n",
        "\n",
        "# Plot the values as a histogram to show their distribution.\n",
        "plt.figure(figsize=(10,10))\n",
        "plt.hist(vec, bins=200)\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 592
        },
        "id": "B5AqV3nhNa4X",
        "outputId": "66b67155-fd4b-46b2-95b0-67a0d0bb3680"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 720x720 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "레이어별로 값을 그룹화하는 것이 모델에 적합하지만, 단어 임베딩을 위해 토큰별로 그룹화한다.\n",
        "\n",
        "현재 차원:\n",
        "```\n",
        "[# layers, # batches, # tokens, # features]\n",
        "```\n",
        "\n",
        "원하는 차원:\n",
        "```\n",
        "[# tokens, # layers, # features]\n",
        "```\n",
        "다행히 PyTorch에는 텐서 차원을 쉽게 재배열 할 수 있는 `permute`함수가 포함되어 있다.\n",
        "\n",
        "그러나 첫번째 차원은 현재 Pyton list이다!\n"
      ],
      "metadata": {
        "id": "K0-xvovcOi0C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# hidden_states' is a Python list.\n",
        "print('      Type of hidden_states: ', type(hidden_states))\n",
        "\n",
        "# Each layer in the list is a torch tensor\n",
        "print('Tensor shape for each layer: ', hidden_states[0].size())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fnrdxtG-PB1q",
        "outputId": "7116c4b8-ad2f-47a6-9049-0ced0fc48b9f"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      Type of hidden_states:  <class 'tuple'>\n",
            "Tensor shape for each layer:  torch.Size([1, 24, 768])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "레이어를 결합해서 하나의 큰 텐서를 만든다."
      ],
      "metadata": {
        "id": "YOeXEbinPc0l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Concatenate the tensors for all layers. We use 'stack' here to\n",
        "# create a new dimension in the tensor.\n",
        "token_embeddings= torch.stack(hidden_states, dim=0)\n",
        "\n",
        "token_embeddings.size()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2w-j7_U6Pf4c",
        "outputId": "422f9ec4-822f-45af-ac83-c48599c79e93"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([13, 1, 24, 768])"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\"batches\" 차원은 필요하지 않으므로 제거한다."
      ],
      "metadata": {
        "id": "8j1l1o4wPu3h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove dimension 1, the \"batches\".\n",
        "token_embeddings= torch.squeeze(token_embeddings, dim=1)\n",
        "\n",
        "token_embeddings.size()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xodMlQ3dPzCg",
        "outputId": "5828f21d-9c7a-4f71-b76f-1e6b0b182358"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([13, 24, 768])"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "마지막으로 `permute`를 사용하여 \"layers\" 및 \"tokens\"차원을 전환할 수 있다."
      ],
      "metadata": {
        "id": "Mni3WiAsRC7_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Swap dimensions 0 and 1\n",
        "token_embeddings= token_embeddings.permute(1,0,2)\n",
        "token_embeddings.size()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qPGEWIxqRKCj",
        "outputId": "b634f354-e592-4b48-e6dd-dc672f4a64fc"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([24, 13, 768])"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.3. Creating word and sentence vectors form hidden states\n",
        "-----\n",
        "은닉층으로 무엇을 할 수 있을지 알아보자. 각 토큰에 대한 개별 벡터 또는 전체 문장의 단일 벡터 표현을 얻고 싶지만, 입력의 각 토큰에 대해 각각 768 크기의 13개의 개별 벡터가 있다.\n",
        "\n",
        "개별 벡터를 얻으려면 일부 레이어 벡터를 결합해야 한다. 어떤 레이어 또는 레이어 조합이 최상의 표현을 제공할 수 있을까?\n",
        "\n",
        "안타깝게도 명확한 답은 없다.하지만 몇 가지 합리적인 접근 방식을 시도해보고, 추가로 살펴볼 수 있는 몇 가지 유용한 리소스를 소개한다."
      ],
      "metadata": {
        "id": "wbAcPRq2Rain"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Word Vectors\n",
        "두 가지 방법으로 단어 벡터를 만들어보자.  \n",
        "먼저 마지막 4개의 레이어를 **연결하여(concatenate)** 토큰 당 단일 벡터를 제공한다. 각 벡터의 길이는 `4 x 768 = 3072`다."
      ],
      "metadata": {
        "id": "3U7D8yaUR6MW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Stores the token vectors, with shape [36 x 3,072]\n",
        "token_vecs_cat = []\n",
        "\n",
        "# 'token_embeddings' is a [22 x 12 x 768] tensor.\n",
        "\n",
        "# For each token in the sentence...\n",
        "for token in token_embeddings:\n",
        "\n",
        "    # 'token' is a [12 x 768] tensor\n",
        "\n",
        "    # Concatenate the vectors (that is, append them together) \n",
        "    # from the last four layers.\n",
        "    # Each layer vector is 768 values, so 'cat_vec' is length 3,072\n",
        "    cat_vec = torch.cat((token[-1], token[-2], token[-3], token[-4]), dim=0)\n",
        "\n",
        "    # Use 'cat_vec' to represent 'token'.\n",
        "    token_vecs_cat.append(cat_vec)\n",
        "\n",
        "print('Shape is: %d x %d' % (len(token_vecs_cat), len(token_vecs_cat[0])))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bxzaO0fHSlm3",
        "outputId": "70a72f1c-25a7-426e-9606-e3a9154fc2b9"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape is: 24 x 3072\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "다른 방법으로 마지막 4개의 레이어를 합산하여**(summing)** 단어 벡터를 만든다."
      ],
      "metadata": {
        "id": "gbX1GYnYT80u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Stores the token vectors, with shape [36 x 768]\n",
        "token_vecs_sum = []\n",
        "\n",
        "# 'token_embeddings' is a [36 x 12 x 768] tensor.\n",
        "\n",
        "# For each token in the sentence...\n",
        "for token in token_embeddings:\n",
        "\n",
        "    # 'token' is a [12 x 768] tensor\n",
        "\n",
        "    # Sum the vectors from the last four layers.\n",
        "    sum_vec = torch.sum(token[-4:], dim=0)\n",
        "\n",
        "    # Use 'sum_vec' to represent 'token'.\n",
        "    token_vecs_sum.append(sum_vec)\n",
        "\n",
        "print('Shape is: %d x %d' % (len(token_vecs_sum), len(token_vecs_sum[0])))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YaWWaeCPUDRa",
        "outputId": "c1b58d2c-be0d-467f-ac3c-1c890fa397c9"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape is: 24 x 768\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Sentence Vectors\n",
        "전체 문장에 대한 단일 벡터를 얻기 위해 여러 application-dependent 전략이 있지만, 간단한 접근 방식은 단일 768크기의 벡터를 생성하는 각 토큰의 두번째에서 마지막 숨겨진 레이어를 평균내는 것이다."
      ],
      "metadata": {
        "id": "Jf8oJTQ-U9BJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 'hidden_states' has shape [13 x 1 x 36 x 768]\n",
        "\n",
        "# 'token_vecs' is a tensor with shape [36 x 768]\n",
        "token_vecs = hidden_states[-2][0]\n",
        "\n",
        "# Calculate the average of all 36 token vectors.\n",
        "sentence_embedding = torch.mean(token_vecs, dim=0)\n",
        "\n",
        "print('Our final sentence embedding vector of shape:',sentence_embedding.size())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6qSpMjIiVIvV",
        "outputId": "409fcc69-1040-40ae-a3e1-78e45e4c88df"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Our final sentence embedding vector of shape: torch.Size([768])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.4 Confirming contextually dependent vectors\n",
        "-----\n",
        "이러한 벡터의 값을 실제로 상황에 따라 달라지는지 확인하기 위해, 'bank'라는 단어의 여러 인스턴스를 살펴보자"
      ],
      "metadata": {
        "id": "C8kK_uR2XiCF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i, token_str in enumerate(tokenized_text):\n",
        "    print(i, token_str)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A_hj3b8jXuJL",
        "outputId": "bcb6d63e-5bf8-485b-d86d-da32bde9fe67"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 [CLS]\n",
            "1 After\n",
            "2 stealing\n",
            "3 money\n",
            "4 from\n",
            "5 the\n",
            "6 bank\n",
            "7 vault\n",
            "8 ,\n",
            "9 the\n",
            "10 bank\n",
            "11 r\n",
            "12 ##ob\n",
            "13 ##ber\n",
            "14 was\n",
            "15 seen\n",
            "16 fishing\n",
            "17 on\n",
            "18 the\n",
            "19 Mississippi\n",
            "20 river\n",
            "21 bank\n",
            "22 .\n",
            "23 [SEP]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "'bank'라는 인스턴스는 7, 17에 있다. 이 분석에서는 마지막 4개의 레이어를 합산하여 만든 단어 벡터를 사용한다. 벡터를 출력하여 비교해 볼 수 있다."
      ],
      "metadata": {
        "id": "dUfJEwnrX4fq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print('First 5 vector values for each instance of \"bank\".')\n",
        "print('')\n",
        "print(\"bank vault   \", str(token_vecs_sum[6][:5]))\n",
        "print(\"bank robber  \", str(token_vecs_sum[10][:5]))\n",
        "print(\"river bank   \", str(token_vecs_sum[19][:5]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GYET0UuQYF58",
        "outputId": "8cc04bf9-8354-4357-b222-013c609c0764"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First 5 vector values for each instance of \"bank\".\n",
            "\n",
            "bank vault    tensor([-4.1573,  1.7935, -2.6768,  3.3647,  1.6400])\n",
            "bank robber   tensor([-2.0299, -0.9155, -4.2468,  4.2540,  2.1868])\n",
            "river bank    tensor([ 3.1898, -0.4442, -1.9258, -0.4348,  2.5461])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "값이 다른 것을 볼 수 있지만 더 정확한 비교를 위해 벡터 간의 코사인 유사성을 계산한다."
      ],
      "metadata": {
        "id": "MyetYMkAYZ7F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.spatial.distance import cosine\n",
        "\n",
        "# Calculate the cosine similarity between the word bank \n",
        "# in \"bank robber\" vs \"river bank\" (different meanings).\n",
        "diff_bank = 1 - cosine(token_vecs_sum[10], token_vecs_sum[19])\n",
        "\n",
        "# Calculate the cosine similarity between the word bank\n",
        "# in \"bank robber\" vs \"bank vault\" (same meaning).\n",
        "same_bank = 1 - cosine(token_vecs_sum[10], token_vecs_sum[6])\n",
        "\n",
        "print('Vector similarity for  *similar*  meanings:  %.2f' % same_bank)\n",
        "print('Vector similarity for *different* meanings:  %.2f' % diff_bank)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K56WW5VuYejW",
        "outputId": "ce9e4343-02e3-45cf-a9c0-4c16de9f8635"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vector similarity for  *similar*  meanings:  0.90\n",
            "Vector similarity for *different* meanings:  0.68\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Week2_1 Assignment\n",
        "\n",
        "# [BASIC](#Basic)\n",
        "- BERT 모델의 hidden state에서 **특정 단어의 embedding을 여러 방식으로 추출 및 생성**할 수 있다.\n",
        "\n",
        "# [CHALLENGE](#Challenge)\n",
        "- **cosine similarity 함수를 구현**할 수 있다. \n",
        "- **단어들의 유사도**를 cosine similarity로 비교할 수 있다. \n",
        "\n",
        "# [ADVANCED](#Advanced)\n",
        "- 문장 embedding을 구해 **문장 간 유사도**를 구할 수 있다.\n",
        "\n",
        "### Reference\n",
        "- [BERT word embedding & sentence embedding tutorial 영문 블로그](https://mccormickml.com/2019/05/14/BERT-word-embeddings-tutorial/#33-creating-word-and-sentence-vectors-from-hidden-states)"
      ],
      "metadata": {
        "id": "kbEypsoFl4s2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import sys\n",
        "import pandas as pd\n",
        "import numpy as np \n",
        "import torch\n",
        "import random"
      ],
      "metadata": {
        "id": "m3URgspsoQlU"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Basic\n",
        "### BERT 모델과 토크나이저 로드   \n",
        "- 두 사람의 대화에서 (단어 및 문장의) embedding을 생성하고자 한다. 아래 대화를 BERT 모델에 입력해 출력값 중 \"hidden states\"값을 가져오자.\n",
        "- `Hidden States`는 3차원 텐서를 가지고 있는 list 타입이다. List에는 BERT 모델의 각 layer마다의 hidden state 3차원 텐서를 갖고 있으며 각 텐서는 (batch_size, sequence_length, hidden_size) shape을 가진다. BERT-base 모델은 12 layer를 갖고 있고 이와 별도로 Embedding Layer 1개를 더 갖고 있기 때문에 `len(hidden states)`는 13개가 된다. \n",
        "    - batch_size: 학습 시 설정한 배치 사이즈. 또는 BERT 모델에 입력된 문장의 개수\n",
        "    - sequence_length: 문장의 token의 개수. \n",
        "    - hidden size: token의 embedding size \n",
        "- Reference\n",
        "    - [BertTokenizer.tokenize() 함수의 매개변수 설명](https://huggingface.co/transformers/v3.0.2/main_classes/tokenizer.html#transformers.PreTrainedTokenizer.__call__)\n",
        "    - [BERTModel.forward() 함수의 매개변수 및 리턴 값 설명](https://huggingface.co/docs/transformers/model_doc/bert#transformers.BertModel.forward)"
      ],
      "metadata": {
        "id": "dTNhsIq_pzT5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizer, BertModel\n",
        "tokenizer_bert = BertTokenizer.from_pretrained('bert-base-cased')\n",
        "model_bert = BertModel.from_pretrained(\"bert-base-cased\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UJRKJHGAp5Do",
        "outputId": "34697d8f-5a15-4ecf-d836-37f570a03588"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "normal_person= ['What do you do when you have free time?']\n",
        "nerd= ['I code. code frees my minds, body and soul.']\n",
        "normal_person.append('(what a nerd...) coding?')\n",
        "nerd.append(\"Yes. coding is the best thing to do in the free time.\")\n",
        "\n",
        "for i in range(len(normal_person)):\n",
        "    print(f\"Normal Person asked: {normal_person[i]}\")\n",
        "    print(f\"Nerd answers: {nerd[i]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xXptF2VbqXAs",
        "outputId": "5861d42e-125e-4b53-ad13-c91d3df627ca"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Normal Person asked: What do you do when you have free time?\n",
            "Nerd answers: I code. code frees my minds, body and soul.\n",
            "Normal Person asked: (what a nerd...) coding?\n",
            "Nerd answers: Yes. coding is the best thing to do in the free time.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 매개변수 설명\n",
        "# truncation <- max_len 넘어가지 않도록 자르기\n",
        "# padding <- max(seq_len, max_len) zero padding\n",
        "# return_tensors <- return 2d tensor\n",
        "\n",
        "inputs= tokenizer_bert(\n",
        "    text= normal_person,\n",
        "    text_pair= nerd,\n",
        "    truncation= True,\n",
        "    padding= \"longest\",\n",
        "    return_tensors= 'pt'\n",
        ")\n",
        "\n",
        "print(inputs['input_ids'].shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1K_wte7Rq6Eg",
        "outputId": "d6146ec3-d569-424f-f5b2-694f5418c19b"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 28])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# decoding\n",
        "for i in range(len(inputs['input_ids'])):\n",
        "    print(f\"Coversation {i} -> '{tokenizer_bert.decode(inputs['input_ids'][i])}' \")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8eDgjoh5rXDi",
        "outputId": "fc8fd431-39ae-492f-95c1-ac35da4fefc8"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Coversation 0 -> '[CLS] What do you do when you have free time? [SEP] I code. code frees my minds, body and soul. [SEP] [PAD] [PAD]' \n",
            "Coversation 1 -> '[CLS] ( what a nerd... ) coding? [SEP] Yes. coding is the best thing to do in the free time. [SEP]' \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# \"code\" 단어의 token id(각 단어에게 고유하게 주어진 id)를 출력\n",
        "tokenizer_bert.encode('code', add_special_tokens=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vqsxqrYqrtd8",
        "outputId": "99dcde46-d7f9-48b5-ac70-a9c8c802bf69"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[3463]"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if torch.cuda.is_available():\n",
        "    device= torch.device('cuda')\n",
        "else:\n",
        "    device= torch.device('cpu')\n",
        "\n",
        "print(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_C6UzO5SspVR",
        "outputId": "c2b75104-683e-4424-e996-0627cac17c82"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 입력 데이터와 BERT 모델을 \"GPU\" 장치로 로드함\n",
        "inputs= inputs.to(device)\n",
        "model_bert.to(device)"
      ],
      "metadata": {
        "id": "Yqu8Wnrgs00U",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6ad4cf7b-b955-421d-a055-0dddf3540593"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertModel(\n",
              "  (embeddings): BertEmbeddings(\n",
              "    (word_embeddings): Embedding(28996, 768, padding_idx=0)\n",
              "    (position_embeddings): Embedding(512, 768)\n",
              "    (token_type_embeddings): Embedding(2, 768)\n",
              "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "  )\n",
              "  (encoder): BertEncoder(\n",
              "    (layer): ModuleList(\n",
              "      (0): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (intermediate_act_fn): GELUActivation()\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (1): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (intermediate_act_fn): GELUActivation()\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (2): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (intermediate_act_fn): GELUActivation()\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (3): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (intermediate_act_fn): GELUActivation()\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (4): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (intermediate_act_fn): GELUActivation()\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (5): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (intermediate_act_fn): GELUActivation()\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (6): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (intermediate_act_fn): GELUActivation()\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (7): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (intermediate_act_fn): GELUActivation()\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (8): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (intermediate_act_fn): GELUActivation()\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (9): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (intermediate_act_fn): GELUActivation()\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (10): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (intermediate_act_fn): GELUActivation()\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (11): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (intermediate_act_fn): GELUActivation()\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (pooler): BertPooler(\n",
              "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "    (activation): Tanh()\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 입력 데이터를 BERT 모델에 넣어 출력값을 가져옴\n",
        "outputs= model_bert(\n",
        "    **inputs,\n",
        "    output_hidden_states=True\n",
        ")"
      ],
      "metadata": {
        "id": "ETfeq8Rbsz9t"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "outputs.keys()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P40OPrVHtZCf",
        "outputId": "6d23f9ab-10a0-426f-e3ca-dcbee4afd8c4"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "odict_keys(['last_hidden_state', 'pooler_output', 'hidden_states'])"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "hidden_states= outputs['hidden_states']\n",
        "print(f'# layers: {len(hidden_states)}')\n",
        "print(f\"tensor shape in each layer: {hidden_states[-1].shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fGmYSezStqqq",
        "outputId": "b36ab076-edfe-4000-f21e-aca8d5f70496"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# layers: 13\n",
            "tensor shape in each layer: torch.Size([2, 28, 768])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###  Q1. 1번째 sequence (문장)에서 \"code\"라는 단어의 인덱스를 모두 반환하라.\n",
        "- \"code\" 단어는 총 2개 존재 "
      ],
      "metadata": {
        "id": "a3SAf7DUxTcl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_index(seq, word):\n",
        "    code_encode = tokenizer_bert.encode(word, add_special_tokens=False)\n",
        "    index= (seq==code_encode[0]).nonzero()\n",
        "    return index\n",
        "\n",
        "# input\n",
        "# seq1: 1번째 sequence\n",
        "# token: 단어\n",
        "seq1= inputs['input_ids'][0]\n",
        "token= \"code\"\n",
        "\n",
        "# output\n",
        "token_index= get_index(seq1, token)\n",
        "print(token_index)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bHMM0u5pt5So",
        "outputId": "a8186e2a-7f3f-4e4d-ce93-8d5c6c007101"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[13],\n",
            "        [15]], device='cuda:0')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Q2. 1번째 sequence의 1번째 \"code\" 토큰의 embedding을 여러가지 방식으로 구하고자 한다. BERT hidden state를 다음의 방식으로 인덱싱해 embedding을 구하라\n",
        "- 1 layer\n",
        "- last layer\n",
        "- sum all 12 layers\n",
        "- sum last 4 layers\n",
        "- concat last 4 layers\n",
        "- average last 4 layers"
      ],
      "metadata": {
        "id": "wx6KkTaw1dA6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "index= token_index[0]\n",
        "\n",
        "# 1 layer\n",
        "first_layer_emb= hidden_states[1][0, index.item(), :]\n",
        "print(first_layer_emb.shape)\n",
        "\n",
        "# last layer\n",
        "last_layer_emb = hidden_states[-1][0, index.item(),:]\n",
        "print(last_layer_emb.shape)\n",
        "\n",
        "# sum all 12 layers\n",
        "sum_all_layer_emb = sum([hs[0, index.item(),:] for hs in hidden_states])\n",
        "print(sum_all_layer_emb.shape)\n",
        "\n",
        "# sum last 4 layers\n",
        "sum_last4_layer_emb = sum([hs[0, index.item(),:]for hs in hidden_states[-4:]])\n",
        "print(sum_last4_layer_emb.shape)\n",
        "\n",
        "# concat last 4 layers\n",
        "concat_last4_layer_emb = torch.cat([hs[0, index.item(),:]for hs in hidden_states[-4:]], dim=0)\n",
        "print(concat_last4_layer_emb.shape)\n",
        "\n",
        "# mean last 4 layers\n",
        "mean_last4_layer_emb = sum([hs[0, index.item(),:]for hs in hidden_states[-4:]]) /4\n",
        "print(mean_last4_layer_emb.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wb1z7902y0iZ",
        "outputId": "60256dc4-ccca-4ccc-a1e7-50f153ea5c37"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([768])\n",
            "torch.Size([768])\n",
            "torch.Size([768])\n",
            "torch.Size([768])\n",
            "torch.Size([3072])\n",
            "torch.Size([768])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Q3. `sum_last_four_layer` 방식으로 1번째 sequence의 2개의 \"code\" 토큰 사이의 코사인 유사도를 계산하라"
      ],
      "metadata": {
        "id": "oTh8Ulhv8i5u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def cosine_similarity_manual(x, y, small_number=1e-8):\n",
        "  from scipy.spatial.distance import cosine\n",
        "  result = 1 - cosine(x.cpu().detach().numpy(),\n",
        "                      y.cpu().detach().numpy())\n",
        "\n",
        "  return result\n",
        "\n",
        "# input\n",
        "# x: 1번째 sequence의 1번째 \"code\"의 sum_last_four_layer 방식 embedding\n",
        "# y: 1번째 sequence의 2번째 \"code\"의 sum_last_four_layer 방식 embedding\n",
        "x = sum([hs[0, token_index[0].item(),:]for hs in hidden_states[-4:]])\n",
        "y = sum([hs[0, token_index[1].item(),:]for hs in hidden_states[-4:]])\n",
        "\n",
        "# output\n",
        "score = cosine_similarity_manual(x, y)\n",
        "print(score)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9pbcvafE8kbG",
        "outputId": "73efc8be-8413-4b4c-ac02-2dc4100350ea"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.8400022983551025\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Q4. 2번째 sequence에서 \"coding\"이라는 토큰의 위치를 반환하라"
      ],
      "metadata": {
        "id": "GiL3zRUP_icF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Q1과 동일한 문제 \n",
        "\n",
        "# input\n",
        "# seq1: 2번째 sequence\n",
        "# token: 단어\n",
        "seq2 = inputs['input_ids'][1]\n",
        "token = \"coding\"\n",
        "\n",
        "# output\n",
        "# Q1에서 구현한 함수 사용\n",
        "token_index = get_index(seq2, token)\n",
        "print(token_index)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cpPfzBny-PyE",
        "outputId": "552d94a4-4f84-4df5-f0b9-31cec5a402d1"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[10],\n",
            "        [15]], device='cuda:0')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Q5. `concat_last4_layer_emb` 방식으로 2번째 sequence의 2개의 \"coding\" 토큰 사이의 코사인 유사도를 계산하라"
      ],
      "metadata": {
        "id": "OxW5uMrE_koW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Q3과 동일한 문제\n",
        "\n",
        "# input\n",
        "# x: 2번째 sequence의 1번째 \"coding\"의 concat_last4_layer_emb\n",
        "# y: 2번째 sequence의 2번째 \"coding\"의 concat_last4_layer_emb\n",
        "x = torch.cat([ hs[1, token_index[0].item(), :] for hs in hidden_states[-4:] ], dim=0)\n",
        "y = torch.cat([ hs[1, token_index[1].item(), :] for hs in hidden_states[-4:] ], dim=0)\n",
        "\n",
        "# output\n",
        "# Q3에서 구현한 함수 사용\n",
        "score = cosine_similarity_manual(x, y)\n",
        "print(score)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AROU09CO_j3z",
        "outputId": "f3083d5a-c198-42f2-8481-63feb295a242"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.8681784272193909\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Q6. 2번째 sequence에서 랜덤하게 토큰 하나를 뽑아보자. 그 랜덤 토큰과 2번째 sequence의 2번째 \"coding\" 토큰의 코사인 유사도를 계산해보자"
      ],
      "metadata": {
        "id": "kRS7TBWq_nyW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "hidden_states[0].shape[1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gvN--NF5Z8ey",
        "outputId": "28382cbc-eb2e-4548-9bac-028c0efe7669"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "28"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# input\n",
        "# random_idx: random 모듈 사용하여 뽑은 랜덤 토큰의 인덱스\n",
        "# random_word: random_idx에 해당하는 단어\n",
        "# x: 2번째 sequence의 2번째 \"coding\" 토큰의 concat_last4_layer_emb\n",
        "# y: 랜덤 토큰의 concat_last4_layer_emb\n",
        "\n",
        "random_idx = random.randint(0,hidden_states[0].shape[1])\n",
        "random_token_id = inputs['input_ids'][-1][random_idx].item()\n",
        "random_word = tokenizer_bert.decode([random_token_id])\n",
        "print('Other word:', random_word)\n",
        "\n",
        "x = torch.cat([ hs[1, token_index[1].item(), :] for hs in hidden_states[-4:] ], dim=0)\n",
        "y = torch.cat([ hs[1, random_idx, :] for hs in hidden_states[-4:] ], dim=0)\n",
        "\n",
        "# output\n",
        "# Q3에서 구현한 함수 사용\n",
        "score = cosine_similarity_manual(x, y)\n",
        "print(score)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XvPOBgsK_pEm",
        "outputId": "2f09f902-b2d8-40f9-c32e-8ffa26a5ccce"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Other word: what\n",
            "0.5863762497901917\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Advanced"
      ],
      "metadata": {
        "id": "-aXVhIdS_qwP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Q7. 1번째 sequence와 2번째 sequence의 문장 유사도를 구해보자. 문장의 엠베딩은 마지막 레이어의 첫번째 토큰 ('[CLS]')으로 생성한다."
      ],
      "metadata": {
        "id": "zjwNWbmI_uBi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# input\n",
        "# x: 1번째 sequence의 embedding\n",
        "# y: 2번째 sequence의 embedding\n",
        "x = hidden_states[-1][0, 0, :]\n",
        "y = hidden_states[-1][1, 0, :]\n",
        "\n",
        "# output\n",
        "# Q3에서 구현한 함수 사용\n",
        "score =  cosine_similarity_manual(x, y)\n",
        "print(score)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cWuaV-QR_sJA",
        "outputId": "6db487f3-86ef-4563-bf7c-fa6f0f3707fa"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.7969229817390442\n"
          ]
        }
      ]
    }
  ]
}