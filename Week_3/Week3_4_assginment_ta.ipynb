{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "592U6lXs3d2t"
      },
      "source": [
        "# Week3_4 Assignment\n",
        "\n",
        "## [BASIC](#Basic) \n",
        "- Encoder & Decoder Layer 코드를 직접 필사하고 각 함수에 주석을 달 수 있다. \n",
        "\n",
        "## [CHALLENGE](#Challenge)\n",
        "- 텐서의 크기(shape)를 계산할 수 있다. \n",
        "\n",
        "## [ADVANCED](#Advanced)\n",
        "- 완성된 transformer 모델의 모든 학습 가능한 파라미터 이름과 크기(shape)를 출력할 수 있다.\n",
        "\n",
        "### Informs\n",
        "이번 과제에서는 \"[Annotated Transformer](https://nlp.seas.harvard.edu/2018/04/03/attention.html)\"의 코드를 필사해본다.   \n",
        "\"Annotated Transformer\"는 \"Attention is all you need\" 논문에서 제안한 transformer 모델을 pytorch 라이브러리로 직접 구현한다.   \n",
        "코드 필사를 통해 다음을 배울 수 있다.    \n",
        "- Encoder, Decoder 구조\n",
        "- Attention Mechanism\n",
        "- \"residual connection\", \"layer normalization\" 등의 구조 "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yrzroTiabKfd"
      },
      "source": [
        "코드 필사를 시작하기 앞서, transformer 모델의 최종 구조를 살펴보자.    \n",
        "\n",
        "<img src=\"https://github.com/ChristinaROK/PreOnboarding_AI_assets/blob/36a670a7b6233d5218a495150beb337a899ecb70/week3/week3_3_full.png?raw=true\" width=\"500\" align=\"center\"/>\n",
        "\n",
        "최종 모델은 `EncoderDecoder()` 클래스에 여러 인스턴스를 생성자의 입력 파라미터로 넣어 생성한다.    \n",
        "앞으로 우리는 `EncoderDecoder()` 클래스와 같은 여러 클래스들을 구현하고 연결할 것이다. 따라서 대략적인 클래스간의 관계를 살펴보고 이해한다면 보다 큰 그림을 가지고 코드 필사를 할 수 있을 것이다. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TKkzVvF0bKfd"
      },
      "source": [
        "Transformer 모델은 크게 4가지 클래스로 구현된다.    \n",
        "- Frame\n",
        "    - frame 역할을 하는 `EncoderDecoder` 클래스\n",
        "- Input Embedding & Encoding\n",
        "    - 입력값을 벡터화하는 `Embeddings`, `PositionalEncoding`\n",
        "- Encoder & Decoder\n",
        "    - 각 6개 layer를 갖고 있는 `Encoder`, `Decoder`\n",
        "    - layer 1층을 구현한 `EncoderLayer`, `DecoderLayer`\n",
        "- Sublayer\n",
        "    - `EncoderLayer`, `DecoderLayer` 내부에서 사용되는 Sublayer 클래스인 `MultiHeadAttiontion`, `PositionwiseFeedForward`\n",
        "    - Sublayer 클래스들을 연결하는 `SublayerConnection`\n",
        "    \n",
        "아래 좌측 도식에서 각 클래스의 색상은 아래 우측 도식(transformer 구조)의 색상과 맵핑되어 있다.    \n",
        "각 클래스의 역할과 클래스 간 연결 관계를 생각하면서 transformer를 코드로 구현해보자.   \n",
        "\n",
        "\n",
        "<img src=\"https://github.com/ChristinaROK/PreOnboarding_AI_assets/blob/36a670a7b6233d5218a495150beb337a899ecb70/week3/week3_3_map.png?raw=true\" width=\"400\" height=\"400\" align=\"left\"/>\n",
        "<img src=\"https://github.com/ChristinaROK/PreOnboarding_AI_assets/blob/36a670a7b6233d5218a495150beb337a899ecb70/week3/week3_3_transformer.png?raw=true\" width=\"300\" height=\"400\" align=\"right\"/>\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M7dZCBohbKfe"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "import pandas as pd\n",
        "import numpy as np \n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "\n",
        "import math, copy, time\n",
        "import random"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MQPlahT9bKff"
      },
      "source": [
        "## Basic"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bc43XaLPbKff"
      },
      "source": [
        "### Frame\n",
        "- `EncoderDecoder`\n",
        "\n",
        "아래 도식은 `EncoderDecoder` 클래스의 `forward()`, `encode()`, `decode()` 메소드를 도식화 한 것이다.    \n",
        " \n",
        "<img src=\"https://github.com/ChristinaROK/PreOnboarding_AI_assets/blob/36a670a7b6233d5218a495150beb337a899ecb70/week3/week3_3_encoderdecoder.png?raw=true\" width=500>\n",
        "\n",
        "\n",
        "- `Generator`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NofYE60vbKff"
      },
      "outputs": [],
      "source": [
        "class EncoderDecoder(nn.Module):\n",
        "    \"\"\"\n",
        "    Encoder-Decoder 뼈대 모델\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, encoder, decoder, src_embed, tgt_embed, generator):\n",
        "        super(EncoderDecoder, self).__init__()\n",
        "        self.encoder = encoder # encoder layer\n",
        "        self.decoder = decoder # decoder layer\n",
        "        self.src_embed = src_embed # embedding layer before encoder \n",
        "        self.tgt_embed = tgt_embed # embedding layer before decoder \n",
        "        self.generator = generator # classification (prediction) layer after decoder (FFNN + softmax)\n",
        "    \n",
        "    \n",
        "    def forward(self, src, tgt, src_mask, tgt_mask):\n",
        "        \"\"\"\n",
        "        src와 tgt입력을 각각 인코더, 디코더 처리해 그 결과를 반환\n",
        "        \"\"\"\n",
        "        return self.decode(self.encode(src, src_mask),\n",
        "                           src_mask,\n",
        "                           tgt,\n",
        "                           tgt_mask\n",
        "                          )\n",
        "    \n",
        "    \n",
        "    def encode(self, src, src_mask):\n",
        "        return self.encoder(self.src_embed(src), src_mask)\n",
        "    \n",
        "    \n",
        "    def decode(self, memory, src_mask, tgt, tgt_mask):\n",
        "        return self.decoder(self.tgt_embed(tgt), memory, src_mask, tgt_mask)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nATJ_UGLbKff"
      },
      "outputs": [],
      "source": [
        "class Generator(nn.Module):\n",
        "    \"\"\"\n",
        "    vocab(총 단어의 수)에서 하나의 단어를 예측하는 linear + softmax 모델\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, d_model, vocab):\n",
        "        super(Generator, self).__init__()\n",
        "        self.proj = nn.Linear(d_model, vocab)\n",
        "    \n",
        "    \n",
        "    def forward(self, x):\n",
        "        return F.log_softmax(self.proj(x), dim=-1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KO1uzWbqbKfg"
      },
      "source": [
        "### Encoder\n",
        "- `Encoder`\n",
        "- `EncoderLayer`\n",
        "- `SublayerConnection`\n",
        "- Reference\n",
        "    - Layer Normalization\n",
        "        - [한국어 설명](https://yonghyuc.wordpress.com/2020/03/04/batch-norm-vs-layer-norm/)\n",
        "        - [torch official docs](https://pytorch.org/docs/stable/generated/torch.nn.LayerNorm.html)\n",
        "    - Residual Connection\n",
        "        - [한국어 설명](https://itrepo.tistory.com/36)\n",
        "    - pytorch ModuleList\n",
        "        - [torch official docs](https://pytorch.org/docs/1.9.1/generated/torch.nn.ModuleList.html)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xCngukKybKfg"
      },
      "outputs": [],
      "source": [
        "def clones(module, N):\n",
        "    \"\"\"module과 동일한 구조의 레이어를 N개 생성해 ModuleList에 담아 반환\"\"\"\n",
        "    return nn.ModuleList([copy.deepcopy(module) for _ in range(N)])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vjXHapUrbKfg"
      },
      "outputs": [],
      "source": [
        "class Encoder(nn.Module):\n",
        "    \"\"\"\n",
        "    N개의 EncoderLayer를 쌓은 모델\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, layer, N):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.layers = clones(layer, N)\n",
        "        self.norm = LayerNorm(layer.size) # Custom Class\n",
        "    \n",
        "    \n",
        "    def forward(self, x, mask):\n",
        "        \"\"\"\n",
        "        입력값 x, mask를 순차적으로 EncoderLayer에 입력\n",
        "        \"\"\"\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, mask)\n",
        "        return self.norm(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bxPIxms8bKfg"
      },
      "outputs": [],
      "source": [
        "class LayerNorm(nn.Module):\n",
        "    \"\"\"\n",
        "    Layer Normalization = 각 입력값의 feature를 정규화\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, features, eps=1e-6):\n",
        "        super(LayerNorm, self).__init__()\n",
        "        self.a_2 = nn.Parameter(torch.ones(features))\n",
        "        self.b_2 = nn.Parameter(torch.zeros(features))\n",
        "        self.eps = eps\n",
        "    \n",
        "    \n",
        "    def forward(self, x):\n",
        "        mean = x.mean(dim=-1, keepdim=True)\n",
        "        std = x.std(dim=-1, keepdim=True)\n",
        "        return self.a_2 * (x-mean) / (std + self.eps) + self.b_2\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HiTiEMYbbKfg"
      },
      "outputs": [],
      "source": [
        "class SublayerConnection(nn.Module):\n",
        "    \"\"\"\n",
        "    입력값을 순차적으로 \n",
        "    1. layer normalization,\n",
        "    2. sublayer,\n",
        "    3. dropout,\n",
        "    4. residual connection\n",
        "    에 통과\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, size, dropout):\n",
        "        super(SublayerConnection, self).__init__()\n",
        "        self.norm = LayerNorm(size)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "    \n",
        "    def forward(self, x, sublayer):\n",
        "        \"\"\"\n",
        "        residual connection을 반환\n",
        "        \"\"\"\n",
        "        return x + self.dropout(sublayer(self.norm(x)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dqa6L1P2bKfh"
      },
      "outputs": [],
      "source": [
        "class EncoderLayer(nn.Module):\n",
        "    \"\"\"\n",
        "    2개의 sublayer로 구성된 인코더\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, size, self_attn, feed_forward, dropout):\n",
        "        super(EncoderLayer, self).__init__()\n",
        "        self.self_attn = self_attn\n",
        "        self.feed_forward = feed_forward\n",
        "        self.sublayer = clones(SublayerConnection(size, dropout), 2)\n",
        "        self.size = size\n",
        "        \n",
        "    def forward(self, x, mask):\n",
        "        x = self.sublayer[0](x, lambda x: self.self_attn(x,x,x,mask))\n",
        "        return self.sublayer[1](x, self.feed_forward)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BPqXnQeWbKfh"
      },
      "source": [
        "### Decoder\n",
        "- `Decoder`\n",
        "- `DecoderLayer`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GbIlLdGmbKfh"
      },
      "outputs": [],
      "source": [
        "class Decoder(nn.Module):\n",
        "    \"\"\"\n",
        "    N개의 DecoderLayer를 쌓은 모델\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, layer, N):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.layers = clones(layer, N)\n",
        "        self.norm = LayerNorm(layer.size)\n",
        "    \n",
        "    def forward(self, x, memory, src_mask, tgt_mask):\n",
        "        \"\"\"\n",
        "        입력값 x, tgt_mask와 encoder에서 전달 받은 memory, src_mask를 순차적으로 DecoderLayer에 입력 \n",
        "        \"\"\"\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, memory, src_mask, tgt_mask)\n",
        "        return self.norm(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4sZG4Jp4bKfh"
      },
      "outputs": [],
      "source": [
        "class DecoderLayer(nn.Module):\n",
        "    \"\"\"\n",
        "    3개의 sublayer로 구성된 디코더\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, size, self_attn, src_attn, feed_forward, dropout):\n",
        "        super(DecoderLayer, self).__init__()\n",
        "        self.size = size\n",
        "        self.self_attn = self_attn\n",
        "        self.src_attn = src_attn\n",
        "        self.feed_forward = feed_forward\n",
        "        self.sublayer = clones(SublayerConnection(size, dropout), 3)\n",
        "    \n",
        "    \n",
        "    def forward(self, x, memory, src_mask, tgt_mask):\n",
        "        m = memory\n",
        "        x = self.sublayer[0](x, lambda x : self.self_attn(x,x,x,tgt_mask))\n",
        "        x = self.sublayer[1](x, lambda x : self.self_attn(x,m,m,src_mask))\n",
        "        return self.sublayer[2](x, self.feed_forward)\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uqYqQDpPbKfi"
      },
      "source": [
        "### Sublayer\n",
        "- `attention` 함수\n",
        "\n",
        "<img src=\"https://github.com/ChristinaROK/PreOnboarding_AI_assets/blob/36a670a7b6233d5218a495150beb337a899ecb70/week3/week3_3_attention.png?raw=true\" width=\"500\" align=\"center\"/>  \n",
        "\n",
        "- `MultiHeadedAttention`\n",
        "- `PositionwiseFeedForward`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "auJosc-FbKfi"
      },
      "source": [
        "### Challenge\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PN5ocFa1bKfi"
      },
      "source": [
        "### Q1. 위 도식에 따라 `score`, `p_attn`, `attention` 을 구하라 "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OqhvNO38bKfi"
      },
      "outputs": [],
      "source": [
        "def attention(query, key, value, mask=None, dropout=None):\n",
        "    \n",
        "    d_k = query.size(-1)\n",
        "    scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(d_k) \n",
        "    \n",
        "    # mask된 token 위치의 score 값을 아주 작은 값인 -1e9로 대체함 (masked token은 attention 계산을 하지 않기 위함)\n",
        "    if mask is not None:\n",
        "        scores= scores.masked_fill(mask == 0, -1e9)\n",
        "    \n",
        "    p_attn = F.softmax(scores, dim=-1)\n",
        "    \n",
        "    if dropout is not None:\n",
        "        p_attn = dropout(p_attn)\n",
        "    \n",
        "    attention = torch.matmul(p_attn, value)\n",
        "    return  attention, p_attn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3V1wDaZfbKfj"
      },
      "source": [
        "###Q2. query, key, value가 모두 (m, d_k) shape의 matrix라고 가정할 때, `score`, `p_attn`, `attention`의 shape을 각각 구하라\n",
        "- score : (m,m)\n",
        "- p_attn : (m,m)\n",
        "- attention : (m,d_k)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7-ewNXswbKfj"
      },
      "source": [
        "### (아래의 **Q3을 먼저 풀고 돌아오세요**) Q4.  query, key, value가 모두 (12, 8, 1, 64) shape의 tensor라고 가정할 때 , `score`, `p_attn`, `attention`의 shape을 각각 구하라\n",
        "\n",
        "- score : (12, 8, 1, 1)\n",
        "- p_attn : (12, 8, 1, 1)\n",
        "- attention : (12, 8, 1, 64)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o7awFpLtbKfj"
      },
      "source": [
        "- `MultiHeadedAttention`\n",
        "\n",
        "<img src=\"https://github.com/ChristinaROK/PreOnboarding_AI_assets/blob/36a670a7b6233d5218a495150beb337a899ecb70/week3/week3_3_multihead.png?raw=true\" width=\"300\" align=\"center\"/>  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mRhVIN4gbKfj"
      },
      "outputs": [],
      "source": [
        "class MultiHeadedAttention(nn.Module):\n",
        "    def __init__(self, h, d_model, dropout=0.1):\n",
        "        super(MultiHeadedAttention, self).__init__()\n",
        "        assert d_model % h == 0\n",
        "        self.d_k = d_model // h # multi headed attention은 d_model을 head 개수로 나눠 dimesion reduction을 함.\n",
        "        self.h = h\n",
        "        self.linears = clones(nn.Linear(d_model, d_model), 4)\n",
        "        self.attn = None\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "    \n",
        "    \n",
        "    def forward(self, query, key, value, mask=None):\n",
        "        if mask is not None:\n",
        "            mask = mask.unsqueeze(dim=1) # 새로운 dimension을 (axis 1 앞에) 추가\n",
        "        nbatches = query.size(0)\n",
        "        \n",
        "        # query, key, value를 각각 서로 다른 linear layer에 통과시켜(=linear projection) 얻은 값은 다시 query, key, value 변수에 할당\n",
        "        query, key, value = [l(x).view(nbatches, -1, self.h, self.d_k).transpose(1,2)\n",
        "          for l, x in zip(self.linears, (query,key,value))]\n",
        "        \n",
        "        # attention 적용\n",
        "        x, self.attn = attention(query, key, value, mask=mask, dropout=self.dropout)\n",
        "        \n",
        "        # 8개 head의 attention을 concatenate하여 마지막 linear layer에 통과\n",
        "        x = x.transpose(1,2).contiguous().view(nbatches, -1, self.h * self.d_k)\n",
        "        return self.linears[-1](x)\n",
        "            "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ttRzSRxKbKfk"
      },
      "source": [
        "### Q3.  query, key, value가 모두 (12, 512) shape의 matrix이고, h 값이 8 이라고 가정할 때, 아래 값의 shape을 각각 구하라\n",
        "\n",
        "- `d_k` (d_k = d_model // h) : 64\n",
        "- `nn.Linear(d_model, d_model)(query)` : (12,512)\n",
        "- `nn.Linear(d_model, d_model)(query).view(nbatches, -1, h, d_k)` : (12, 1, 8, 64)\n",
        "- `nn.Linear(d_model, d_model)(query).view(nbatches, -1, h, d_k).transpose(1,2)` : (12, 8, 1, 64)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "721kJh1FbKfk"
      },
      "source": [
        "- `PositionwiseFeedForward`\n",
        "\n",
        "<img src=\"https://github.com/ChristinaROK/PreOnboarding_AI_assets/blob/36a670a7b6233d5218a495150beb337a899ecb70/week3/week3_3_pwff.png?raw=true\" width=\"300\" align=\"center\"/>  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V3fhZcEJbKfk"
      },
      "outputs": [],
      "source": [
        "class PositionwiseFeedForward(nn.Module):\n",
        "    \n",
        "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
        "        super(PositionwiseFeedForward, self).__init__()\n",
        "        self.w_1 = nn.Linear(d_model, d_ff)\n",
        "        self.w_2 = nn.Linear(d_ff, d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "    \n",
        "    \n",
        "    def forward(self, x):\n",
        "        return self.w_2(self.dropout(F.relu(self.w_1(x))))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j9soPUUMbKfk"
      },
      "source": [
        "### Input Embedding & Encoding\n",
        "- `Embeddings`\n",
        "    - [pytorch official docs](https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "adOkIDsjbKfk"
      },
      "outputs": [],
      "source": [
        "class Embeddings(nn.Module):\n",
        "    \"\"\"\n",
        "    입력값 x (연속적인 토큰)를 지정된 vocab 사이즈의 lookup 테이블에서 d_model 사이즈의 엠베딩으로 변환\n",
        "    \"\"\"\n",
        "    def __init__(self, d_model, vocab):\n",
        "        super(Embeddings, self).__init__()\n",
        "        self.lut = nn.Embedding(vocab, d_model)\n",
        "        self.d_model = d_model\n",
        "    \n",
        "    def forward(self, x):\n",
        "        return self.lut(x) * math.sqrt(self.d_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tkpngqxtbKfk"
      },
      "source": [
        "- `PositionalEncoding`\n",
        "\n",
        "<img src=\"https://github.com/ChristinaROK/PreOnboarding_AI_assets/blob/36a670a7b6233d5218a495150beb337a899ecb70/week3/week3_3_pe.png?raw=true\" width=\"500\" align=\"center\"/>  \n",
        "\n",
        "- `position` 변수 설명\n",
        "    - 모든 position (=최대 토큰 개수)의 값을 갖고 있는 matrix\n",
        "- `div_term` 변수 설명\n",
        "\n",
        "<img src=\"https://github.com/ChristinaROK/PreOnboarding_AI_assets/blob/36a670a7b6233d5218a495150beb337a899ecb70/week3/week3_3_div.png?raw=true\" width=\"500\" align=\"center\"/>  \n",
        "- `Embedding` + `Encoding` 도식화 \n",
        "\n",
        "<img src=\"https://github.com/ChristinaROK/PreOnboarding_AI_assets/blob/36a670a7b6233d5218a495150beb337a899ecb70/week3/week3_3_emb_enc.png?raw=true\" width=\"400\" align=\"center\"/>  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5eMg2TYlbKfl"
      },
      "outputs": [],
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "    \"\"\"\n",
        "    입력값 x (엠베딩 된 3차원 텐서 (nbatches, max_len, d_model))에 positional encoding을 더해 반환\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, d_model, dropout, max_len = 5000):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0,max_len).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2) * -(math.log(10000.0) / d_model))\n",
        "        \n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0)\n",
        "        self.register_buffer('pe', pe)\n",
        "        \n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = x + Variable(self.pe[:, :x.size(1)], \n",
        "                        requires_grad=False)\n",
        "        return self.dropout(x)\n",
        "        "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pALWsGhvbKfl"
      },
      "source": [
        "### Q4.  max_len이 512이고, d_model이 512라고 가정할 때, `position`과 `div_term`의 shape을 구하라\n",
        "\n",
        "- `position` : (512, 1)\n",
        "- `div_term` : (256, 1)\n",
        "- `position * div_term` : (512, 256)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FVYd12BNbKfl"
      },
      "source": [
        "### Advanced"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "988d5AtSbKfl"
      },
      "source": [
        "### Finally Build Model\n",
        "- Xavier Initialization\n",
        "    - [한국어 자료](https://huangdi.tistory.com/8)\n",
        "    - [pytorch official docs](https://pytorch.org/docs/stable/nn.init.html#torch.nn.init.xavier_uniform_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vqBC3vCjbKfm"
      },
      "outputs": [],
      "source": [
        "def make_model(src_vocab, tgt_vocab, \n",
        "               N=6, d_model=512, d_ff=2048, h=8, dropout=0.1):\n",
        "    \n",
        "    c = copy.deepcopy\n",
        "    attn = MultiHeadedAttention(h, d_model)\n",
        "    ff = PositionwiseFeedForward(d_model, d_ff, dropout)\n",
        "    position = PositionalEncoding(d_model, dropout)\n",
        "    model = EncoderDecoder(\n",
        "        Encoder(\n",
        "            EncoderLayer(d_model, c(attn), c(ff), dropout),\n",
        "            N\n",
        "        ),\n",
        "        Decoder(\n",
        "            DecoderLayer(d_model, c(attn), c(attn), c(ff), dropout),\n",
        "            N\n",
        "        ),\n",
        "        nn.Sequential(Embeddings(d_model, src_vocab), c(position)),\n",
        "        nn.Sequential(Embeddings(d_model, tgt_vocab), c(position)),\n",
        "        Generator(d_model, tgt_vocab)\n",
        "    )\n",
        "    \n",
        "    # 모든 학습 파라미터를 \n",
        "    for p in model.parameters():\n",
        "        if p.dim() > 1:\n",
        "            nn.init.xavier_uniform_(p)\n",
        "    return model\n",
        "            "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XozR0BBLbKfm"
      },
      "outputs": [],
      "source": [
        "model = make_model(10,10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nt93mHK2bKfm"
      },
      "source": [
        "### Q5. 위 코드로 만든 모델의 모든 파라미터의 이름과 크기 (shape) 을 출력하라"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ub3lTZ4RbKfn",
        "outputId": "9513cbc9-acc6-4d44-e157-61cd66647538"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "encoder.layers.0.self_attn.linears.0.weight shape: torch.Size([512, 512])\n",
            "encoder.layers.0.self_attn.linears.0.bias shape: torch.Size([512])\n",
            "encoder.layers.0.self_attn.linears.1.weight shape: torch.Size([512, 512])\n",
            "encoder.layers.0.self_attn.linears.1.bias shape: torch.Size([512])\n",
            "encoder.layers.0.self_attn.linears.2.weight shape: torch.Size([512, 512])\n",
            "encoder.layers.0.self_attn.linears.2.bias shape: torch.Size([512])\n",
            "encoder.layers.0.self_attn.linears.3.weight shape: torch.Size([512, 512])\n",
            "encoder.layers.0.self_attn.linears.3.bias shape: torch.Size([512])\n",
            "encoder.layers.0.feed_forward.w_1.weight shape: torch.Size([2048, 512])\n",
            "encoder.layers.0.feed_forward.w_1.bias shape: torch.Size([2048])\n",
            "encoder.layers.0.feed_forward.w_2.weight shape: torch.Size([512, 2048])\n",
            "encoder.layers.0.feed_forward.w_2.bias shape: torch.Size([512])\n",
            "encoder.layers.0.sublayer.0.norm.a_2 shape: torch.Size([512])\n",
            "encoder.layers.0.sublayer.0.norm.b_2 shape: torch.Size([512])\n",
            "encoder.layers.0.sublayer.1.norm.a_2 shape: torch.Size([512])\n",
            "encoder.layers.0.sublayer.1.norm.b_2 shape: torch.Size([512])\n",
            "encoder.layers.1.self_attn.linears.0.weight shape: torch.Size([512, 512])\n",
            "encoder.layers.1.self_attn.linears.0.bias shape: torch.Size([512])\n",
            "encoder.layers.1.self_attn.linears.1.weight shape: torch.Size([512, 512])\n",
            "encoder.layers.1.self_attn.linears.1.bias shape: torch.Size([512])\n",
            "encoder.layers.1.self_attn.linears.2.weight shape: torch.Size([512, 512])\n",
            "encoder.layers.1.self_attn.linears.2.bias shape: torch.Size([512])\n",
            "encoder.layers.1.self_attn.linears.3.weight shape: torch.Size([512, 512])\n",
            "encoder.layers.1.self_attn.linears.3.bias shape: torch.Size([512])\n",
            "encoder.layers.1.feed_forward.w_1.weight shape: torch.Size([2048, 512])\n",
            "encoder.layers.1.feed_forward.w_1.bias shape: torch.Size([2048])\n",
            "encoder.layers.1.feed_forward.w_2.weight shape: torch.Size([512, 2048])\n",
            "encoder.layers.1.feed_forward.w_2.bias shape: torch.Size([512])\n",
            "encoder.layers.1.sublayer.0.norm.a_2 shape: torch.Size([512])\n",
            "encoder.layers.1.sublayer.0.norm.b_2 shape: torch.Size([512])\n",
            "encoder.layers.1.sublayer.1.norm.a_2 shape: torch.Size([512])\n",
            "encoder.layers.1.sublayer.1.norm.b_2 shape: torch.Size([512])\n",
            "encoder.layers.2.self_attn.linears.0.weight shape: torch.Size([512, 512])\n",
            "encoder.layers.2.self_attn.linears.0.bias shape: torch.Size([512])\n",
            "encoder.layers.2.self_attn.linears.1.weight shape: torch.Size([512, 512])\n",
            "encoder.layers.2.self_attn.linears.1.bias shape: torch.Size([512])\n",
            "encoder.layers.2.self_attn.linears.2.weight shape: torch.Size([512, 512])\n",
            "encoder.layers.2.self_attn.linears.2.bias shape: torch.Size([512])\n",
            "encoder.layers.2.self_attn.linears.3.weight shape: torch.Size([512, 512])\n",
            "encoder.layers.2.self_attn.linears.3.bias shape: torch.Size([512])\n",
            "encoder.layers.2.feed_forward.w_1.weight shape: torch.Size([2048, 512])\n",
            "encoder.layers.2.feed_forward.w_1.bias shape: torch.Size([2048])\n",
            "encoder.layers.2.feed_forward.w_2.weight shape: torch.Size([512, 2048])\n",
            "encoder.layers.2.feed_forward.w_2.bias shape: torch.Size([512])\n",
            "encoder.layers.2.sublayer.0.norm.a_2 shape: torch.Size([512])\n",
            "encoder.layers.2.sublayer.0.norm.b_2 shape: torch.Size([512])\n",
            "encoder.layers.2.sublayer.1.norm.a_2 shape: torch.Size([512])\n",
            "encoder.layers.2.sublayer.1.norm.b_2 shape: torch.Size([512])\n",
            "encoder.layers.3.self_attn.linears.0.weight shape: torch.Size([512, 512])\n",
            "encoder.layers.3.self_attn.linears.0.bias shape: torch.Size([512])\n",
            "encoder.layers.3.self_attn.linears.1.weight shape: torch.Size([512, 512])\n",
            "encoder.layers.3.self_attn.linears.1.bias shape: torch.Size([512])\n",
            "encoder.layers.3.self_attn.linears.2.weight shape: torch.Size([512, 512])\n",
            "encoder.layers.3.self_attn.linears.2.bias shape: torch.Size([512])\n",
            "encoder.layers.3.self_attn.linears.3.weight shape: torch.Size([512, 512])\n",
            "encoder.layers.3.self_attn.linears.3.bias shape: torch.Size([512])\n",
            "encoder.layers.3.feed_forward.w_1.weight shape: torch.Size([2048, 512])\n",
            "encoder.layers.3.feed_forward.w_1.bias shape: torch.Size([2048])\n",
            "encoder.layers.3.feed_forward.w_2.weight shape: torch.Size([512, 2048])\n",
            "encoder.layers.3.feed_forward.w_2.bias shape: torch.Size([512])\n",
            "encoder.layers.3.sublayer.0.norm.a_2 shape: torch.Size([512])\n",
            "encoder.layers.3.sublayer.0.norm.b_2 shape: torch.Size([512])\n",
            "encoder.layers.3.sublayer.1.norm.a_2 shape: torch.Size([512])\n",
            "encoder.layers.3.sublayer.1.norm.b_2 shape: torch.Size([512])\n",
            "encoder.layers.4.self_attn.linears.0.weight shape: torch.Size([512, 512])\n",
            "encoder.layers.4.self_attn.linears.0.bias shape: torch.Size([512])\n",
            "encoder.layers.4.self_attn.linears.1.weight shape: torch.Size([512, 512])\n",
            "encoder.layers.4.self_attn.linears.1.bias shape: torch.Size([512])\n",
            "encoder.layers.4.self_attn.linears.2.weight shape: torch.Size([512, 512])\n",
            "encoder.layers.4.self_attn.linears.2.bias shape: torch.Size([512])\n",
            "encoder.layers.4.self_attn.linears.3.weight shape: torch.Size([512, 512])\n",
            "encoder.layers.4.self_attn.linears.3.bias shape: torch.Size([512])\n",
            "encoder.layers.4.feed_forward.w_1.weight shape: torch.Size([2048, 512])\n",
            "encoder.layers.4.feed_forward.w_1.bias shape: torch.Size([2048])\n",
            "encoder.layers.4.feed_forward.w_2.weight shape: torch.Size([512, 2048])\n",
            "encoder.layers.4.feed_forward.w_2.bias shape: torch.Size([512])\n",
            "encoder.layers.4.sublayer.0.norm.a_2 shape: torch.Size([512])\n",
            "encoder.layers.4.sublayer.0.norm.b_2 shape: torch.Size([512])\n",
            "encoder.layers.4.sublayer.1.norm.a_2 shape: torch.Size([512])\n",
            "encoder.layers.4.sublayer.1.norm.b_2 shape: torch.Size([512])\n",
            "encoder.layers.5.self_attn.linears.0.weight shape: torch.Size([512, 512])\n",
            "encoder.layers.5.self_attn.linears.0.bias shape: torch.Size([512])\n",
            "encoder.layers.5.self_attn.linears.1.weight shape: torch.Size([512, 512])\n",
            "encoder.layers.5.self_attn.linears.1.bias shape: torch.Size([512])\n",
            "encoder.layers.5.self_attn.linears.2.weight shape: torch.Size([512, 512])\n",
            "encoder.layers.5.self_attn.linears.2.bias shape: torch.Size([512])\n",
            "encoder.layers.5.self_attn.linears.3.weight shape: torch.Size([512, 512])\n",
            "encoder.layers.5.self_attn.linears.3.bias shape: torch.Size([512])\n",
            "encoder.layers.5.feed_forward.w_1.weight shape: torch.Size([2048, 512])\n",
            "encoder.layers.5.feed_forward.w_1.bias shape: torch.Size([2048])\n",
            "encoder.layers.5.feed_forward.w_2.weight shape: torch.Size([512, 2048])\n",
            "encoder.layers.5.feed_forward.w_2.bias shape: torch.Size([512])\n",
            "encoder.layers.5.sublayer.0.norm.a_2 shape: torch.Size([512])\n",
            "encoder.layers.5.sublayer.0.norm.b_2 shape: torch.Size([512])\n",
            "encoder.layers.5.sublayer.1.norm.a_2 shape: torch.Size([512])\n",
            "encoder.layers.5.sublayer.1.norm.b_2 shape: torch.Size([512])\n",
            "encoder.norm.a_2 shape: torch.Size([512])\n",
            "encoder.norm.b_2 shape: torch.Size([512])\n",
            "decoder.layers.0.self_attn.linears.0.weight shape: torch.Size([512, 512])\n",
            "decoder.layers.0.self_attn.linears.0.bias shape: torch.Size([512])\n",
            "decoder.layers.0.self_attn.linears.1.weight shape: torch.Size([512, 512])\n",
            "decoder.layers.0.self_attn.linears.1.bias shape: torch.Size([512])\n",
            "decoder.layers.0.self_attn.linears.2.weight shape: torch.Size([512, 512])\n",
            "decoder.layers.0.self_attn.linears.2.bias shape: torch.Size([512])\n",
            "decoder.layers.0.self_attn.linears.3.weight shape: torch.Size([512, 512])\n",
            "decoder.layers.0.self_attn.linears.3.bias shape: torch.Size([512])\n",
            "decoder.layers.0.src_attn.linears.0.weight shape: torch.Size([512, 512])\n",
            "decoder.layers.0.src_attn.linears.0.bias shape: torch.Size([512])\n",
            "decoder.layers.0.src_attn.linears.1.weight shape: torch.Size([512, 512])\n",
            "decoder.layers.0.src_attn.linears.1.bias shape: torch.Size([512])\n",
            "decoder.layers.0.src_attn.linears.2.weight shape: torch.Size([512, 512])\n",
            "decoder.layers.0.src_attn.linears.2.bias shape: torch.Size([512])\n",
            "decoder.layers.0.src_attn.linears.3.weight shape: torch.Size([512, 512])\n",
            "decoder.layers.0.src_attn.linears.3.bias shape: torch.Size([512])\n",
            "decoder.layers.0.feed_forward.w_1.weight shape: torch.Size([2048, 512])\n",
            "decoder.layers.0.feed_forward.w_1.bias shape: torch.Size([2048])\n",
            "decoder.layers.0.feed_forward.w_2.weight shape: torch.Size([512, 2048])\n",
            "decoder.layers.0.feed_forward.w_2.bias shape: torch.Size([512])\n",
            "decoder.layers.0.sublayer.0.norm.a_2 shape: torch.Size([512])\n",
            "decoder.layers.0.sublayer.0.norm.b_2 shape: torch.Size([512])\n",
            "decoder.layers.0.sublayer.1.norm.a_2 shape: torch.Size([512])\n",
            "decoder.layers.0.sublayer.1.norm.b_2 shape: torch.Size([512])\n",
            "decoder.layers.0.sublayer.2.norm.a_2 shape: torch.Size([512])\n",
            "decoder.layers.0.sublayer.2.norm.b_2 shape: torch.Size([512])\n",
            "decoder.layers.1.self_attn.linears.0.weight shape: torch.Size([512, 512])\n",
            "decoder.layers.1.self_attn.linears.0.bias shape: torch.Size([512])\n",
            "decoder.layers.1.self_attn.linears.1.weight shape: torch.Size([512, 512])\n",
            "decoder.layers.1.self_attn.linears.1.bias shape: torch.Size([512])\n",
            "decoder.layers.1.self_attn.linears.2.weight shape: torch.Size([512, 512])\n",
            "decoder.layers.1.self_attn.linears.2.bias shape: torch.Size([512])\n",
            "decoder.layers.1.self_attn.linears.3.weight shape: torch.Size([512, 512])\n",
            "decoder.layers.1.self_attn.linears.3.bias shape: torch.Size([512])\n",
            "decoder.layers.1.src_attn.linears.0.weight shape: torch.Size([512, 512])\n",
            "decoder.layers.1.src_attn.linears.0.bias shape: torch.Size([512])\n",
            "decoder.layers.1.src_attn.linears.1.weight shape: torch.Size([512, 512])\n",
            "decoder.layers.1.src_attn.linears.1.bias shape: torch.Size([512])\n",
            "decoder.layers.1.src_attn.linears.2.weight shape: torch.Size([512, 512])\n",
            "decoder.layers.1.src_attn.linears.2.bias shape: torch.Size([512])\n",
            "decoder.layers.1.src_attn.linears.3.weight shape: torch.Size([512, 512])\n",
            "decoder.layers.1.src_attn.linears.3.bias shape: torch.Size([512])\n",
            "decoder.layers.1.feed_forward.w_1.weight shape: torch.Size([2048, 512])\n",
            "decoder.layers.1.feed_forward.w_1.bias shape: torch.Size([2048])\n",
            "decoder.layers.1.feed_forward.w_2.weight shape: torch.Size([512, 2048])\n",
            "decoder.layers.1.feed_forward.w_2.bias shape: torch.Size([512])\n",
            "decoder.layers.1.sublayer.0.norm.a_2 shape: torch.Size([512])\n",
            "decoder.layers.1.sublayer.0.norm.b_2 shape: torch.Size([512])\n",
            "decoder.layers.1.sublayer.1.norm.a_2 shape: torch.Size([512])\n",
            "decoder.layers.1.sublayer.1.norm.b_2 shape: torch.Size([512])\n",
            "decoder.layers.1.sublayer.2.norm.a_2 shape: torch.Size([512])\n",
            "decoder.layers.1.sublayer.2.norm.b_2 shape: torch.Size([512])\n",
            "decoder.layers.2.self_attn.linears.0.weight shape: torch.Size([512, 512])\n",
            "decoder.layers.2.self_attn.linears.0.bias shape: torch.Size([512])\n",
            "decoder.layers.2.self_attn.linears.1.weight shape: torch.Size([512, 512])\n",
            "decoder.layers.2.self_attn.linears.1.bias shape: torch.Size([512])\n",
            "decoder.layers.2.self_attn.linears.2.weight shape: torch.Size([512, 512])\n",
            "decoder.layers.2.self_attn.linears.2.bias shape: torch.Size([512])\n",
            "decoder.layers.2.self_attn.linears.3.weight shape: torch.Size([512, 512])\n",
            "decoder.layers.2.self_attn.linears.3.bias shape: torch.Size([512])\n",
            "decoder.layers.2.src_attn.linears.0.weight shape: torch.Size([512, 512])\n",
            "decoder.layers.2.src_attn.linears.0.bias shape: torch.Size([512])\n",
            "decoder.layers.2.src_attn.linears.1.weight shape: torch.Size([512, 512])\n",
            "decoder.layers.2.src_attn.linears.1.bias shape: torch.Size([512])\n",
            "decoder.layers.2.src_attn.linears.2.weight shape: torch.Size([512, 512])\n",
            "decoder.layers.2.src_attn.linears.2.bias shape: torch.Size([512])\n",
            "decoder.layers.2.src_attn.linears.3.weight shape: torch.Size([512, 512])\n",
            "decoder.layers.2.src_attn.linears.3.bias shape: torch.Size([512])\n",
            "decoder.layers.2.feed_forward.w_1.weight shape: torch.Size([2048, 512])\n",
            "decoder.layers.2.feed_forward.w_1.bias shape: torch.Size([2048])\n",
            "decoder.layers.2.feed_forward.w_2.weight shape: torch.Size([512, 2048])\n",
            "decoder.layers.2.feed_forward.w_2.bias shape: torch.Size([512])\n",
            "decoder.layers.2.sublayer.0.norm.a_2 shape: torch.Size([512])\n",
            "decoder.layers.2.sublayer.0.norm.b_2 shape: torch.Size([512])\n",
            "decoder.layers.2.sublayer.1.norm.a_2 shape: torch.Size([512])\n",
            "decoder.layers.2.sublayer.1.norm.b_2 shape: torch.Size([512])\n",
            "decoder.layers.2.sublayer.2.norm.a_2 shape: torch.Size([512])\n",
            "decoder.layers.2.sublayer.2.norm.b_2 shape: torch.Size([512])\n",
            "decoder.layers.3.self_attn.linears.0.weight shape: torch.Size([512, 512])\n",
            "decoder.layers.3.self_attn.linears.0.bias shape: torch.Size([512])\n",
            "decoder.layers.3.self_attn.linears.1.weight shape: torch.Size([512, 512])\n",
            "decoder.layers.3.self_attn.linears.1.bias shape: torch.Size([512])\n",
            "decoder.layers.3.self_attn.linears.2.weight shape: torch.Size([512, 512])\n",
            "decoder.layers.3.self_attn.linears.2.bias shape: torch.Size([512])\n",
            "decoder.layers.3.self_attn.linears.3.weight shape: torch.Size([512, 512])\n",
            "decoder.layers.3.self_attn.linears.3.bias shape: torch.Size([512])\n",
            "decoder.layers.3.src_attn.linears.0.weight shape: torch.Size([512, 512])\n",
            "decoder.layers.3.src_attn.linears.0.bias shape: torch.Size([512])\n",
            "decoder.layers.3.src_attn.linears.1.weight shape: torch.Size([512, 512])\n",
            "decoder.layers.3.src_attn.linears.1.bias shape: torch.Size([512])\n",
            "decoder.layers.3.src_attn.linears.2.weight shape: torch.Size([512, 512])\n",
            "decoder.layers.3.src_attn.linears.2.bias shape: torch.Size([512])\n",
            "decoder.layers.3.src_attn.linears.3.weight shape: torch.Size([512, 512])\n",
            "decoder.layers.3.src_attn.linears.3.bias shape: torch.Size([512])\n",
            "decoder.layers.3.feed_forward.w_1.weight shape: torch.Size([2048, 512])\n",
            "decoder.layers.3.feed_forward.w_1.bias shape: torch.Size([2048])\n",
            "decoder.layers.3.feed_forward.w_2.weight shape: torch.Size([512, 2048])\n",
            "decoder.layers.3.feed_forward.w_2.bias shape: torch.Size([512])\n",
            "decoder.layers.3.sublayer.0.norm.a_2 shape: torch.Size([512])\n",
            "decoder.layers.3.sublayer.0.norm.b_2 shape: torch.Size([512])\n",
            "decoder.layers.3.sublayer.1.norm.a_2 shape: torch.Size([512])\n",
            "decoder.layers.3.sublayer.1.norm.b_2 shape: torch.Size([512])\n",
            "decoder.layers.3.sublayer.2.norm.a_2 shape: torch.Size([512])\n",
            "decoder.layers.3.sublayer.2.norm.b_2 shape: torch.Size([512])\n",
            "decoder.layers.4.self_attn.linears.0.weight shape: torch.Size([512, 512])\n",
            "decoder.layers.4.self_attn.linears.0.bias shape: torch.Size([512])\n",
            "decoder.layers.4.self_attn.linears.1.weight shape: torch.Size([512, 512])\n",
            "decoder.layers.4.self_attn.linears.1.bias shape: torch.Size([512])\n",
            "decoder.layers.4.self_attn.linears.2.weight shape: torch.Size([512, 512])\n",
            "decoder.layers.4.self_attn.linears.2.bias shape: torch.Size([512])\n",
            "decoder.layers.4.self_attn.linears.3.weight shape: torch.Size([512, 512])\n",
            "decoder.layers.4.self_attn.linears.3.bias shape: torch.Size([512])\n",
            "decoder.layers.4.src_attn.linears.0.weight shape: torch.Size([512, 512])\n",
            "decoder.layers.4.src_attn.linears.0.bias shape: torch.Size([512])\n",
            "decoder.layers.4.src_attn.linears.1.weight shape: torch.Size([512, 512])\n",
            "decoder.layers.4.src_attn.linears.1.bias shape: torch.Size([512])\n",
            "decoder.layers.4.src_attn.linears.2.weight shape: torch.Size([512, 512])\n",
            "decoder.layers.4.src_attn.linears.2.bias shape: torch.Size([512])\n",
            "decoder.layers.4.src_attn.linears.3.weight shape: torch.Size([512, 512])\n",
            "decoder.layers.4.src_attn.linears.3.bias shape: torch.Size([512])\n",
            "decoder.layers.4.feed_forward.w_1.weight shape: torch.Size([2048, 512])\n",
            "decoder.layers.4.feed_forward.w_1.bias shape: torch.Size([2048])\n",
            "decoder.layers.4.feed_forward.w_2.weight shape: torch.Size([512, 2048])\n",
            "decoder.layers.4.feed_forward.w_2.bias shape: torch.Size([512])\n",
            "decoder.layers.4.sublayer.0.norm.a_2 shape: torch.Size([512])\n",
            "decoder.layers.4.sublayer.0.norm.b_2 shape: torch.Size([512])\n",
            "decoder.layers.4.sublayer.1.norm.a_2 shape: torch.Size([512])\n",
            "decoder.layers.4.sublayer.1.norm.b_2 shape: torch.Size([512])\n",
            "decoder.layers.4.sublayer.2.norm.a_2 shape: torch.Size([512])\n",
            "decoder.layers.4.sublayer.2.norm.b_2 shape: torch.Size([512])\n",
            "decoder.layers.5.self_attn.linears.0.weight shape: torch.Size([512, 512])\n",
            "decoder.layers.5.self_attn.linears.0.bias shape: torch.Size([512])\n",
            "decoder.layers.5.self_attn.linears.1.weight shape: torch.Size([512, 512])\n",
            "decoder.layers.5.self_attn.linears.1.bias shape: torch.Size([512])\n",
            "decoder.layers.5.self_attn.linears.2.weight shape: torch.Size([512, 512])\n",
            "decoder.layers.5.self_attn.linears.2.bias shape: torch.Size([512])\n",
            "decoder.layers.5.self_attn.linears.3.weight shape: torch.Size([512, 512])\n",
            "decoder.layers.5.self_attn.linears.3.bias shape: torch.Size([512])\n",
            "decoder.layers.5.src_attn.linears.0.weight shape: torch.Size([512, 512])\n",
            "decoder.layers.5.src_attn.linears.0.bias shape: torch.Size([512])\n",
            "decoder.layers.5.src_attn.linears.1.weight shape: torch.Size([512, 512])\n",
            "decoder.layers.5.src_attn.linears.1.bias shape: torch.Size([512])\n",
            "decoder.layers.5.src_attn.linears.2.weight shape: torch.Size([512, 512])\n",
            "decoder.layers.5.src_attn.linears.2.bias shape: torch.Size([512])\n",
            "decoder.layers.5.src_attn.linears.3.weight shape: torch.Size([512, 512])\n",
            "decoder.layers.5.src_attn.linears.3.bias shape: torch.Size([512])\n",
            "decoder.layers.5.feed_forward.w_1.weight shape: torch.Size([2048, 512])\n",
            "decoder.layers.5.feed_forward.w_1.bias shape: torch.Size([2048])\n",
            "decoder.layers.5.feed_forward.w_2.weight shape: torch.Size([512, 2048])\n",
            "decoder.layers.5.feed_forward.w_2.bias shape: torch.Size([512])\n",
            "decoder.layers.5.sublayer.0.norm.a_2 shape: torch.Size([512])\n",
            "decoder.layers.5.sublayer.0.norm.b_2 shape: torch.Size([512])\n",
            "decoder.layers.5.sublayer.1.norm.a_2 shape: torch.Size([512])\n",
            "decoder.layers.5.sublayer.1.norm.b_2 shape: torch.Size([512])\n",
            "decoder.layers.5.sublayer.2.norm.a_2 shape: torch.Size([512])\n",
            "decoder.layers.5.sublayer.2.norm.b_2 shape: torch.Size([512])\n",
            "decoder.norm.a_2 shape: torch.Size([512])\n",
            "decoder.norm.b_2 shape: torch.Size([512])\n",
            "src_embed.0.lut.weight shape: torch.Size([10, 512])\n",
            "src_embed.1.pe shape: torch.Size([1, 5000, 512])\n",
            "tgt_embed.0.lut.weight shape: torch.Size([10, 512])\n",
            "tgt_embed.1.pe shape: torch.Size([1, 5000, 512])\n",
            "generator.proj.weight shape: torch.Size([10, 512])\n",
            "generator.proj.bias shape: torch.Size([10])\n"
          ]
        }
      ],
      "source": [
        "for name, value in model.state_dict().items():\n",
        "    print(f\"{name} shape: {value.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IjyQ1934bKfo"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "Week3_4_assginment_ta.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "torch",
      "language": "python",
      "name": "torch"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}