{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ugGR7pnI4WSe"
      },
      "source": [
        "# Week3_1 Assignment\n",
        "\n",
        "## [BASIC](#Basic) \n",
        "- 토크나이징이 완료된 위키 백과 코퍼스를 다운받고 **단어 사전을 구축하는 함수를 구현**할 수 있다.\n",
        "- `Skip-Gram` 방식의 학습 데이터 셋을 생성하는 **Dataset과 Dataloader 클래스를 구현**할 수 있다.\n",
        "- **Negative Sampling** 함수를 구현할 수 있다. \n",
        "\n",
        "\n",
        "## [CHALLENGE](#Challenge)\n",
        "- Skip-Gram을 학습 과정 튜토리얼을 따라하며, **Skip-Gram을 학습하는 클래스를 구현**할 수 있다. \n",
        "\n",
        "\n",
        "## [ADVANCED](#Advanced)\n",
        "- Skip-Gram 방식으로 word embedding을 학습하는 **Word2Vec 클래스를 구현**하고 실제로 학습할 수 있다.\n",
        "- 학습이 완료된 word embedding을 불러와 **Gensim 패키지를 사용해 유사한 단어**를 뽑을 수 있다. \n",
        "\n",
        "### Reference\n",
        "- [Skip-Gram negative sampling 한국어 튜토리얼](https://wikidocs.net/69141)\n",
        "    - (참고) 위 튜토리얼에서는 target word와 context word 페어의 레이블은 1로, target word와 negative sample word 페어의 레이블은 0이 되도록 학습 데이터를 구현해 binary classification을 구현한다. 하지만 우리는 word2vec 논문 방식을 그대로 따르기 위해 label을 생성하지 않고 대신 loss 함수를 변행해서 binary classification을 학습할 것이다. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-02-19T14:29:36.641276Z",
          "start_time": "2022-02-19T14:29:36.638642Z"
        },
        "id": "HlEy3xfY4WSh"
      },
      "outputs": [],
      "source": [
        "import os \n",
        "import sys\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "from typing import List, Dict\n",
        "import random"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-02-19T12:50:41.644583Z",
          "start_time": "2022-02-19T12:50:41.642937Z"
        },
        "id": "cBrr7-gt4jnf"
      },
      "outputs": [],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-02-19T14:26:59.276355Z",
          "start_time": "2022-02-19T14:26:58.411434Z"
        },
        "id": "6mC9lhsJ4WSh"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.autograd import Variable\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.optim import SGD\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-02-19T14:30:05.586472Z",
          "start_time": "2022-02-19T14:30:05.583611Z"
        },
        "id": "17g7UZ5g4WSi"
      },
      "outputs": [],
      "source": [
        "# seed\n",
        "seed = 7777\n",
        "np.random.seed(seed)\n",
        "random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed_all(seed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-02-19T14:30:06.721039Z",
          "start_time": "2022-02-19T14:30:06.717559Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v3UlC7Jn4WSi",
        "outputId": "1b93f8e0-64c2-4dc3-a18a-7dc321cf1136"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# available GPUs : 1\n",
            "GPU name : Tesla K80\n",
            "cuda\n"
          ]
        }
      ],
      "source": [
        "# device type\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "    print(f\"# available GPUs : {torch.cuda.device_count()}\")\n",
        "    print(f\"GPU name : {torch.cuda.get_device_name()}\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k8sfv5KY4WSk"
      },
      "source": [
        "## Basic"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sHs8_LU04WSj"
      },
      "source": [
        "### 토크나이징이 완료된 위키 백과 코퍼스 다운로드 및 불용어 사전 크롤링\n",
        "- 나의 구글 드라이브에 데이터를 다운받아 영구적으로 사용할 수 있도록 하자. \n",
        "    - [데이터 다운로드 출처](https://ratsgo.github.io/embedding/downloaddata.html)\n",
        "- 다운받은 데이터는 토크나이징이 완료된 상태이지만 불용어를 포함하고 있다. 따라서 향후 불용어를 제거하기 위해 불용어 사전을 크롤링하자. \n",
        "    - [불용어 사전 출처](https://www.ranks.nl/stopwords/korean)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CGhLvEQECmWC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "67a9873e-6681-4c29-ad07-8ff434032657"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JYD_WjPYCmWC"
      },
      "outputs": [],
      "source": [
        "cd \"/content/drive/MyDrive/NLP강의/강의자료\" # 데이터 다운로드할 위치 입력"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-02-19T14:27:11.886643Z",
          "start_time": "2022-02-19T14:27:11.884858Z"
        },
        "id": "4QPBJ6UZ4WSj"
      },
      "outputs": [],
      "source": [
        "# 데이터 다운로드\n",
        "!pip install gdown\n",
        "!gdown https://drive.google.com/u/0/uc?id=1Ybp_DmzNEpsBrUKZ1-NoPDzCMO39f-fx\n",
        "!unzip tokenized.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-02-19T14:27:15.633947Z",
          "start_time": "2022-02-19T14:27:13.829982Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cTHCHmO24WSj",
        "outputId": "f54a0180-a089-4e8b-a442-63d4a3eb4672"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/urllib3/connectionpool.py:847: InsecureRequestWarning: Unverified HTTPS request is being made. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings\n",
            "  InsecureRequestWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# Korean stop words: 677\n"
          ]
        }
      ],
      "source": [
        "# 한국어 불용어 리스트 크롤링\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "url = \"https://www.ranks.nl/stopwords/korean\"\n",
        "response = requests.get(url, verify = False)\n",
        "\n",
        "if response.status_code == 200:\n",
        "    soup = BeautifulSoup(response.text,'html.parser')\n",
        "    content = soup.select_one('#article178ebefbfb1b165454ec9f168f545239 > div.panel-body > table > tbody > tr')\n",
        "    stop_words=[]\n",
        "    for x in content.strings:\n",
        "        x=x.strip()\n",
        "        if x:\n",
        "            stop_words.append(x)\n",
        "    print(f\"# Korean stop words: {len(stop_words)}\")\n",
        "else:\n",
        "    print(response.status_code)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-02-19T14:27:15.642775Z",
          "start_time": "2022-02-19T14:27:15.635333Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "3d0IqhDF4WSk",
        "outputId": "3b3f13c1-dbb8-4a27-ef5b-76de7d392490"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'아'"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "stop_words[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_t76Q1pQ4WSk"
      },
      "source": [
        "### 단어 사전 구축 함수 구현 \n",
        "- 문서 리스트를 입력 받아 사전을 생성하는 `make_vocab()` 함수를 구현하라.\n",
        "- 함수 정의\n",
        "    - 입력 매개변수\n",
        "        - docs : 문서 리스트\n",
        "        - min_count : 최소 단어 등장 빈도수 (단어 빈도가 `min_count` 미만인 단어는 사전에 포함하지 않음)\n",
        "    - 조건\n",
        "        - 문서 길이 제한\n",
        "            - 단어 개수가 3개 이하인 문서는 처리하지 않음. (skip)\n",
        "        - 사전에 포함되는 단어 빈도수 제한\n",
        "            - 단어가 빈도가 `min_count` 미만은 단어는 사전에 포함하지 않음.\n",
        "        - 불용어 제거 \n",
        "            - 불용어 리스트에 포함된 단어는 제거 \n",
        "    - 반환값 \n",
        "        - word2count : 단어별 빈도 사전 (key: 단어, value: 등장 횟수)\n",
        "        - wid2word : 단어별 인덱스(wid) 사전 (key: 단어 인덱스(int), value: 단어)\n",
        "        - word2wid : 인덱스(wid)별 단어 사전 (key: 단어, value: 단어 인덱스(int))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-02-19T14:33:01.637431Z",
          "start_time": "2022-02-19T14:32:56.730711Z"
        },
        "id": "xkjqztIA4WSl"
      },
      "outputs": [],
      "source": [
        "# 코퍼스 로드\n",
        "_DATA_DIR=\"/content/drive/MyDrive/NLP강의/강의자료/tokenized\"\n",
        "\n",
        "with open(os.path.join(_DATA_DIR, \"wiki_ko_mecab.txt\")) as reader:\n",
        "    docs = reader.readlines()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-02-19T14:33:03.423002Z",
          "start_time": "2022-02-19T14:33:03.419818Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WAKB6bbt4WSl",
        "outputId": "1d9192d7-cc63-4f1e-a937-d84693678b10"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# wiki documents: 311,237\n"
          ]
        }
      ],
      "source": [
        "print(f\"# wiki documents: {len(docs):,}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-02-19T14:33:04.016885Z",
          "start_time": "2022-02-19T14:33:03.962269Z"
        },
        "id": "-OI1MCXv4WSl",
        "outputId": "c2736bd2-159b-4a8a-a37d-2e166341c18d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "500"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ],
      "source": [
        "# 500개로 문서 개수를 줄임\n",
        "docs=random.sample(docs,500)\n",
        "len(docs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-02-19T14:33:26.392627Z",
          "start_time": "2022-02-19T14:33:26.382358Z"
        },
        "id": "V_R8kOo6CmWF",
        "outputId": "34460cce-e1df-4562-8ee7-cfed626405ee",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Check : 남모 공주  南 毛 公主  는 신라 의 공주  왕족 으로 법흥왕 과 보과 공주 부여 씨 의 딸 이 며 백제 동성왕 의 외손녀 였 다  경쟁자 인 준정 과 함께 신라 의 초대 여성 원화  화랑  였 다  그 가 준정 에게 암살 당한 것 을 계기 로 화랑 은 여성 이 아닌 남성 미소년 으로 선발 하 게 되 었 다  신라 진흥왕 에게 는 사촌 누나 이 자 이모 가 된다  신라 의 청소년 조직 이 었 던 화랑도 는 처음 에 는 남모  준정 두 미녀 를 뽑 아 이 를  원화 라 했으며 이 들 주위 에 는    여 명 의 무리 를 따르 게 하 였 다  그러나 준정 과 남모 는 서로 최고 가 되 고자 시기 하 였 다  준정 은 박영실 을 섬겼 는데  지소태후 는 자신 의 두 번 째 남편 이 기 도 한 그 를 싫어해서 준정 의 원화 를 없애 고 낭도 가 부족 한 남모 에게 위화랑 의 낭도 를 더 해 주 었 다  그 뒤 남모 는 준정 의 초대 로 그 의 집 에 갔 다가 억지로 권하 는 술 을 받아마시 고 취한 뒤 준정 에 의해 강물 에 던져져 살해 되 었 다  이 일 이 발각 돼 준 정도 사형 에 처해지 고 나라 에서 는 귀족 출신 의 잘 생기 고 품행 이 곧 은 남자 를 뽑 아 곱 게 단장 한 후 이 를 화랑 이 라 칭하 고 받들 게 하 였 다   부왕 신라 제   대 국왕 법흥왕 모후 보과 공주 부여 씨  宝 果 公主 扶餘 氏   공주 남모 공주 외조부 백제 제   대 국왕 동성왕 외조모 신라 이찬 비지 의 딸  화랑전사 마루        년  배우  박효빈  신라 법흥왕 백제 동성왕 준정 화랑 분류     년 죽음 분류  신라 의 왕녀 분류  신라 의 왕족 분류  화랑 분류  암살 된 사람 분류  독살 된 사람 분류  법흥왕\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# 문서 내 숫자, 영어 대소문자, 특수문자를 제거\n",
        "docs = [re.sub('[0-9a-zA-Z-=+,#/\\?:^$.@*\\\"※~&%ㆍ!』\\\\‘〈〉|\\(\\)\\[\\]\\<\\>`\\'…》《]','', doc) for doc in docs]\n",
        "print(f\"Check : {docs[0][:1000]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-02-19T14:33:27.904880Z",
          "start_time": "2022-02-19T14:33:27.899620Z"
        },
        "id": "OAkkQsvO4WSl"
      },
      "outputs": [],
      "source": [
        "def make_vocab(docs:List[str], min_count:int):\n",
        "    \"\"\"\n",
        "    'docs'문서 리스트를 입력 받아 단어 사전을 생성.\n",
        "    \n",
        "    return \n",
        "        - word2count : 단어별 빈도 사전\n",
        "        - wid2word : 단어별 인덱스(wid) 사전 \n",
        "        - word2wid : 인덱스(wid)별 단어 사전\n",
        "    \"\"\"\n",
        "\n",
        "    word2count = dict()\n",
        "    word2id = dict()\n",
        "    id2word = dict()\n",
        "\n",
        "    _word2count = dict() # 단어별 등장 빈도를 기록하기 위한 임시 딕셔너리 생성\n",
        "    for doc in tqdm(docs):\n",
        "        word_list = doc.split()\n",
        "        # 조건 1. 문서 길이 제한\n",
        "        if len(word_list)>3:\n",
        "            for word in word_list:\n",
        "                # 조건 2. 불용어 제거\n",
        "                if word in stop_words:\n",
        "                    continue\n",
        "                try:\n",
        "                    _word2count[word]+=1\n",
        "                except KeyError:\n",
        "                    _word2count[word]=1\n",
        "\n",
        "    # 조건 3. 토큰 최소 빈도를 만족하는 토큰만 사전(word2count)에 추가\n",
        "    idx=0\n",
        "    for w,c in _word2count.items():\n",
        "        if c<min_count:\n",
        "            continue\n",
        "        word2count[w] = c\n",
        "        word2id[w] = idx\n",
        "        id2word[idx] = w\n",
        "        idx+=1\n",
        "\n",
        "    \n",
        "    return word2count, word2id, id2word"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-02-19T14:33:30.157872Z",
          "start_time": "2022-02-19T14:33:28.473330Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ieS5SiQx4WSm",
        "outputId": "f76cf0a2-5676-444b-9b01-ac31d480b12c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 500/500 [00:02<00:00, 185.87it/s]\n"
          ]
        }
      ],
      "source": [
        "word2count, word2id, id2word = make_vocab(docs, min_count=5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-02-19T14:33:30.754722Z",
          "start_time": "2022-02-19T14:33:30.752115Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cT1MRN1EJtx6",
        "outputId": "9381d931-24f0-4b9c-a174-e7fb1e6149a3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "164,843\n"
          ]
        }
      ],
      "source": [
        "doc_len = sum(word2count.values()) # 문서 내 모든 단어의 개수 (unique한 단어 개수 * 단어 등장 빈도)\n",
        "print(f\"{doc_len:,}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-02-19T14:33:32.916830Z",
          "start_time": "2022-02-19T14:33:32.914355Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e_1MneB54WSm",
        "outputId": "8ce57158-7f85-4979-bab9-d127927fde71"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# unique word : 5,882\n"
          ]
        }
      ],
      "source": [
        "print(f\"# unique word : {len(word2id):,}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gHxtZqtk4WSm"
      },
      "source": [
        "### Dataset 클래스 구현\n",
        "- Skip-Gram 방식의 학습 데이터 셋(`Tuple(target_word, context_word)`)을 생성하는 `CustomDataset` 클래스를 구현하라.\n",
        "- 클래스 정의\n",
        "    - 생성자(`__init__()` 함수) 입력 매개변수\n",
        "        - docs: 문서 리스트\n",
        "        - word2id: 단어별 인덱스(wid) 사전\n",
        "        - window_size: Skip-Gram의 윈도우 사이즈\n",
        "    - 메소드\n",
        "        - `make_pair()`\n",
        "            - 문서를 단어로 쪼개고, 사전에 존재하는 단어들만 단어 인덱스로 변경\n",
        "            - Skip-gram 방식의 `(target_word, context_word)` 페어(tuple)들을 `pairs` 리스트에 담아 반환\n",
        "        - `__len__()`\n",
        "            - `pairs` 리스트의 개수 반환\n",
        "        - `__getitem__(index)`\n",
        "            - `pairs` 리스트를 인덱싱\n",
        "    - 주의 사항\n",
        "        - `nn.Module`를 부모 클래스로 상속 받음 \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-02-19T14:33:38.111290Z",
          "start_time": "2022-02-19T14:33:38.104531Z"
        },
        "id": "UPiLcYCZ4WSm"
      },
      "outputs": [],
      "source": [
        "class CustomDataset(Dataset):\n",
        "    \"\"\"\n",
        "    문서 리스트를 받아 skip-gram 방식의 (target_word, context_word) 데이터 셋을 생성\n",
        "    \"\"\"\n",
        "    def __init__(self, docs:List[str], word2id:Dict[str,int], window_size:int=5):\n",
        "        self.docs = docs\n",
        "        self.word2id = word2id\n",
        "        self.window_size = window_size\n",
        "        self.pairs = self.make_pair()\n",
        "    \n",
        "    def make_pair(self):\n",
        "        \"\"\"\n",
        "        (target, context) 형식의 Skip-gram pair 데이터 셋 생성 \n",
        "        \"\"\"\n",
        "        pairs = []\n",
        "        for doc in tqdm(self.docs):\n",
        "            word_ids = [] # 문서 내 (사전에 존재하는) 단어의 인덱스를 저장하기 위한 리스트 (=학습 데이터)\n",
        "            for word in doc.split():\n",
        "                # 1. 사전(\bword2id)에 존재하는 단어만 단어 인덱스(wid)로 변경해 학습 데이터에 추가\n",
        "                if self.word2id.get(word):\n",
        "                    word_ids.append(self.word2id.get(word))\n",
        "                else:\n",
        "                    continue\n",
        "\n",
        "            # 2. 학습 데이터 구축\n",
        "            for i, u in enumerate(word_ids):\n",
        "                for j in range(self.window_size):\n",
        "                    if i-1-j>=0: # target_word 기준 왼쪽 방향의 context_word들을 추가\n",
        "                        pairs.append((u, word_ids[i-1-j]))\n",
        "                    if i+1+j<len(word_ids): # target_word 기준 오른쪽 방향의 context_word들을 추가\n",
        "                        pairs.append((u, word_ids[i+1+j]))\n",
        "        return pairs\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.pairs)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        return self.pairs[idx]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-02-19T14:33:38.945361Z",
          "start_time": "2022-02-19T14:33:38.385577Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YntOw2q94WSm",
        "outputId": "0419e2fc-6af0-43ed-ab54-26ccffcb8fd4"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 500/500 [00:00<00:00, 932.95it/s]\n"
          ]
        }
      ],
      "source": [
        "dataset = CustomDataset(docs, word2id, window_size=5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-02-19T14:33:38.949614Z",
          "start_time": "2022-02-19T14:33:38.946663Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-RpNbAjk4WSn",
        "outputId": "d312287e-05a8-4eb5-addf-06949ac48eca"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "1374830"
            ]
          },
          "execution_count": 46,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-02-19T14:33:43.072635Z",
          "start_time": "2022-02-19T14:33:43.069526Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1FBwcL4H4WSn",
        "outputId": "f30f089c-8e76-4da4-aaf5-673f0d96e307"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(1, 2)"
            ]
          },
          "execution_count": 47,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dataset[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-02-19T14:33:51.040595Z",
          "start_time": "2022-02-19T14:33:51.031473Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wTAwTjKk4WSn",
        "outputId": "b0917894-d725-4678-ac65-564117436bee",
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(영국, 록)\n",
            "(영국, 밴드)\n",
            "(영국, 비틀즈)\n",
            "(영국, 첫)\n",
            "(영국, 번)\n",
            "(록, 영국)\n",
            "(록, 밴드)\n",
            "(록, 비틀즈)\n",
            "(록, 첫)\n",
            "(록, 번)\n",
            "(록, 째)\n",
            "(밴드, 록)\n",
            "(밴드, 비틀즈)\n",
            "(밴드, 영국)\n",
            "(밴드, 첫)\n",
            "(밴드, 번)\n",
            "(밴드, 째)\n",
            "(밴드, 싱글)\n",
            "(비틀즈, 밴드)\n",
            "(비틀즈, 첫)\n",
            "(비틀즈, 록)\n",
            "(비틀즈, 번)\n",
            "(비틀즈, 영국)\n",
            "(비틀즈, 째)\n",
            "(비틀즈, 싱글)\n",
            "(비틀즈, 며)\n",
            "(첫, 비틀즈)\n",
            "(첫, 번)\n",
            "(첫, 밴드)\n",
            "(첫, 째)\n",
            "(첫, 록)\n",
            "(첫, 싱글)\n",
            "(첫, 영국)\n",
            "(첫, 며)\n",
            "(첫, 은)\n",
            "(번, 첫)\n",
            "(번, 째)\n",
            "(번, 비틀즈)\n",
            "(번, 싱글)\n",
            "(번, 밴드)\n",
            "(번, 며)\n",
            "(번, 록)\n",
            "(번, 은)\n",
            "(번, 영국)\n",
            "(번, 다)\n",
            "(째, 번)\n",
            "(째, 싱글)\n",
            "(째, 첫)\n",
            "(째, 며)\n",
            "(째, 비틀즈)\n",
            "(째, 은)\n",
            "(째, 밴드)\n",
            "(째, 다)\n",
            "(째, 록)\n",
            "(째, 영국)\n",
            "(싱글, 째)\n",
            "(싱글, 며)\n",
            "(싱글, 번)\n",
            "(싱글, 은)\n",
            "(싱글, 첫)\n",
            "(싱글, 다)\n",
            "(싱글, 비틀즈)\n",
            "(싱글, 영국)\n",
            "(싱글, 밴드)\n",
            "(싱글, 발매되)\n",
            "(며, 싱글)\n",
            "(며, 은)\n",
            "(며, 째)\n",
            "(며, 다)\n",
            "(며, 번)\n",
            "(며, 영국)\n",
            "(며, 첫)\n",
            "(며, 발매되)\n",
            "(며, 비틀즈)\n",
            "(며, 었)\n",
            "(은, 며)\n",
            "(은, 다)\n",
            "(은, 싱글)\n",
            "(은, 영국)\n",
            "(은, 째)\n",
            "(은, 발매되)\n",
            "(은, 번)\n",
            "(은, 었)\n",
            "(은, 첫)\n",
            "(은, 위)\n",
            "(다, 은)\n",
            "(다, 영국)\n",
            "(다, 며)\n",
            "(다, 발매되)\n",
            "(다, 싱글)\n",
            "(다, 었)\n",
            "(다, 째)\n",
            "(다, 위)\n",
            "(다, 번)\n",
            "(다, 올랐)\n",
            "(영국, 다)\n",
            "(영국, 발매되)\n",
            "(영국, 은)\n",
            "(영국, 었)\n",
            "(영국, 며)\n"
          ]
        }
      ],
      "source": [
        "# verify (target word, context word)\n",
        "for i, pair in enumerate(dataset):\n",
        "    if i==100:\n",
        "        break\n",
        "    print(f\"({id2word[pair[0]]}, {id2word[pair[1]]})\")\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P0Z50-Dr4WSn"
      },
      "source": [
        "### 위에서 생성한 `dataset`으로 DataLoader  객체 생성\n",
        "- `DataLoader` 클래스로 `train_dataloader`객체를 생성하라. \n",
        "    - 생성자 매개변수와 값\n",
        "        - dataset = 위에서 생성한 dataset\n",
        "        - batch_size = 64\n",
        "        - shuffle = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-02-19T14:34:02.645176Z",
          "start_time": "2022-02-19T14:34:02.642780Z"
        },
        "id": "GXcAvFB14WSn"
      },
      "outputs": [],
      "source": [
        "train_dataloader = DataLoader(\n",
        "    dataset, batch_size=64, shuffle=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-02-19T14:34:02.777322Z",
          "start_time": "2022-02-19T14:34:02.774335Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Yfcwi_14WSn",
        "outputId": "99fc1870-aa25-47b3-e0fb-ffa0b00933a5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "21482"
            ]
          },
          "execution_count": 50,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(train_dataloader)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eTs16gsU4WSn"
      },
      "source": [
        "### Negative Sampling 함수 구현\n",
        "- Skip-Gram은 복잡도를 줄이기 위한 방법으로 negative sampling을 사용한다. \n",
        "- `sample_table`이 다음과 같이 주어졌을 때, sample_table에서 랜덤으로 값을 뽑아 (batch_size, n_neg_sample) shape의 matrix를 반환하는 `get_neg_v_negative_sampling()`함수를 구현하라. \n",
        "- Sample Table은 negative distribution을 따른다. \n",
        "    - [negative distribution 설명](https://aegis4048.github.io/optimize_computational_efficiency_of_skip-gram_with_negative_sampling#How-are-negative-samples-drawn?)\n",
        "- 함수 정의\n",
        "    - 입력 매개변수\n",
        "        - batch_size : 배치 사이즈, matrix의 row 개수 \n",
        "        - n_neg_sample : negative sample의 개수, matrix의 column 개수\n",
        "    - 반환값 \n",
        "        - neg_v : 추출된 negative sample (2차원의 리스트)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-02-19T14:34:11.397509Z",
          "start_time": "2022-02-19T14:34:11.386389Z"
        },
        "id": "PUqIB6dH4WSn",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# negative sample을 추출할 sample table 생성 (해당 코드를 참고)\n",
        "sample_table = []\n",
        "sample_table_size = doc_len\n",
        "\n",
        "# noise distribution 생성\n",
        "alpha = 3/4\n",
        "frequency_list = np.array(list(word2count.values())) ** alpha\n",
        "Z = sum(frequency_list)\n",
        "ratio = frequency_list/Z\n",
        "negative_sample_dist = np.round(ratio*sample_table_size)\n",
        "\n",
        "for wid, c in enumerate(negative_sample_dist):\n",
        "    sample_table.extend([wid]*int(c))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-02-19T14:34:11.508414Z",
          "start_time": "2022-02-19T14:34:11.505464Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wdu8qK8x4WSn",
        "outputId": "f2e0249e-a155-40ef-de5f-41fae6907fe3"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "143880"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ],
      "source": [
        "len(sample_table)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-02-19T14:34:11.656046Z",
          "start_time": "2022-02-19T14:34:11.653325Z"
        },
        "id": "mQIVrOIR4WSn"
      },
      "outputs": [],
      "source": [
        "def get_neg_v_negative_sampling(batch_size:int, n_neg_sample:int):\n",
        "    \"\"\"\n",
        "    (batch_size, n_neg_sample) shape의 네거티브 샘플 메트릭스 생성\n",
        "    \"\"\"\n",
        "    neg_v = np.random.choice(\n",
        "        sample_table, size=(batch_size, n_neg_sample)\n",
        "    ).tolist()\n",
        "    return neg_v"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-02-19T14:34:12.345976Z",
          "start_time": "2022-02-19T14:34:12.333448Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8wwT4Af04WSo",
        "outputId": "e4d2b67c-7692-4903-aac6-c2249b5525e8"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[52, 5544, 62, 3648, 2568],\n",
              " [394, 5247, 480, 4551, 1700],\n",
              " [340, 272, 86, 4529, 2120],\n",
              " [2025, 1128, 4625, 1168, 122]]"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ],
      "source": [
        "get_neg_v_negative_sampling(4, 5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nLnDXPvJ4WSo"
      },
      "source": [
        "## Challenge"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B5UubCzK4WSo"
      },
      "source": [
        "### 미니 튜토리얼\n",
        "- 아래 튜토리얼을 따라하며 Skip-Gram 모델의 `forward` 및 `loss` 연산 방식을 이해하자\n",
        "- Reference\n",
        "    - [torch.nn.Embedding](https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html)\n",
        "    - [torch bmm](https://pytorch.org/docs/stable/generated/torch.bmm.html)\n",
        "    - [Skip-Gram negative sampling loss function 설명 영문 블로그](https://aegis4048.github.io/optimize_computational_efficiency_of_skip-gram_with_negative_sampling#Derivation-of-Cost-Function-in-Negative-Sampling)\n",
        "    - [Skip-Gram negative sampling loss function 설명 한글 블로그](https://reniew.github.io/22/)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-02-19T12:51:46.954048Z",
          "start_time": "2022-02-19T12:51:46.951529Z"
        },
        "id": "IAR68hsY4WSo"
      },
      "outputs": [],
      "source": [
        "# hyper parameter example\n",
        "emb_size = 30000 # vocab size\n",
        "emb_dimension = 300 # word embedding 차원\n",
        "n_neg_sample = 5\n",
        "batch_size = 32"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-02-19T12:51:49.340056Z",
          "start_time": "2022-02-19T12:51:47.300999Z"
        },
        "id": "zzOsVUn94WSo"
      },
      "outputs": [],
      "source": [
        "# 1. Embedding Matrix와 Context Matrix를 생성\n",
        "u_embedding = nn.Embedding(emb_size, emb_dimension, sparse=True).to(device)\n",
        "v_embedding = nn.Embedding(emb_size, emb_dimension, sparse=True).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-02-19T12:51:49.352240Z",
          "start_time": "2022-02-19T12:51:49.341437Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I7J_ADc44WSo",
        "outputId": "c2077e47-89c7-4a56-ccc6-3b4ca9947c8b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Target word idx : tensor([16006, 15856, 22769, 21999, 17424, 13754, 17522, 24105,   749, 26012,\n",
            "         7080, 25945, 27692,  7803, 24946,  4086,  5704, 23590,  5156, 14020,\n",
            "        19283,  4554, 25031,  3852,  9993,   927,  6887,  1212, 23439,  8402,\n",
            "         8949,  3589]) Pos context word idx : tensor([26604,  7228, 21106,  2445, 22561, 20317,   520, 23209, 24259,  2540,\n",
            "        18154, 13936,  9540,    19, 18001, 29570,  2955, 13281,   440,   406,\n",
            "         5808,  7510,  3181, 28899, 29524,  1703, 18395, 12983, 22293, 28641,\n",
            "        16214,  5538]) Neg context word idx : [[823, 1301, 5312, 1865, 821], [1946, 986, 2513, 659, 2387], [5086, 1904, 468, 86, 5043], [3311, 22, 1325, 706, 896], [5298, 814, 3826, 1689, 1336], [3599, 3356, 924, 3986, 132], [5015, 363, 158, 3237, 432], [1401, 3709, 691, 151, 4815], [1852, 981, 2022, 1130, 2436], [4133, 601, 1702, 4309, 444], [2304, 606, 99, 813, 5280], [3305, 5131, 61, 2040, 211], [701, 2695, 5814, 1391, 5090], [5556, 1286, 3795, 1396, 141], [1784, 1156, 4657, 2503, 124], [2118, 433, 1786, 5216, 2151], [189, 3219, 814, 2756, 5230], [3931, 3064, 1935, 1575, 1881], [4745, 96, 1546, 99, 49], [4529, 2416, 301, 2441, 1256], [1046, 5590, 2985, 1648, 1210], [2006, 4598, 3978, 1960, 4220], [215, 2555, 4205, 2364, 54], [1205, 4780, 5320, 207, 361], [3450, 3954, 3347, 311, 1830], [3407, 27, 3410, 4612, 1335], [4133, 5511, 412, 183, 3156], [40, 1927, 2404, 5507, 54], [22, 1204, 464, 1571, 1976], [330, 5409, 3248, 437, 356], [69, 567, 1801, 24, 392], [356, 419, 4861, 4148, 3332]]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# 2. wid(단어 인덱스)를 임의로 생성\n",
        "pos_u = torch.randint(high = emb_size, size = (batch_size,))\n",
        "pos_v = torch.randint(high = emb_size, size = (batch_size,))\n",
        "neg_v = get_neg_v_negative_sampling(batch_size, n_neg_sample)\n",
        "print(f\"Target word idx : {pos_u} Pos context word idx : {pos_v} Neg context word idx : {neg_v}\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-02-19T12:51:49.364020Z",
          "start_time": "2022-02-19T12:51:49.353486Z"
        },
        "id": "4iEG0nCZ4WSo"
      },
      "outputs": [],
      "source": [
        "# 3. tensor로 변환\n",
        "pos_u = Variable(torch.LongTensor(pos_u)).to(device)\n",
        "pos_v = Variable(torch.LongTensor(pos_v)).to(device)\n",
        "neg_v = Variable(torch.LongTensor(neg_v)).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-02-19T12:51:51.391896Z",
          "start_time": "2022-02-19T12:51:51.387084Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gqbNbajG4WSo",
        "outputId": "f912bb66-5615-4ecc-fc8e-8fafce483b10"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "shape of pos_u embedding : torch.Size([32, 300])\n",
            " shape of pos_v embedding : torch.Size([32, 300])\n",
            " shape of neg_v embedding : torch.Size([32, 5, 300])\n"
          ]
        }
      ],
      "source": [
        "# 4. wid로 각각의 embedding matrix에서 word embedding 값을 가져오기\n",
        "pos_u = u_embedding(pos_u)\n",
        "pos_v = v_embedding(pos_v)\n",
        "neg_v = v_embedding(neg_v)\n",
        "print(f\"shape of pos_u embedding : {pos_u.shape}\\n shape of pos_v embedding : {pos_v.shape}\\n shape of neg_v embedding : {neg_v.shape}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-02-19T12:51:53.121477Z",
          "start_time": "2022-02-19T12:51:52.646148Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uDWUrSwo4WSo",
        "outputId": "bcc0ff2f-2be2-4bfe-8aa7-d1e19b6f4375"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([32, 300])\n",
            "shape of pos logits : torch.Size([32])\n",
            "\n",
            "torch.Size([32, 5, 300])\n",
            "torch.Size([32, 300])\n",
            "torch.Size([32, 300, 1])\n",
            "torch.Size([32, 5, 1])\n",
            "shape of logits : torch.Size([32, 5])\n"
          ]
        }
      ],
      "source": [
        "# 5. dot product \n",
        "pos_score = torch.mul(pos_u, pos_v) # 행렬 element-wise 곱 (= row 곱 )\n",
        "print(pos_score.shape)\n",
        "pos_score = torch.sum(pos_score, dim=1)\n",
        "print(f\"shape of pos logits : {pos_score.shape}\\n\")\n",
        "\n",
        "print(neg_v.shape) # 3d tensor (b,n,m)\n",
        "print(pos_u.shape)\n",
        "print(pos_u.unsqueeze(dim=2).shape) # last axis에 1차원을 추가 (b,m,p)\n",
        "neg_score = torch.bmm(neg_v, pos_u.unsqueeze(dim=2)) # batch-matrix-matrix multiplication output (b,n,p)\n",
        "print(neg_score.shape)\n",
        "neg_score = neg_score.squeeze() \n",
        "print(f\"shape of logits : {neg_score.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-02-19T12:51:53.670418Z",
          "start_time": "2022-02-19T12:51:53.665671Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "adOpcoL54WSo",
        "outputId": "74ac29ee-ce2b-4e54-9182-be0f5189f49c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "pos logits : -241.4199676513672\n",
            "neg logits : -1060.181396484375\n",
            "Loss : 1301.601318359375\n"
          ]
        }
      ],
      "source": [
        "# 6. loss 구하기\n",
        "pos_score = F.logsigmoid(pos_score)\n",
        "neg_score = F.logsigmoid(-1*neg_score) # negative의 logit은 minimize 하기 위해 -1 곱함\n",
        "print(f\"pos logits : {pos_score.sum()}\")\n",
        "print(f\"neg logits : {neg_score.sum()}\")\n",
        "loss = -1 * (torch.sum(pos_score) + torch.sum(neg_score))\n",
        "print(f\"Loss : {loss}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "muEceOGZ4WSo"
      },
      "source": [
        "### Skip-gram 클래스 구현\n",
        "- Skip-Gram 방식으로 단어 embedding을 학습하는 `SkipGram` 클래스를 구현하라.\n",
        "- 클래스 정의\n",
        "    - 생성자(`__init__()` 함수) 입력 매개변수\n",
        "        - `vocab_size` : 사전내 단어 개수\n",
        "        - `emb_dimension` : 엠베딩 크기\n",
        "        - `device` : 연산 장치 종류\n",
        "    - 생성자에서 생성해야할 변수 \n",
        "        - `vocab_size` : 사전내 단어 개수\n",
        "        - `emb_dimension` : 엠베딩 크기\n",
        "        - `u_embedding` : (vocab_size, emb_dimension) 엠베딩 메트릭스 (target_word)\n",
        "        - `v_embedding` : (vocab_size, emb_dimension) 엠베딩 메트릭스 (context_word)\n",
        "    - 메소드\n",
        "        - `init_embedding()`\n",
        "            - 엠베딩 메트릭스 값을 초기화\n",
        "        - `forward()`\n",
        "            - 위 튜토리얼과 같이 dot product를 수행한 후 score를 생성\n",
        "            - loss를 반환 (loss 설명 추가)\n",
        "        - `save_emedding()`\n",
        "            - `u_embedding`의 단어 엠베딩 값을 단어 별로 파일에 저장\n",
        "    - 주의 사항     \n",
        "        - `nn.Module`를 부모 클래스로 상속 받음 "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-02-19T14:34:15.731306Z",
          "start_time": "2022-02-19T14:34:15.721129Z"
        },
        "id": "pnmMamP44WSo"
      },
      "outputs": [],
      "source": [
        "class SkipGram(nn.Module):\n",
        "    def __init__(self, vocab_size:int, emb_dimension:int, device:str):\n",
        "        super(SkipGram, self).__init__()\n",
        "        self.vocab_size = vocab_size\n",
        "        self.emb_dimension = emb_dimension\n",
        "        self.u_embedding = nn.Embedding(vocab_size, emb_dimension, sparse=True).to(device)\n",
        "        self.v_embedding = nn.Embedding(vocab_size, emb_dimension, sparse=True).to(device)\n",
        "        self.init_embedding()\n",
        "    \n",
        "    \n",
        "    def init_embedding(self):\n",
        "        \"\"\"\n",
        "        u_embedding과 v_embedding 메트릭스 값을 초기화\n",
        "        \"\"\"\n",
        "        initrange = 0.5 / self.emb_dimension\n",
        "        self.u_embedding.weight.data.uniform_(-initrange, initrange)\n",
        "        self.v_embedding.weight.data.uniform_(-0, 0)\n",
        "    \n",
        "    \n",
        "    def forward(self, pos_u, pos_v, neg_v):\n",
        "        \"\"\"\n",
        "        ...\n",
        "        \"\"\"    \n",
        "            \n",
        "        pos_u = self.u_embedding(pos_u)\n",
        "        pos_v = self.v_embedding(pos_v)\n",
        "        neg_v = self.v_embedding(neg_v)\n",
        "        \n",
        "        pos_score = torch.mul(pos_u, pos_v)\n",
        "        pos_score = torch.sum(pos_score, dim=1)\n",
        "        pos_score = F.logsigmoid(pos_score)\n",
        "        neg_score = torch.bmm(neg_v, pos_u.unsqueeze(dim=2)).squeeze()\n",
        "        neg_score = F.logsigmoid(-1 * neg_score)\n",
        "        \n",
        "        loss = -1 * (torch.sum(pos_score) + torch.sum(neg_score))\n",
        "        return loss\n",
        "    \n",
        "    def save_embedding(self, id2word, file_name, use_cuda):\n",
        "        \"\"\"\n",
        "        'file_name' 위치에 word와 word_embedding을 line-by로 저장\n",
        "        \"\"\"\n",
        "        if use_cuda: # parameter를 gpu 메모리에서 cpu 메모리로 옮김\n",
        "            embedding = self.u_embedding.weight.cpu().data.numpy()\n",
        "        else:\n",
        "            embedding = self.u_embedding.weight.data.numpy()\n",
        "        \n",
        "        with open(file_name, 'w') as writer:\n",
        "            # 파일의 첫 줄은 '단어 개수' 그리고 '단어 embedding 사이즈' 값을 입력해야 함\n",
        "            writer.write(f\"{len(id2word)} {embedding.shape[-1]}\\n\")\n",
        "            \n",
        "            for wid, word in id2word.items():\n",
        "                e = embedding[wid]\n",
        "                e = \" \".join([str(e_) for e_ in e])\n",
        "                writer.write(f\"{word} {e}\\n\")\n",
        "                "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RqqMo0zL4WSo"
      },
      "source": [
        "## Advanced"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wSWd5gV24WSp"
      },
      "source": [
        "### Skip-Gram 방식의  Word2Vec 클래스 구현\n",
        "- Skip-Gram 방식으로 단어 embedding을 학습하는 `Word2Vec` 클래스를 구현하라.\n",
        "- 클래스 정의\n",
        "    - 생성자(`__init__()`) 입력 매개 변수\n",
        "        - `input_file` : 학습할 문서 리스트\n",
        "        - `output_file_name` : 학습된 word embedding을 저장할 파일 위치\n",
        "        - `device` : 연상 장치 종류\n",
        "        - `emb_dimension` : word embedding 차원\n",
        "        - `batch_size` : 학습 배치 사이즈\n",
        "        - `window_size` : skip-gram 윈도우 사이즈 (context word 개수를 결정)\n",
        "        - `n_neg_sample` : negative sample 개수\n",
        "        - `iteration` : 학습 반복 횟수\n",
        "        - `lr` : learning rate\n",
        "        - `min_count` : 사전에 추가될 단어의 최소 등장 빈도\n",
        "    - 생성자에서 생성해야 할 변수 \n",
        "        - `docs` : 학습할 문서 리스트\n",
        "        - `output_file_name` : 학습된 word embedding을 저장할 파일 위치\n",
        "        - `word2count`, `word2id`, `id2word` : 위에서 구현한 `make_vocab()` 함수의 반환 값\n",
        "        - `device` : 연산 장치 종류\n",
        "        - `emb_size` : vocab의 (unique한) 단어 종류 \n",
        "        - `emb_dimension` : word embedding 차원\n",
        "        - `batch_size` : 학습 배치 사이즈\n",
        "        - `window_size` : skip-gram 윈도우 사이즈 (context word 개수를 결정)\n",
        "        - `n_neg_sample` : negative sample 개수\n",
        "        - `iteration` : 학습 반복 횟수\n",
        "        - `lr` : learning rate\n",
        "        - `model` : `SkipGram` 클래스의 인스턴스\n",
        "        - `optimizer` : `SGD` 클래스의 인스턴스\n",
        "    - 메소드\n",
        "        - `train()`\n",
        "            - 입력 매개변수 \n",
        "                - `train_dataloader`\n",
        "            - Iteration 횟수만큼 input_file 학습 데이터를 학습한다. 매 epoch마다 for loop 돌면서 batch 단위 학습 데이터를 skip gram 모델에 학습함. 학습이 끝나면 word embedding을 output_file_name 파일에 저장.\n",
        "- Reference\n",
        "    - [Optimizer - SGD](https://pytorch.org/docs/stable/generated/torch.optim.SGD.html)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-02-19T14:34:20.503555Z",
          "start_time": "2022-02-19T14:34:20.491585Z"
        },
        "id": "Td-GQrqI4WSp"
      },
      "outputs": [],
      "source": [
        "class Word2Vec:\n",
        "    def __init__(self, \n",
        "                input_file: List[str],\n",
        "                output_file_name: str,\n",
        "                 device: str,\n",
        "                 emb_dimension=300,\n",
        "                 batch_size = 64,\n",
        "                 window_size=5,\n",
        "                 n_neg_sample = 5,\n",
        "                 iteration=1,\n",
        "                 lr = 0.02,\n",
        "                 min_count=5):\n",
        "        self.docs = input_file\n",
        "        self.output_file_name = output_file_name\n",
        "        self.word2count, self.word2id, self.id2word = make_vocab(self.docs, min_count=min_count)\n",
        "        self.device = device\n",
        "        self.emb_size = len(self.word2id)\n",
        "        self.emb_dimension = emb_dimension\n",
        "        self.batch_size =batch_size\n",
        "        self.window_size = window_size\n",
        "        self.n_neg_sample = n_neg_sample\n",
        "        self.iteration = iteration\n",
        "        self.lr = lr\n",
        "        self.model = SkipGram(self.emb_size, self.emb_dimension, self.device)\n",
        "        self.optimizer = SGD(\n",
        "            self.model.parameters(),\n",
        "            lr = self.lr\n",
        "        )\n",
        "        # 파일 저장할 폴더 생성\n",
        "        os.makedirs(self.output_file_name, exist_ok=True)\n",
        "        \n",
        "    \n",
        "    def train(self, train_dataloader):\n",
        "        \n",
        "        # lr 값을 조절하는 스케줄러 인스턴스 변수를 생성\n",
        "        self.scheduler = get_linear_schedule_with_warmup(\n",
        "            self.optimizer,\n",
        "            num_warmup_steps=0,\n",
        "            num_training_steps=len(train_dataloader)*self.iteration\n",
        "        )\n",
        "        \n",
        "        for epoch in range(self.iteration):\n",
        "            \n",
        "            print(f\"*****Epoch {epoch} Train Start*****\")\n",
        "            print(f\"*****Epoch {epoch} Total Step {len(train_dataloader)}*****\")\n",
        "            total_loss, batch_loss, batch_step = 0,0,0\n",
        "\n",
        "            for step, batch in enumerate(train_dataloader):\n",
        "                batch_step+=1\n",
        "\n",
        "                pos_u, pos_v = batch\n",
        "                # negative data 생성\n",
        "                # neg_v = get_neg_v_negative_sampling(self.batch_size, self.n_neg_sample)\n",
        "                neg_v = get_neg_v_negative_sampling(pos_u.shape[0], self.n_neg_sample)\n",
        "                \n",
        "                # 데이터를 tensor화 & device 설정\n",
        "                pos_u = Variable(torch.LongTensor(pos_u)).to(self.device)\n",
        "                pos_v = Variable(torch.LongTensor(pos_v)).to(self.device)\n",
        "                neg_v = Variable(torch.LongTensor(neg_v)).to(self.device)\n",
        "\n",
        "                # gradient 초기화\n",
        "                self.optimizer.zero_grad()\n",
        "                self.model.zero_grad()\n",
        "\n",
        "                # forward\n",
        "                loss = self.model.forward(pos_u, pos_v, neg_v)\n",
        "\n",
        "                # loss\n",
        "                loss.backward()\n",
        "                self.optimizer.step()\n",
        "                self.scheduler.step()\n",
        "\n",
        "                batch_loss += loss.item()\n",
        "                total_loss += loss.item()\n",
        "                \n",
        "                if (step%500 == 0) and (step!=0):\n",
        "                    print(f\"Step: {step} Loss: {batch_loss/batch_step:.4f} lr: {self.optimizer.param_groups[0]['lr']:.4f}\")\n",
        "                    # 변수 초기화    \n",
        "                    batch_loss, batch_step = 0,0\n",
        "            \n",
        "            print(f\"Epoch {epoch} Total Mean Loss : {total_loss/(step+1):.4f}\")\n",
        "            print(f\"*****Epoch {epoch} Train Finished*****\\n\")\n",
        "            \n",
        "            print(f\"*****Epoch {epoch} Saving Embedding...*****\")\n",
        "            self.model.save_embedding(self.id2word, os.path.join(self.output_file_name, f'w2v_{epoch}.txt'), True if 'cuda' in self.device.type else False)\n",
        "            print(f\"*****Epoch {epoch} Embedding Saved at {os.path.join(self.output_file_name, f'w2v_{epoch}.txt')}*****\\n\")\n",
        "                    \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-02-19T14:34:29.561892Z",
          "start_time": "2022-02-19T14:34:26.103659Z"
        },
        "id": "Ywx9R8n24WSp",
        "outputId": "805ed831-8ed6-4bf4-cfd2-ee9e7436e2dc"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 500/500 [00:01<00:00, 297.95it/s]\n"
          ]
        }
      ],
      "source": [
        "# Word2Vec 클래스의 인스턴스 생성\n",
        "output_file = os.path.join(\".\", \"word2vec_wiki\")\n",
        "w2v = Word2Vec(docs, output_file, device, n_neg_sample=10, iteration=3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-02-19T14:34:34.615469Z",
          "start_time": "2022-02-19T14:34:34.055502Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ufBxjKxN4WSp",
        "outputId": "a2c60034-f27a-4bcf-d376-511404d7ff98"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 500/500 [00:00<00:00, 956.28it/s] \n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "21482"
            ]
          },
          "execution_count": 58,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# 학습 데이터 셋 및 데이터 로더 생성\n",
        "dataset = CustomDataset(w2v.docs, w2v.word2id, w2v.window_size)\n",
        "train_dataloader = DataLoader(dataset, w2v.batch_size)\n",
        "len(train_dataloader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-02-19T14:45:38.362817Z",
          "start_time": "2022-02-19T14:34:37.382371Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "9JBUrUJ34WSp",
        "outputId": "6c9d2d38-0d46-4628-bd8d-43fb40a781da"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "*****Epoch 0 Train Start*****\n",
            "*****Epoch 0 Total Step 21482*****\n",
            "Step: 500 Loss: 484.5451 lr: 0.0198\n",
            "Step: 1000 Loss: 395.7239 lr: 0.0197\n",
            "Step: 1500 Loss: 248.9553 lr: 0.0195\n",
            "Step: 2000 Loss: 200.7881 lr: 0.0194\n",
            "Step: 2500 Loss: 119.2248 lr: 0.0192\n",
            "Step: 3000 Loss: 104.1140 lr: 0.0191\n",
            "Step: 3500 Loss: 229.0389 lr: 0.0189\n",
            "Step: 4000 Loss: 235.0788 lr: 0.0188\n",
            "Step: 4500 Loss: 237.9144 lr: 0.0186\n",
            "Step: 5000 Loss: 212.8852 lr: 0.0184\n",
            "Step: 5500 Loss: 192.6870 lr: 0.0183\n",
            "Step: 6000 Loss: 209.6250 lr: 0.0181\n",
            "Step: 6500 Loss: 215.1007 lr: 0.0180\n",
            "Step: 7000 Loss: 164.4239 lr: 0.0178\n",
            "Step: 7500 Loss: 170.4262 lr: 0.0177\n",
            "Step: 8000 Loss: 187.0456 lr: 0.0175\n",
            "Step: 8500 Loss: 204.3405 lr: 0.0174\n",
            "Step: 9000 Loss: 194.8780 lr: 0.0172\n",
            "Step: 9500 Loss: 205.0695 lr: 0.0171\n",
            "Step: 10000 Loss: 186.4944 lr: 0.0169\n",
            "Step: 10500 Loss: 185.5174 lr: 0.0167\n",
            "Step: 11000 Loss: 197.1707 lr: 0.0166\n",
            "Step: 11500 Loss: 194.6444 lr: 0.0164\n",
            "Step: 12000 Loss: 188.4481 lr: 0.0163\n",
            "Step: 12500 Loss: 197.3222 lr: 0.0161\n",
            "Step: 13000 Loss: 194.7771 lr: 0.0160\n",
            "Step: 13500 Loss: 195.8214 lr: 0.0158\n",
            "Step: 14000 Loss: 185.1680 lr: 0.0157\n",
            "Step: 14500 Loss: 189.6969 lr: 0.0155\n",
            "Step: 15000 Loss: 186.8213 lr: 0.0153\n",
            "Step: 15500 Loss: 190.8038 lr: 0.0152\n",
            "Step: 16000 Loss: 170.4494 lr: 0.0150\n",
            "Step: 16500 Loss: 192.8111 lr: 0.0149\n",
            "Step: 17000 Loss: 184.9003 lr: 0.0147\n",
            "Step: 17500 Loss: 185.6305 lr: 0.0146\n",
            "Step: 18000 Loss: 185.8654 lr: 0.0144\n",
            "Step: 18500 Loss: 189.6958 lr: 0.0143\n",
            "Step: 19000 Loss: 173.8700 lr: 0.0141\n",
            "Step: 19500 Loss: 189.8594 lr: 0.0139\n",
            "Step: 20000 Loss: 192.2000 lr: 0.0138\n",
            "Step: 20500 Loss: 198.4497 lr: 0.0136\n",
            "Step: 21000 Loss: 187.7875 lr: 0.0135\n",
            "Epoch 0 Total Mean Loss : 202.9676\n",
            "*****Epoch 0 Train Finished*****\n",
            "\n",
            "*****Epoch 0 Saving Embedding...*****\n",
            "*****Epoch 0 Embedding Saved at ./word2vec_wiki/w2v_0.txt*****\n",
            "\n",
            "*****Epoch 1 Train Start*****\n",
            "*****Epoch 1 Total Step 21482*****\n",
            "Step: 500 Loss: 193.4911 lr: 0.0132\n",
            "Step: 1000 Loss: 187.1919 lr: 0.0130\n",
            "Step: 1500 Loss: 149.1231 lr: 0.0129\n",
            "Step: 2000 Loss: 173.5729 lr: 0.0127\n",
            "Step: 2500 Loss: 111.5738 lr: 0.0126\n",
            "Step: 3000 Loss: 106.5843 lr: 0.0124\n",
            "Step: 3500 Loss: 186.4074 lr: 0.0122\n",
            "Step: 4000 Loss: 194.9714 lr: 0.0121\n",
            "Step: 4500 Loss: 195.0088 lr: 0.0119\n",
            "Step: 5000 Loss: 188.9668 lr: 0.0118\n",
            "Step: 5500 Loss: 178.8818 lr: 0.0116\n",
            "Step: 6000 Loss: 191.7482 lr: 0.0115\n",
            "Step: 6500 Loss: 197.7579 lr: 0.0113\n",
            "Step: 7000 Loss: 168.4209 lr: 0.0112\n",
            "Step: 7500 Loss: 168.6567 lr: 0.0110\n",
            "Step: 8000 Loss: 181.6353 lr: 0.0109\n",
            "Step: 8500 Loss: 196.0211 lr: 0.0107\n",
            "Step: 9000 Loss: 191.4741 lr: 0.0105\n",
            "Step: 9500 Loss: 197.6260 lr: 0.0104\n",
            "Step: 10000 Loss: 185.5579 lr: 0.0102\n",
            "Step: 10500 Loss: 182.2152 lr: 0.0101\n",
            "Step: 11000 Loss: 189.9476 lr: 0.0099\n",
            "Step: 11500 Loss: 189.7585 lr: 0.0098\n",
            "Step: 12000 Loss: 184.6092 lr: 0.0096\n",
            "Step: 12500 Loss: 195.0764 lr: 0.0095\n",
            "Step: 13000 Loss: 191.4367 lr: 0.0093\n",
            "Step: 13500 Loss: 192.0262 lr: 0.0091\n",
            "Step: 14000 Loss: 183.4224 lr: 0.0090\n",
            "Step: 14500 Loss: 187.3478 lr: 0.0088\n",
            "Step: 15000 Loss: 187.1950 lr: 0.0087\n",
            "Step: 15500 Loss: 192.7276 lr: 0.0085\n",
            "Step: 16000 Loss: 171.3071 lr: 0.0084\n",
            "Step: 16500 Loss: 192.0430 lr: 0.0082\n",
            "Step: 17000 Loss: 187.0110 lr: 0.0081\n",
            "Step: 17500 Loss: 188.1948 lr: 0.0079\n",
            "Step: 18000 Loss: 186.4071 lr: 0.0077\n",
            "Step: 18500 Loss: 191.9085 lr: 0.0076\n",
            "Step: 19000 Loss: 178.3674 lr: 0.0074\n",
            "Step: 19500 Loss: 189.2011 lr: 0.0073\n",
            "Step: 20000 Loss: 193.7389 lr: 0.0071\n",
            "Step: 20500 Loss: 195.9125 lr: 0.0070\n",
            "Step: 21000 Loss: 189.5715 lr: 0.0068\n",
            "Epoch 1 Total Mean Loss : 182.6888\n",
            "*****Epoch 1 Train Finished*****\n",
            "\n",
            "*****Epoch 1 Saving Embedding...*****\n",
            "*****Epoch 1 Embedding Saved at ./word2vec_wiki/w2v_1.txt*****\n",
            "\n",
            "*****Epoch 2 Train Start*****\n",
            "*****Epoch 2 Total Step 21482*****\n",
            "Step: 500 Loss: 193.3081 lr: 0.0065\n",
            "Step: 1000 Loss: 187.5862 lr: 0.0064\n",
            "Step: 1500 Loss: 149.0169 lr: 0.0062\n",
            "Step: 2000 Loss: 181.7055 lr: 0.0060\n",
            "Step: 2500 Loss: 125.2737 lr: 0.0059\n",
            "Step: 3000 Loss: 120.3993 lr: 0.0057\n",
            "Step: 3500 Loss: 187.0557 lr: 0.0056\n",
            "Step: 4000 Loss: 199.2047 lr: 0.0054\n",
            "Step: 4500 Loss: 198.6556 lr: 0.0053\n",
            "Step: 5000 Loss: 194.3407 lr: 0.0051\n",
            "Step: 5500 Loss: 186.2842 lr: 0.0050\n",
            "Step: 6000 Loss: 195.1073 lr: 0.0048\n",
            "Step: 6500 Loss: 203.8314 lr: 0.0046\n",
            "Step: 7000 Loss: 183.8470 lr: 0.0045\n",
            "Step: 7500 Loss: 182.0090 lr: 0.0043\n",
            "Step: 8000 Loss: 188.1357 lr: 0.0042\n",
            "Step: 8500 Loss: 198.4584 lr: 0.0040\n",
            "Step: 9000 Loss: 201.3073 lr: 0.0039\n",
            "Step: 9500 Loss: 198.3313 lr: 0.0037\n",
            "Step: 10000 Loss: 193.9402 lr: 0.0036\n",
            "Step: 10500 Loss: 187.6336 lr: 0.0034\n",
            "Step: 11000 Loss: 194.5650 lr: 0.0033\n",
            "Step: 11500 Loss: 192.8356 lr: 0.0031\n",
            "Step: 12000 Loss: 188.5741 lr: 0.0029\n",
            "Step: 12500 Loss: 197.9906 lr: 0.0028\n",
            "Step: 13000 Loss: 193.8125 lr: 0.0026\n",
            "Step: 13500 Loss: 198.6066 lr: 0.0025\n",
            "Step: 14000 Loss: 189.4090 lr: 0.0023\n",
            "Step: 14500 Loss: 190.9812 lr: 0.0022\n",
            "Step: 15000 Loss: 194.2522 lr: 0.0020\n",
            "Step: 15500 Loss: 203.1802 lr: 0.0019\n",
            "Step: 16000 Loss: 184.4098 lr: 0.0017\n",
            "Step: 16500 Loss: 194.7722 lr: 0.0015\n",
            "Step: 17000 Loss: 197.5684 lr: 0.0014\n",
            "Step: 17500 Loss: 199.2104 lr: 0.0012\n",
            "Step: 18000 Loss: 193.9008 lr: 0.0011\n",
            "Step: 18500 Loss: 201.4198 lr: 0.0009\n",
            "Step: 19000 Loss: 192.8857 lr: 0.0008\n",
            "Step: 19500 Loss: 193.0344 lr: 0.0006\n",
            "Step: 20000 Loss: 203.3453 lr: 0.0005\n",
            "Step: 20500 Loss: 197.2147 lr: 0.0003\n",
            "Step: 21000 Loss: 195.3592 lr: 0.0001\n",
            "Epoch 2 Total Mean Loss : 189.6529\n",
            "*****Epoch 2 Train Finished*****\n",
            "\n",
            "*****Epoch 2 Saving Embedding...*****\n",
            "*****Epoch 2 Embedding Saved at ./word2vec_wiki/w2v_2.txt*****\n",
            "\n"
          ]
        }
      ],
      "source": [
        "w2v.train(train_dataloader)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3uTIm4vJ4WSp"
      },
      "source": [
        "### 유사한 단어 확인\n",
        "- 사전에 존재하는 단어들과 유사한 단어를 검색해보자. Gensim 패키지는 유사 단어 외에도 단어간의 유사도를 계산하는 여러 함수를 제공한다. 실험을 통해 word2vec의 한계점을 발견했다면 아래에 markdown으로 작성해보자. \n",
        "  - 예1. 학습 때 보지 못한 단어는 embedding을 구할 수 없음. (Out Of Vocabulary Problem)\n",
        "  - 예2. 같은 단어라고 할지라도 학습 문서의 도메인에따라 embedding 값이 달라짐. \n",
        "- [Gensim 패키지 document](https://radimrehurek.com/gensim/models/keyedvectors.html)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-02-19T14:47:59.605389Z",
          "start_time": "2022-02-19T14:47:59.368925Z"
        },
        "id": "AKpBuVlP4WSp"
      },
      "outputs": [],
      "source": [
        "import gensim"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-02-19T14:49:06.590460Z",
          "start_time": "2022-02-19T14:49:05.174241Z"
        },
        "id": "_7WPtss_CmWM"
      },
      "outputs": [],
      "source": [
        "word_vectors = gensim.models.KeyedVectors.load_word2vec_format('./word2vec_wiki/w2v_1.txt', binary=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-02-19T14:55:31.040003Z",
          "start_time": "2022-02-19T14:55:31.034836Z"
        },
        "id": "DVeJhIthCmWM",
        "outputId": "75164bbc-5617-4cf0-f979-bfdae550fcc4"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('주한', 0.9916805624961853),\n",
              " ('광역시', 0.991572380065918),\n",
              " ('육군사관학교', 0.9912100434303284),\n",
              " ('보병', 0.9907797574996948),\n",
              " ('방송국', 0.9904221892356873),\n",
              " ('박물관', 0.9901329278945923),\n",
              " ('한양', 0.9900871515274048),\n",
              " ('알아인', 0.9899305105209351),\n",
              " ('당원', 0.98966383934021),\n",
              " ('개신교도', 0.9896210432052612)]"
            ]
          },
          "execution_count": 94,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "word_vectors.most_similar(positive=['대통령'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-02-19T14:49:11.324372Z",
          "start_time": "2022-02-19T14:49:11.315429Z"
        },
        "id": "cLFVRhPjCmWM",
        "outputId": "23df1bc2-1e59-49d2-fb83-8bca0c8215ab"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('막달레나', 0.9994158744812012),\n",
              " ('명탐정', 0.9993422627449036),\n",
              " ('재상', 0.9993184208869934),\n",
              " ('뉴캐슬', 0.9992764592170715),\n",
              " ('린든', 0.9992668628692627),\n",
              " ('줄넘기', 0.9992555975914001),\n",
              " ('공예', 0.9992364048957825),\n",
              " ('워즈', 0.9992086887359619),\n",
              " ('찹', 0.9991920590400696),\n",
              " ('배기', 0.9991424083709717)]"
            ]
          },
          "execution_count": 69,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "word_vectors.most_similar(positive=['코난'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GogcQk7tCmWM"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Week3_1_assignment_ta.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "torch",
      "language": "python",
      "name": "torch"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}