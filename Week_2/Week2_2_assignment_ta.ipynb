{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Week2_2_assignment_ta.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import os \n",
        "import sys\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ],
      "metadata": {
        "id": "KNFrUj-0cHNs"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Week2_2 Class"
      ],
      "metadata": {
        "id": "0-ppJSZ-r_kE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## 1. Pytorch Graph\n",
        "`torcn.nn` 모듈은 텐서 그래프를 생성하는 다양한 함수를 제공한다.\n",
        "\n",
        "[OFFICAL DOCUMENT](https://pytorch.org/docs/stable/nn.html)\n",
        "\n",
        "## Table of Contents\n",
        "1. [Container](#Container)\n",
        "2. [Layers](#Layers)\n",
        "3. [Loss](#Loss)\n",
        "4. [추가](#To-Learn-More..)\n",
        "\n"
      ],
      "metadata": {
        "id": "oLUM2tOacP81"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.1. Container\n",
        "- [OFFICIAL DOC](https://pytorch.org/docs/stable/generated/torch.nn.Module.html#module)\n",
        "\n",
        "#### 1.1.1 `Module` class\n",
        "- Neural Network를 생성할 때 반드시 `Module` 클래스를 부모 클래스로 상속받아야 함\n",
        "- `Modul` 부모 클래스의 변수 및 메소드 사용 가능 (ex. `eval()`,`train()`, `parameters()`, `state_dict()`, `to()`)\n",
        "- `forward()` 메소드는 모든 자식 클래스에서 반드시 **오버라이딩**해야 함\n",
        "- [출처](https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module)\n",
        "\n",
        "<img src=\"https://github.com/ChristinaROK/PreOnboarding_AI_assets/blob/36a670a7b6233d5218a495150beb337a899ecb70/week2/week2_2_model.jpeg?raw=true\" alt=\"model\" width=600>"
      ],
      "metadata": {
        "id": "ZLOxKoGHci1-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Model(nn.Module):  # 반드시 Module class를 상속받아야 한다.\n",
        "    def __init__(self, input_shape):\n",
        "        super(Model, self).__init__()\n",
        "        self.layer1= nn.Linear(input_shape, 32)\n",
        "        self.layer2= nn.Linear(32, 64)\n",
        "        self.layer_out= nn.Linear(64, 1)\n",
        "\n",
        "        self.relu= nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x= self.relu(self.layer1(x))\n",
        "        x= self.relu(self.layer2(x))\n",
        "        x= self.layer_out(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "f6zi3543dV2H"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model= Model(30) # train의 unit이 30\n",
        "\n",
        "for param in model.parameters():\n",
        "    print(param.shape)\n",
        "\n",
        "for name, state in model.state_dict().items():\n",
        "    print(f\"{name} -> size: {state.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3gKrcGnDpIj_",
        "outputId": "dd9ab4ba-7e17-4a94-a356-c6507a83231f"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([32, 30])\n",
            "torch.Size([32])\n",
            "torch.Size([64, 32])\n",
            "torch.Size([64])\n",
            "torch.Size([1, 64])\n",
            "torch.Size([1])\n",
            "layer1.weight -> size: torch.Size([32, 30])\n",
            "layer1.bias -> size: torch.Size([32])\n",
            "layer2.weight -> size: torch.Size([64, 32])\n",
            "layer2.bias -> size: torch.Size([64])\n",
            "layer_out.weight -> size: torch.Size([1, 64])\n",
            "layer_out.bias -> size: torch.Size([1])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(device)\n",
        "model.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lkOufZPgtxI3",
        "outputId": "0d34089f-de27-42ba-8d76-8a388fa2c6b2"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Model(\n",
              "  (layer1): Linear(in_features=30, out_features=32, bias=True)\n",
              "  (layer2): Linear(in_features=32, out_features=64, bias=True)\n",
              "  (layer_out): Linear(in_features=64, out_features=1, bias=True)\n",
              "  (relu): ReLU()\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1.1.1 `Sequential` class\n",
        "- 여러 layer를 연결한 container\n",
        "- 이전 layer의 output이 다음 layer의 input으로 입력됨 (순차적)\n",
        "- [출처](https://pytorch.org/docs/stable/generated/torch.nn.Sequential.html#torch.nn.Sequential)"
      ],
      "metadata": {
        "id": "shqLu5kCt6Fv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model= nn.Sequential(\n",
        "    nn.Linear(30, 32), # 1번 layer의 출력값이\n",
        "    nn.ReLU(),         # 1번 activation 함수의 입력값으로 들어가고\n",
        "    nn.Linear(32, 64), # 2번 layer의 입력값으로 1번 activation 함수의 출력값이 들어가서\n",
        "    nn.ReLU(),         # 2번 activation의 입력값으로 입력\n",
        "    nn.Linear(64,1)    # 그 결과가 다시 출력 layer로 입력되어 결과값 반환\n",
        ")\n",
        "\n",
        "model.eval()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_voodHHQuNI1",
        "outputId": "12e83d44-9b7e-4fd7-a4a5-61aaea53d51e"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Sequential(\n",
              "  (0): Linear(in_features=30, out_features=32, bias=True)\n",
              "  (1): ReLU()\n",
              "  (2): Linear(in_features=32, out_features=64, bias=True)\n",
              "  (3): ReLU()\n",
              "  (4): Linear(in_features=64, out_features=1, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.2. Layers\n",
        "- Linear( )\n",
        "    - `input @ weight.T + bias`\n",
        "- LSTM( )\n",
        "    - [OFFICAL DOCS](https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html#torch.nn.LSTM) \n",
        "    - nn.LSTM(`input_size`, `hidden_size`)\n",
        "    - `input` shape: (문장 길이, 배치 사이즈, 단어 임베딩 사이즈 == input suze)\n",
        "    - `hidden_size` shape: (lstm 개수 * 레이어 수, 배치 사이즈, 히든 사이즈 == hidden size)\n",
        "\n",
        "    <img src=\"https://github.com/ChristinaROK/PreOnboarding_AI_assets/blob/36a670a7b6233d5218a495150beb337a899ecb70/week2/week2_2_lstm.png?raw=true\" width=500>\n",
        "    - [출처](https://colah.github.io/posts/2015-08-Understanding-LSTMs/)"
      ],
      "metadata": {
        "id": "oX0zO-EivXOw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model= nn.Linear(20, 30)\n",
        "print(f\"W shape: {model.weight.shape}\")\n",
        "print(f\"bias shape: {model.bias.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "__W7XJY_wiLA",
        "outputId": "b4b521a5-f238-4982-b19a-0a3452909613"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "W shape: torch.Size([30, 20])\n",
            "bias shape: torch.Size([30])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ex= \"I love coding . Just kidding .\"\n",
        "inputs= ex.split()\n",
        "print(inputs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SNpl02xDwrFN",
        "outputId": "67aaa89e-1642-4fb7-f852-ede8e39699af"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['I', 'love', 'coding', '.', 'Just', 'kidding', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_embedding= [torch.randn(1, 5) for _ in range(len(inputs))] # 임의로 난수로 단어 임베딩\n",
        "print(f'input_embedding: {input_embedding}')\n",
        "lstm= nn.LSTM(5, 5)  # (input dim, output dim)\n",
        "hidden= (\n",
        "    torch.randn(1, 1, 5),  # (모든 레이어의 lstm 개수, batch size, hidden_size)\n",
        "    torch.randn(1, 1, 5),\n",
        ")\n",
        "\n",
        "# 한 단어씩 입력\n",
        "for idx, i in enumerate(input_embedding):\n",
        "    out, hidden = lstm(i.view(1, 1, -1), hidden)\n",
        "    print(f\"{idx+1} word: output shape ({out.shape}) / hidden state shape ({hidden[0].shape})\")\n",
        "\n",
        "assert out.detach().equal(hidden[0].detach())\n",
        "\n",
        "print('-------------------------------------------------------------------------------------------')\n",
        "\n",
        "# sequence를 입력\n",
        "input_embedding= torch.cat(input_embedding).view(len(inputs), 1, -1)\n",
        "print(f\"input sequence shape: {input_embedding.shape}\")\n",
        "\n",
        "hidden= (\n",
        "    torch.randn(1, 1, 5),\n",
        "    torch.randn(1, 1, 5),\n",
        ")\n",
        "\n",
        "out, hidden= lstm(input_embedding, hidden)\n",
        "print(f\"output shape: {out.shape}\")\n",
        "print(f\"hidden shape: {hidden[0].shape}\")\n",
        "\n",
        "assert out[-1, :, :].detach().equal(hidden[0][-1,:,:].detach())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7-9daKToxMBm",
        "outputId": "1f1e4150-b251-4d9a-cbfa-55aba21aba0d"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input_embedding: [tensor([[ 1.1381,  0.2423,  0.8096, -0.2866, -0.2022]]), tensor([[ 0.2569,  0.6500,  0.4125, -1.0496, -0.8965]]), tensor([[ 0.7071,  1.4113, -1.4082, -1.4036,  0.3996]]), tensor([[-0.9132, -0.0932, -0.1082,  1.0174, -1.0952]]), tensor([[ 0.1479, -1.5313, -0.0890, -0.7902,  0.6156]]), tensor([[-1.0946,  0.4375,  0.2711, -0.4052, -0.7780]]), tensor([[-1.1408,  0.1480,  0.3153,  0.7641, -1.0154]])]\n",
            "1 word: output shape (torch.Size([1, 1, 5])) / hidden state shape (torch.Size([1, 1, 5]))\n",
            "2 word: output shape (torch.Size([1, 1, 5])) / hidden state shape (torch.Size([1, 1, 5]))\n",
            "3 word: output shape (torch.Size([1, 1, 5])) / hidden state shape (torch.Size([1, 1, 5]))\n",
            "4 word: output shape (torch.Size([1, 1, 5])) / hidden state shape (torch.Size([1, 1, 5]))\n",
            "5 word: output shape (torch.Size([1, 1, 5])) / hidden state shape (torch.Size([1, 1, 5]))\n",
            "6 word: output shape (torch.Size([1, 1, 5])) / hidden state shape (torch.Size([1, 1, 5]))\n",
            "7 word: output shape (torch.Size([1, 1, 5])) / hidden state shape (torch.Size([1, 1, 5]))\n",
            "-------------------------------------------------------------------------------------------\n",
            "input sequence shape: torch.Size([7, 1, 5])\n",
            "output shape: torch.Size([7, 1, 5])\n",
            "hidden shape: torch.Size([1, 1, 5])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.3. Activation\n",
        "- nonlinear activations"
      ],
      "metadata": {
        "id": "-HnNQpDQ_Zqj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nn.LeakyReLU()\n",
        "nn.ReLU()\n",
        "nn.Sigmoid()\n",
        "nn.GELU()\n",
        "nn.Tanh()\n",
        "nn.Softmax()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n5pSqJbr_n7V",
        "outputId": "788e9669-7ab4-4b38-dbb0-ffa447b44d35"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Softmax(dim=None)"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.4. Loss\n",
        "- loss = loss_class()\n",
        "    - loss(`y_hat`, `y`)\n",
        "    - `loss().backward()`\n",
        "- Mean Square Error Loss \n",
        "    - <img src=\"https://github.com/ChristinaROK/PreOnboarding_AI_assets/blob/36a670a7b6233d5218a495150beb337a899ecb70/week2/week2_2_mse.png?raw=true\" width=200>\n",
        "- Cross Entropy Loss \n",
        "    - <img src=\"https://github.com/ChristinaROK/PreOnboarding_AI_assets/blob/36a670a7b6233d5218a495150beb337a899ecb70/week2/week2_2_ce.png?raw=true\" width=200>\n",
        "- Binary Cross Entropy Loss \n",
        "    - <img src=\"https://github.com/ChristinaROK/PreOnboarding_AI_assets/blob/36a670a7b6233d5218a495150beb337a899ecb70/week2/week2_2_bce.png?raw=true\" width=500>"
      ],
      "metadata": {
        "id": "8NuVZCNj_zpg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# l2 distance loss\n",
        "nn.MSELoss()\n",
        "\n",
        "# cross entropy\n",
        "## multi class\n",
        "nn.CrossEntropyLoss()  # softmax 0\n",
        "\n",
        "## binary class\n",
        "nn.BCELoss()\n",
        "nn.BCEWithLogitsLoss() # sigmoid 0 + BCELoss()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6bmFOd2k_5hy",
        "outputId": "893ecc0c-3f2c-4ff0-a4f2-f93fa420ea31"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BCEWithLogitsLoss()"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size= 3\n",
        "C= 5\n",
        "logits= torch.randn(batch_size, C, requires_grad=True)\n",
        "print(f\"Logits: {logits}\")\n",
        "\n",
        "loss= nn.CrossEntropyLoss()\n",
        "\n",
        "target= torch.empty(batch_size, dtype= torch.long).random_(C)\n",
        "print(f\"Target: {target}\")\n",
        "\n",
        "loss= loss(logits, target)\n",
        "print(f\"Loss: {loss}\")\n",
        "\n",
        "loss.backward()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G_icHcwqAS_o",
        "outputId": "55820316-0618-482d-b1de-623f73bade78"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logits: tensor([[ 0.3367,  2.9557, -1.0266,  1.2258,  0.5213],\n",
            "        [ 0.2764,  0.0517,  0.3147, -1.1077,  1.9244],\n",
            "        [-1.0124, -0.9056,  0.7089,  0.0415, -0.4226]], requires_grad=True)\n",
            "Target: tensor([0, 1, 3])\n",
            "Loss: 2.2416300773620605\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Week2_2 Assignment\n",
        "\n",
        "## [BASIC](#Basic) \n",
        "- \"네이버 영화 감성 분류\" 데이터를 불러와 `pandas` 라이브러리를 사용해 **전처리** 할 수 있다.\n",
        "- 적은 데이터로도 높은 성능을 내기 위해, pre-trained `BERT` 모델 위에 1개의 hidden layer를 쌓아 **fine-tuning**할 수 있다.\n",
        "\n",
        "## [CHALLENGE](#Challenge)\n",
        "- 토큰화된 학습 데이터를 배치 단위로 갖는 **traindata iterator**를 구현할 수 있다. \n",
        "\n",
        "## [ADVANCED](#Advanced)\n",
        "- **loss와 optimizer 함수**를 사용할 수 있다. \n",
        "- traindata iterator를 for loop 돌며 **fine-tuning** 할 수 있다.\n",
        "- fine-tuning의 2가지 방법론을 비교할 수 있다. \n",
        "  - BERT 파라미터를 **freeze** 한 채 fine-tuning (Vision에서 주로 사용하는 방법론)\n",
        "  - BERT 파라미터를 **unfreeze** 한 채 fine-tuning (NLP에서 주로 사용하는 방법론)\n",
        "\n",
        "\n",
        "### Reference\n",
        "- [huggingface 한국어 오픈소스 모델](https://huggingface.co/models?language=ko&sort=downloads&search=bert)\n",
        "- [transformer BertForSequenceClassification 소스 코드](https://github.com/huggingface/transformers/blob/v4.15.0/src/transformers/models/bert/modeling_bert.py#L1501)"
      ],
      "metadata": {
        "id": "6F3NHRY_7l3k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import sys\n",
        "import pandas as pd\n",
        "import numpy as np \n",
        "import torch\n",
        "import random"
      ],
      "metadata": {
        "id": "eplNEnRZ7lYP"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# seed\n",
        "seed = 7777\n",
        "random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed_all(seed)"
      ],
      "metadata": {
        "id": "V7jktOEE7rWJ"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# device type\n",
        "if torch.cuda.is_available():\n",
        "    device= torch.device(\"cuda\")\n",
        "    print(f\"# available GPUs: {torch.cuda.device_count()}\")\n",
        "    print(f\"GPU name: {torch.cuda.get_device_name()}\")\n",
        "else:\n",
        "    device= torch.device(\"cpu\")\n",
        "\n",
        "print(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S2ALucRT71s8",
        "outputId": "a58ad73c-ebb2-4790-9e18-125340cf4593"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# available GPUs: 1\n",
            "GPU name: Tesla P100-PCIE-16GB\n",
            "cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Basic"
      ],
      "metadata": {
        "id": "W-p6AjkU8SFP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 데이터 다운로드 및 DataFrame 형태로 불러오기\n",
        "- 내 구글 드라이브에 데이터를 다운받은 후 코랩에 드라이브를 마운트하면 데이터를 영구적으로 사용할 수 있음.\n",
        "- [네이버영화감성분류](https://github.com/e9t/nsmc)\n",
        "  - trainset: 150,000 \n",
        "  - testset: 50,000 "
      ],
      "metadata": {
        "id": "EiFnrfEL8Y01"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DsFEaMY08ZE_",
        "outputId": "ee326039-633a-4605-ba76-6359a6bba7fa"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 데이터 다운로드\n",
        "!git clone https://github.com/e9t/nsmc.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xXIqgiuw8c8h",
        "outputId": "e178fe5c-4229-4b37-d5f7-89fbe8544eb4"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'nsmc' already exists and is not an empty directory.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3IlDnBtuBsGh",
        "outputId": "3f3257b8-776e-43ca-c642-6c27e35b4fce"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.17.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.4.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.11.3)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.6.0)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.11.6)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.63.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.5)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.49)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.7.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "_CUR_DIR = os.path.abspath(os.curdir)\n",
        "print(f\"My current directory : {_CUR_DIR}\")\n",
        "_DATA_DIR = os.path.join(_CUR_DIR, \"nsmc\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xd6LtNls8jNg",
        "outputId": "f8b6e148-5ee3-42de-d2e4-a087540d1d4d"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "My current directory : /content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# nsmc/ratings_train.txt를 DataFrame 형태로 불러오기\n",
        "df = pd.read_csv('/content/nsmc/ratings_train.txt', sep='\\t')\n",
        "print(df.shape)\n",
        "df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        },
        "id": "u8lNyEYd8mEy",
        "outputId": "76d853c9-d8bc-4a92-9575-d78637c59022"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(150000, 3)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "         id                                           document  label\n",
              "0   9976970                                아 더빙.. 진짜 짜증나네요 목소리      0\n",
              "1   3819312                  흠...포스터보고 초딩영화줄....오버연기조차 가볍지 않구나      1\n",
              "2  10265843                                  너무재밓었다그래서보는것을추천한다      0\n",
              "3   9045019                      교도소 이야기구먼 ..솔직히 재미는 없다..평점 조정      0\n",
              "4   6483659  사이몬페그의 익살스런 연기가 돋보였던 영화!스파이더맨에서 늙어보이기만 했던 커스틴 ...      1"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-9d996c7c-e3d9-43f2-9b8f-c18be68a2c80\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>document</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>9976970</td>\n",
              "      <td>아 더빙.. 진짜 짜증나네요 목소리</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>3819312</td>\n",
              "      <td>흠...포스터보고 초딩영화줄....오버연기조차 가볍지 않구나</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>10265843</td>\n",
              "      <td>너무재밓었다그래서보는것을추천한다</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>9045019</td>\n",
              "      <td>교도소 이야기구먼 ..솔직히 재미는 없다..평점 조정</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>6483659</td>\n",
              "      <td>사이몬페그의 익살스런 연기가 돋보였던 영화!스파이더맨에서 늙어보이기만 했던 커스틴 ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-9d996c7c-e3d9-43f2-9b8f-c18be68a2c80')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-9d996c7c-e3d9-43f2-9b8f-c18be68a2c80 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-9d996c7c-e3d9-43f2-9b8f-c18be68a2c80');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 데이터 결측치 제거 및 데이터 수 줄이기 \n",
        "- 학습 데이터 수는 150,000개로 매우 많은 양이다. 하지만 우리가 실생활에서 마주할 데이터는 이렇게 많지 않다. 이 때 유용하게 사용되는 것이 **fine-tuning** 학습 방법이다.   \n",
        "- Fine-tuning은 단어의 의미를 이미 충분히 학습한 모델 (여기서는 **BERT**)을 가져와 그 위에 추가적인 Nueral Network 레이어를 쌓은 후 학습하는 방법론이다. 이미 BERT가 단어의 의미를 충분히 학습했기 때문에 **적은 데이터**로 학습해도 우수한 성능을 낼 수 있다는 장점이 있다. \n",
        "- **데이터의 label의 비율이 5:5를 유지하면서** 학습 데이터 수를 150,000개에서 1,000개로 줄이\b는 함수 `label_evenly_balanced_dataset_sampler`를 구현하라.\n",
        "  - 함수 정의 \n",
        "    - 입력 매개변수\n",
        "      - df : DataFrame\n",
        "      - n_sample : df에서 샘플링할 row의 개수 (여기서는 1000개로 정의한다)\n",
        "    - 조건\n",
        "      - label의 비율이 5:5를 유지할 수 있도록 샘플링한다.\n",
        "    - 반환값\n",
        "      - row의 개수가 1000개인 dataframe"
      ],
      "metadata": {
        "id": "sE34y5ig9PYp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# df에서 결측치 (na 값) 제거\n",
        "\n",
        "df = df.dropna()\n",
        "df.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e3DhuMH79MKJ",
        "outputId": "b0da616f-7e18-4ea4-95b1-5faa042509ca"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(149995, 3)"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# label별 데이터 수 확인\n",
        "# pandas의 value_counts 함수 활용\n",
        "# 0 -> 부정 1 -> 긍정\n",
        "\n",
        "df.label.value_counts()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9PmCEkbZ9UZ_",
        "outputId": "dc93196c-b999-4427-902d-95108b6336ec"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    75170\n",
              "1    74825\n",
              "Name: label, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 학습 데이터 샘플 개수 설정\n",
        "\n",
        "n_sample = 1000\n",
        "\n",
        "# 샘플링 함수 구현\n",
        "# random 모듈에서 제공되는 함수 활용\n",
        "# input: 학습 데이터 샘플 개수\n",
        "# output: 샘플링 데이터\n",
        "\n",
        "\n",
        "def label_evenly_balanced_dataset_sampler(df, sample_size):\n",
        "  \"\"\"\n",
        "  데이터 프레임의을 sample_size만큼 임의 추출해 새로운 데이터 프레임을 생성.\n",
        "  이 때, \"label\"열의 값들이 동일한 비율을 갖도록(5:5) 할 것.\n",
        "  \"\"\"\n",
        "  num_sample= int(sample_size / 2)\n",
        "  label_0 = df[df['label']==0].sample(n=num_sample)\n",
        "  label_1 = df[df['label']==1].sample(n=num_sample)\n",
        "\n",
        "  sample = pd.concat([label_0, label_1])\n",
        "\n",
        "\n",
        "  return sample\n",
        "\n",
        "sample_df = label_evenly_balanced_dataset_sampler(df, n_sample)"
      ],
      "metadata": {
        "id": "7QnAUqhS9VI-"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 검증\n",
        "\n",
        "sample_df.label.value_counts()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "feWekU4j_b1r",
        "outputId": "ce3f074e-d19e-4687-9d4b-858cba47565c"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    500\n",
              "1    500\n",
              "Name: label, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### CustomClassifier 클래스 구현\n",
        "<img src=\"https://github.com/ChristinaROK/PreOnboarding_AI_assets/blob/36a670a7b6233d5218a495150beb337a899ecb70/week2/week2_2_bertclf.png?raw=true\" width=400>\n",
        "\n",
        "- 그림과 같이 사전 학습(pre-trained)된 `BERT` 모델을 불러와 그 위에 **1 hidden layer**와 **binary classifier layer**를 쌓아 fine-tunning 모델을 생성할 것이다.    \n",
        "---\n",
        "- hidden layer 1개와 output layer(binary classifier layer)를 갖는 `CustomClassifier` 클래스를 구현하라.\n",
        "- 클래스 정의\n",
        "  - 생성자 입력 매개변수\n",
        "    - `hidden_size` : BERT의 embedding size\n",
        "    - `n_label` : class(label) 개수\n",
        "  - 생성자에서 생성할 변수\n",
        "    - `bert` : BERT 모델 인스턴스 \n",
        "    - `classifier` : 1 hidden layer + relu +  dropout + classifier layer를 stack한 `nn.Sequential` 모델\n",
        "      - 첫번재 히든 레이어 (첫번째 `nn.Linear`)\n",
        "        - input: BERT의 마지막 layer의 1번재 token ([CLS] 토큰) (shape: `hidden_size`)\n",
        "        - output: (shape: `linear_layer_hidden_size`)\n",
        "      - 아웃풋 레이어 (두번째 `nn.Linear`)\n",
        "        - input: 첫번째 히든 레이어의 아웃풋 (shape: `linear_layer_hidden_size`)\n",
        "        - output: target/label의 개수 (shape:2)\n",
        "  - 메소드\n",
        "    - `forward()`\n",
        "      - BERT output에서 마지막 레이어의 첫번째 토큰 ('[CLS]')의 embedding을 가져와 `self.classifier`에 입력해 아웃풋으로 logits를 출력함.\n",
        "  - 주의 사항\n",
        "    - `CustomClassifier` 클래스는 부모 클래스로 `nn.Module`을 상속 받는다.\n"
      ],
      "metadata": {
        "id": "4H7XeCMl_rq5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from transformers import BertModel"
      ],
      "metadata": {
        "id": "Ute__Wke_lj1"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# classifier 구현\n",
        "class CustomClassifier(nn.Module):\n",
        "\n",
        "  def __init__(self, hidden_size: int, n_label: int):\n",
        "    super(CustomClassifier, self).__init__()\n",
        "\n",
        "    self.bert = BertModel.from_pretrained(\"klue/bert-base\")\n",
        "\n",
        "    dropout_rate = 0.1\n",
        "    linear_layer_hidden_size = 32\n",
        "\n",
        "    self.classifier = nn.Sequential(\n",
        "                            nn.Linear(hidden_size,linear_layer_hidden_size),\n",
        "                            nn.ReLU(),\n",
        "                            nn.Dropout(dropout_rate),\n",
        "                            nn.Linear(linear_layer_hidden_size,n_label)\n",
        "                                    ) # torch.nn에서 제공되는 Sequential, Linear, ReLU, Dropout 함수 활용\n",
        "\n",
        "\n",
        "  def forward(self, input_ids=None, attention_mask=None, token_type_ids=None):\n",
        "\n",
        "    outputs = self.bert(\n",
        "        input_ids,\n",
        "        attention_mask=attention_mask,\n",
        "        token_type_ids=token_type_ids,\n",
        "    )\n",
        "\n",
        "    # BERT 모델의 마지막 레이어의 첫번재 토큰을 인덱싱\n",
        "    last_hidden_state= outputs[0]\n",
        "    cls_token_last_hidden_states = last_hidden_state[:,0,:] # 마지막 layer의 첫 번째 토큰 (\"[CLS]\") 벡터를 가져오기, shape = (1, hidden_size)\n",
        "\n",
        "    logits = self.classifier(cls_token_last_hidden_states)\n",
        "\n",
        "    return logits"
      ],
      "metadata": {
        "id": "k8l0iuQe_qV7"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Challenge"
      ],
      "metadata": {
        "id": "pJQJP7QtHAOI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 학습 데이터를 배치 단위로 저장하는 이터레이터 함수 `data_iterator` 구현\n",
        "- 데이터 프레임을 입력 받아 text를 토큰 id로 변환하고 label은 텐서로 변환해 배치만큼 잘라 (input, target) 튜플 형태의 이터레이터를 생성하는 `data_iterator` 함수를 구현하라.\n",
        "- 함수 정의 \n",
        "  - 입력 매개변수\n",
        "    - `input_column` : text 데이터 column 명\n",
        "    - `target_column` : label 데이터 column 명\n",
        "    -  `batch_size` : 배치 사이즈\n",
        "  - 조건\n",
        "    - 함수는 다음을 수행해야 함 \n",
        "      - 데이터 프레임 랜덤 셔플링\n",
        "      - `tokenizer_bert`로 text를 token_id로 변환 + 텐서화 \n",
        "      - target(label)을 텐서화\n",
        "  - 반환값 \n",
        "    - (input, target) 튜플 형태의 이터레이터를 반환"
      ],
      "metadata": {
        "id": "PI6s7HKrHGTo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizer, BertModel\n",
        "tokenizer_bert= BertTokenizer.from_pretrained('klue/bert-base')"
      ],
      "metadata": {
        "id": "W9mkgm5OHACU"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Tokenizing 예시(1개의 문장)"
      ],
      "metadata": {
        "id": "XGbF2EPRHjM-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. string type의 문장을 가져옴\n",
        "ex_sent= sample_df.document.iloc[0]\n",
        "print(f\"Original Sentence: {ex_sent}\\n\")\n",
        "\n",
        "# 2. 문장을 토크나이즈함. 이때, 특수 토큰 (\"[CLS]\", \"[SEP]\")을 자동으로 추가하고 pytorch의 tensor 형태로 변환해 반환함\n",
        "tensor_sent= tokenizer_bert(\n",
        "    ex_sent, \n",
        "    add_special_tokens=True, # 문장의 앞에 문장 시작을 알리는 \"[CLS]\" 토큰, 문장의 마지막에 문장 끝을 알리는 \"[SEP]\" 토큰을 추가함\n",
        "    return_tensors='pt' # pytorch tensor로 반환할 것\n",
        ")\n",
        "\n",
        "print(f\"Tokenized Sentence: \\n{tensor_sent}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aQb31yH4G4Lq",
        "outputId": "30ddb23f-af42-4f17-9dc5-1c2cccdbc7aa"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Sentence: 도대체 뭐냐..이건 ㅋㅋ\n",
            "\n",
            "Tokenized Sentence: \n",
            "{'input_ids': tensor([[   2, 6641, 1097, 2529,   18,   18, 5370, 3725,    3]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Tokenizing 예시(2개의 문장)"
      ],
      "metadata": {
        "id": "N9Rpam6TIuEn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. 2개의 문장을 가진 list 생성\n",
        "ex_sent_list= list(sample_df.document.iloc[:2].values)\n",
        "for i, sent in enumerate(ex_sent_list):\n",
        "    print(f\"Original Sentence {i+1}: {sent}\")\n",
        "\n",
        "# 2. 문장 리스트를 토크나이즈 함. \n",
        "# 이때, 리스트 내 문장들의 토큰 길이가 동일할 수 있도록 가장 긴 문장을 기준으로 부족한 위치에 \"[PAD]\" 토큰을 추가함\n",
        "tensor_sent_list= tokenizer_bert(\n",
        "    ex_sent_list,\n",
        "    add_special_tokens=True,\n",
        "    return_tensors='pt',\n",
        "    padding='longest'  # 가장 긴 문장을 기준으로 token 개수를 맞춤. 모자른 토큰 위치는 \"[PAD]\" 토큰을 추가\n",
        ")\n",
        "\n",
        "print(f\"\\nTokenized Sentence list: {tensor_sent_list}\")\n",
        "\n",
        "# 토크나이즈된 두 문장의 길이가 동일함을 검증\n",
        "assert tensor_sent_list['input_ids'][0].shape == tensor_sent_list['input_ids'][1].shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p5Tw1pB_Ijgv",
        "outputId": "a42805ed-b9d8-4dd4-f8b9-fa774b373070"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Sentence 1: 도대체 뭐냐..이건 ㅋㅋ\n",
            "Original Sentence 2: 너무 어중간함. 스파이 영화라기엔 액션이 너무 없고, 연애물이라기엔 너무 밋밋하고. 이런 영화는 양쪽의 장점을 살려야 하는데 둘다 망함. 그 결과 볼거리가 없고 지루함\n",
            "\n",
            "Tokenized Sentence list: {'input_ids': tensor([[    2,  6641,  1097,  2529,    18,    18,  5370,  3725,     3,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
            "        [    2,  3760,  1406, 22227,  2530,    18, 13106,  3771, 16947,  2614,\n",
            "          8765,  2052,  3760,  1415,  2088,    16,  7738,  2266,  2052, 16947,\n",
            "          2614,  3760, 23977, 19521,    18,  3667,  3771,  2259,  8108,  2079,\n",
            "          5472,  2069, 28049,  1889, 13964,   867,  2062,  1046,  2530,    18,\n",
            "           636,  3731, 10421,  2116,  1415,  2088,  9734,  2530,     3]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0],\n",
            "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0],\n",
            "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1]])}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def data_iterator(df, input_column, target_column, batch_size):\n",
        "  \"\"\"\n",
        "  데이터 프레임을 셔플한 후 \n",
        "  데이터 프레임의 input_column을 batch_size만큼 잘라 토크나이즈 + 텐서화하고, target_column을 batch_size만큼 잘라 텐서화 하여\n",
        "  (input, output) 튜플 형태의 이터레이터를 생성\n",
        "  \"\"\"\n",
        "\n",
        "  global tokenizer_bert\n",
        "\n",
        "  # 1. 데이터 프레임 셔플\n",
        "  #    pandas의 sample 함수 사용\n",
        "  df = df.sample(frac=1, random_state=seed).reset_index(drop=True)\n",
        "\n",
        "  # 2. 이터레이터 생성\n",
        "  for idx in range(0, df.shape[0], batch_size):\n",
        "    batch_df = df.iloc[idx:idx+batch_size] # batch_size만큼 데이터 추출\n",
        "    \n",
        "    tensorized_input = tokenizer_bert(\n",
        "                            list(batch_df[input_column].values),\n",
        "                            add_special_tokens=True,\n",
        "                            return_tensors='pt',\n",
        "                            padding='longest'\n",
        "    ) # df의 text를 토크나이징 + token id로 변환 + 텐서화 (df의 input_column 사용)\n",
        "    \n",
        "    tensorized_target = torch.tensor(list(batch_df[target_column].values)) # target(label)을 텐서화 (df의 target_column 사용)\n",
        "\n",
        "    yield tensorized_input, tensorized_target # 튜플 형태로 yield"
      ],
      "metadata": {
        "id": "aPfwi9AfJzJb"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size=32\n",
        "train_iterator = data_iterator(sample_df, 'document', 'label', batch_size)"
      ],
      "metadata": {
        "id": "TAO5j5EsJ_gj"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "next(train_iterator)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0NeNg1UnJ_qa",
        "outputId": "4382db75-e316-459d-fc1a-24f49e85bc1a"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "({'input_ids': tensor([[    2, 20225,  2961,  ...,     0,     0,     0],\n",
              "         [    2, 10499,  2116,  ...,     0,     0,     0],\n",
              "         [    2,  1556,  4390,  ...,     0,     0,     0],\n",
              "         ...,\n",
              "         [    2,  3919,  2119,  ...,     0,     0,     0],\n",
              "         [    2, 17211,  2119,  ...,     0,     0,     0],\n",
              "         [    2, 11840,  2410,  ...,     0,     0,     0]]), 'token_type_ids': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
              "         [0, 0, 0,  ..., 0, 0, 0],\n",
              "         [0, 0, 0,  ..., 0, 0, 0],\n",
              "         ...,\n",
              "         [0, 0, 0,  ..., 0, 0, 0],\n",
              "         [0, 0, 0,  ..., 0, 0, 0],\n",
              "         [0, 0, 0,  ..., 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
              "         [1, 1, 1,  ..., 0, 0, 0],\n",
              "         [1, 1, 1,  ..., 0, 0, 0],\n",
              "         ...,\n",
              "         [1, 1, 1,  ..., 0, 0, 0],\n",
              "         [1, 1, 1,  ..., 0, 0, 0],\n",
              "         [1, 1, 1,  ..., 0, 0, 0]])},\n",
              " tensor([0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0,\n",
              "         1, 0, 1, 0, 1, 1, 0, 0]))"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Advanced"
      ],
      "metadata": {
        "id": "Z5Yzjb-bOHKI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### `data_iterator` 함수로 생성한 이터레이터를 for loop 돌면서 배치 단위의 데이터를 모델에 학습하는 `train()` 함수 구현\n",
        "- 함수 정의\n",
        "  - 입력 매개변수\n",
        "    - `model` : BERT + 1 hidden layer classifier 모델\n",
        "    - `data_iterator` : train data iterator\n",
        "- Reference\n",
        "  - [Loss: CrossEntropyLoss official document](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html)\n",
        "  - [Optimizer: AdamW official document](https://pytorch.org/docs/stable/generated/torch.optim.AdamW.html)"
      ],
      "metadata": {
        "id": "-O6CleMNOIpc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.optim import AdamW\n",
        "from torch.nn import CrossEntropyLoss\n",
        "from numpy.core.fromnumeric import nonzero"
      ],
      "metadata": {
        "id": "9zxl-QhrKL2E"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 모델 클래스 정의\n",
        "model= CustomClassifier(hidden_size=768, n_label=2)\n",
        "\n",
        "batch_size= 32\n",
        "\n",
        "# 데이터 이터레이터 정의\n",
        "train_iterator= data_iterator(sample_df, 'document', 'label', batch_size)\n",
        "\n",
        "# 로스 및 옵티마이저\n",
        "loss_fct= CrossEntropyLoss()\n",
        "optimizer= AdamW(\n",
        "    model.parameters(),\n",
        "    lr= 2e-5,\n",
        "    eps= 1e-8\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E91CmqE8Oxt6",
        "outputId": "558d06e8-584a-4876-d905-9ba785465aad"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at klue/bert-base were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, data_iterator):\n",
        "\n",
        "    global loss_fct # 위에서 정의한 loss 함수\n",
        "\n",
        "    # 배치 단위 평균 loss와 총 평균 loss를 계산하기 위해 변수 생성\n",
        "    total_loss, batch_loss, batch_count= 0, 0, 0\n",
        "\n",
        "    # model을 train 모드로 설정 & device 할당\n",
        "    model.train()\n",
        "    model.to(device)\n",
        "\n",
        "    # data iterator를 돌면서 하나씩 학습\n",
        "    for step, batch in enumerate(data_iterator):\n",
        "        batch_count+=1\n",
        "\n",
        "        # tensor 연산 전, 각 tensor에 device 할당\n",
        "        batch= tuple(item.to(device) for item in batch)\n",
        "\n",
        "        batch_input, batch_label= batch\n",
        "\n",
        "        # batch 마다 모델이 갖고 있는 기존 gradient를 초기화\n",
        "        model.zero_grad()\n",
        "\n",
        "        # forward\n",
        "        logits= model(**batch_input)\n",
        "\n",
        "        # loss\n",
        "        loss= loss_fct(logits,batch_label)\n",
        "        batch_loss+= loss.item()\n",
        "        total_loss+= loss.item()\n",
        "\n",
        "        # backward -> 파라미터의 미분(gradient)를 자동으로 계산\n",
        "        loss.backward()\n",
        "\n",
        "        # optimizer 업데이트\n",
        "        optimizer.step()\n",
        "\n",
        "        # 배치 10개씩 처리할 때마다 평균 loss를 출력\n",
        "        if (step % 10 == 0 and step != 0):\n",
        "            print(f\"Step: {step}, Avg Loss: {batch_loss / batch_count:.4f}\")\n",
        "\n",
        "            # 변수 초기화\n",
        "            batch_loss, batch_count= 0,0\n",
        "\n",
        "    print(f\"Mean Loss : {total_loss/(step+1):.4f}\")\n",
        "    print(\"Train Finished\")"
      ],
      "metadata": {
        "id": "nUAJn94UPUMj"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 지금까지 구현한 함수와 클래스를 모두 불러와 `train()` 함수를 실행하자\n",
        "- fine-tuning 모델 클래스 (`CustomClassifier`)\n",
        "    - hidden_size = 768\n",
        "    - n_label = 2\n",
        "- 데이터 이터레이터 함수 (`data_iterator`)\n",
        "    - batch_size = 32\n",
        "- loss \n",
        "    - `CrossEntropyLoss()`\n",
        "- optimizer\n",
        "    - optimizer는 loss(오차)를 상쇄하기 위해 파라미터를 업데이트 하는 과정\n",
        "    - `optimizer.step()` 시 파라미터가 업데이트 됨 \n",
        "    - [Optimizer 종류 설명](https://ganghee-lee.tistory.com/24)\n",
        "    - `AdamW()`\n",
        "        - [AdamW official document](https://pytorch.org/docs/1.9.1/generated/torch.optim.AdamW.html?highlight=adamw)\n",
        "    - lr = 2e-5\n",
        "    "
      ],
      "metadata": {
        "id": "rDTTe4E9Tl8j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 모델 \n",
        "model= CustomClassifier(hidden_size=768, n_label=2)\n",
        "\n",
        "# 데이터 이터레이터\n",
        "batch_size= 32\n",
        "train_iterator= data_iterator(sample_df, 'document', 'label', batch_size)\n",
        "\n",
        "# 로스 및 옵티마이저\n",
        "loss_fct= CrossEntropyLoss()\n",
        "optimizer= AdamW(\n",
        "    model.parameters(),\n",
        "    lr= 2e-5,\n",
        "    eps= 1e-8,\n",
        ")\n",
        "\n",
        "# 학습 시작\n",
        "train(model, train_iterator)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TYDw34-CTgkJ",
        "outputId": "cdb29033-cb8b-4b0d-ca51-473035ac86d6"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at klue/bert-base were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step: 10, Avg Loss: 0.6483\n",
            "Step: 20, Avg Loss: 0.5094\n",
            "Step: 30, Avg Loss: 0.4625\n",
            "Mean Loss : 0.5424\n",
            "Train Finished\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## fine-tuning 2가지 방법론 비교\n",
        "- pre-trained BERT 모델 파라미터를 **freeze**한 채 학습하라\n",
        "    - BERT의 파라미터의 `requires_grad`값을 `False`로 바꾸면, 학습 시 BERT의 파라미터는 미분이 계산되지도, 업데이트 되지도 않는다.\n",
        "    - 이렇게 특정 모델의 파라미터가 업데이트 하지 못하도록 설정하는 것을 **freeze**라고 한다.\n",
        "    - BERT 파라미터를 freeze시킨 채 학습을 진행해보자. 이럴 경우, 우리가 직접 쌓은 fine-tuning layer의 파라미터만 업데이트 된다.\n",
        "- **unfreeze**와 **freeze**모델의 성능을 비교해보자. 어떤 방식이 더 우수한다?"
      ],
      "metadata": {
        "id": "MxuAvniibmX5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomClassifierFreezed(nn.Module):\n",
        "\n",
        "    def __init__(self, hidden_size: int, n_label: int):\n",
        "        super(CustomClassifierFreezed, self).__init__()\n",
        "\n",
        "        self.bert= BertModel.from_pretrained('klue/bert-base')\n",
        "        # freeze BERT parameter\n",
        "        # BERT의 파라미터는 고정값으로 두고 BERT 위에 씌운 linear layer의 파라미터만 학습하려고 한다.\n",
        "        # 이 경우, BERT의 파라미터의 'requires_grad' 값을 False로 변경해줘야 학습 시 해당 파라미터의 미분값이 계산되지 않는다.\n",
        "        for param in self.bert.parameters():\n",
        "            param.requires_grad= False\n",
        "\n",
        "        dropout_rate= 0.1\n",
        "        linear_layer_hidden_size= 32\n",
        "\n",
        "        self.classifier= nn.Sequential(\n",
        "                            nn.Linear(hidden_size,linear_layer_hidden_size),\n",
        "                            nn.ReLU(),\n",
        "                            nn.Dropout(dropout_rate),\n",
        "                            nn.Linear(linear_layer_hidden_size, n_label)\n",
        "                                    )\n",
        "\n",
        "    def forward(self, input_ids=None, attention_mask=None, token_type_ids=None):\n",
        "        outputs= self.bert(\n",
        "            input_ids,\n",
        "            attention_mask= attention_mask,\n",
        "            token_type_ids= token_type_ids,\n",
        "        )\n",
        "\n",
        "        # BERT 모델의 마지막 레이어의 첫번째 토큰을 인덱싱\n",
        "        last_hidden_states= outputs[0]\n",
        "        cls_token_last_hidden_states= last_hidden_states[:,0,:]\n",
        "        logits= self.classifier(cls_token_last_hidden_states)\n",
        "\n",
        "        return logits"
      ],
      "metadata": {
        "id": "I-VfqNR-UQiY"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# freeze 모델\n",
        "# model을 제외한 설정값은 위에서 실행한 unfreeze 모델과 동일\n",
        "model = CustomClassifierFreezed(hidden_size=768, n_label=2)\n",
        "\n",
        "# 데이터 이터레이터\n",
        "batch_size = 32\n",
        "train_iterator = data_iterator(sample_df, 'document', 'label', batch_size)\n",
        "\n",
        "# 로스 및 옵티마이저\n",
        "loss_fct = CrossEntropyLoss()\n",
        "optimizer = AdamW(\n",
        "    model.parameters(),\n",
        "    lr= 2e-5,\n",
        "    eps= 1e-9\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MXH6LpitdtgA",
        "outputId": "fba14baa-6b5a-4873-f7a5-03c7f13e99b2"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at klue/bert-base were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 학습 시작\n",
        "train(model, train_iterator)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gb2xxXZndu0g",
        "outputId": "2e353586-08b8-44ed-9a26-4834128a93c6"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step: 10, Avg Loss: 0.7250\n",
            "Step: 20, Avg Loss: 0.7251\n",
            "Step: 30, Avg Loss: 0.6858\n",
            "Mean Loss : 0.7132\n",
            "Train Finished\n"
          ]
        }
      ]
    }
  ]
}